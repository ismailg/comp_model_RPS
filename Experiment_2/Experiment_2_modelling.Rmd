---
title: "Experiment_2_modelling"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

```

```{r}
library(tidyr)
library(dplyr)
library(DEoptim)
library(optimParallel)
library(ggpubr)

# dat_exp2 <- read.csv("MyData.csv")
# 
# # transform 'winner' variable in numeric score
# dat_exp2$score <- recode(dat_exp2$winner, human = 1, tie = 0, AI = -1)
# 
# # Create a new variable stage.f as a factor for the stages 1,2 ,3 4 in game
# dat_exp2$stage.f <- factor(dat_exp2$stage, labels = c("Stage 1","Stage 2","Stage 3", "Stage 4"),levels=c(1,2,3,4))
# 
# # create a new variable "game.f" as a factor variable of games
# dat_exp2$game.f <- factor(dat_exp2$game, labels = c("RPS","FWG","SHOOT"),levels=c("rps","fwg","shootout"))
# 
# # recode actions to make them equal to the codes in these files
# dat_exp2$h_action <- recode(dat_exp2$human_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G")
# dat_exp2$a_action <- recode(dat_exp2$ai_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G")
# 
# write.csv(dat_exp2, row.names=FALSE,file="data_exp2.csv")

```

```{r}
# Read in cleaned data file
dat_exp2 <- read.csv("data_exp2.csv")

# Function to produce best reponse to ai_action in exp2
exp2_best_response <- function(game,ai_action){
  if (game == "rps"){
    if (ai_action == "R") {return("P")}
    else if (ai_action == "P") {return("S")}
    else if (ai_action == "S") {return("R")}
    
  } else if (game == "fwg"){ 
    if (ai_action == "F") {return("W")}
    else if (ai_action == "W") {return("G")}
    else if (ai_action == "G") {return("F")}
    
  } else if (game == "shootout") {
    if (ai_action == "left") {return("center/right")}
    else if (ai_action == "center") {return("left/right")}
    else if (ai_action == "right") {return("left/center")}
    else if (ai_action == "left/right") {return("center")}
    else if (ai_action == "left/center") {return("right")}
    else if (ai_action == "center/right") {return("left")}
  }
}
```


```{r}

exp2_naive_bayes <- function(par,data,opp_strategy_vec,return_value,opp_mod_transfer) {
  
  # dat = data subset for one participant
  # opp_strategy_vec = vector of possible opponent strtegies, model assumes humans restrict opp strategy space to vector   c("level0","level1","level2")
  # return = -2logLik 
  # or "likelihood_by_trial" vector  (for later plotting, etc)
  # opp_mod_transfer is a boolean. True if distribution of opponent strategies is kept across games. False otherwise. 
  
  # theta = parameter, probability computer will play its best response to what it thinks human is. Truth = 90%. in [0,1].
  # eps: parameter controlling noise in human choice. Probability human will DEVIATE from its best response. In [0,1].
  theta <- par[1]
  eps <- par[2]
  
  num_strat = length(opp_strategy_vec) 
  
  # Load tables predicting oppponent movel
  
  rps_predict_opp <- read.csv("rps_predict_opp.csv")
  fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
  shootout_predict_opp <- read.csv("shootout_predict_opp.csv")
  

  #lik_opp is a vector that holds and update probability distribution of opp strategy given actions.
  lik_opp <- matrix(0.0,num_strat, nrow(data))
  # Initiate prior vector as uniform
  opp_prior_vec <-rep(1/num_strat, num_strat)
  
  lik_hum <- matrix(0.0,nrow(data))
  # br_hum is vector that stores best responses of human to actions taken by each level-k ai opponent at the round.
  br_hum <- rep(NA,num_strat)

  
  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    pred_file_opp <- switch(as.character(t_game),rps = rps_predict_opp,fwg = fwg_predict_opp, shootout = shootout_predict_opp)
    nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      pred_opp <- NA
      lik_hum[t] <- 1/nopts
      
      ## KEY CODE FOR LEARNING TRANSFER !! (if no OM transfer reset prior to uniform)#####
      if (!(opp_mod_transfer) || data[t,"stage"] == 1) {
        opp_prior_vec <- rep(1/num_strat, num_strat)
        
      # we are round 1 of stage 2, keep opp vector from round 20 of stage 1  for stage 3
      } else if (data[t,"stage"] == 2) { 
        opp1_prior <- opp_prior_vec
        opp_prior_vec <- rep(1/num_strat, num_strat)
      
      # we are round 1 of stage 3, keep opp vector from round 20 of stage 2  for stage 4
      } else if (data[t,"stage"] == 3) { 
        opp2_prior <- opp_prior_vec
        opp_prior_vec <- opp1_prior
      
      # We are in round 1 of stage 4, use vector of priors from stage 2
      }  else if (data[t,"stage"] == 4){ 
        opp_prior_vec <- opp2_prior
      }
      
      
    # Rounds 2 to End of stage
    } else {
      # get prediction of opponent action from CSV files
      for (strategy in opp_strategy_vec) {
        k = match(strategy,opp_strategy_vec)
        pred_opp <- as.character(filter(pred_file_opp, pred_file_opp$human_previous == as.character(data[t-1,"h_action"]) & pred_file_opp$computer_previous == as.character(data[t-1,"a_action"]))[[strategy]])
        
        # Given opponent action predicted, what would be human action best response for each opp strat
        br_hum[[k]] <- exp2_best_response(t_game,as.character(pred_opp))
        
        # First we deal with easy case of unique predicted action ( multiple possible preds in shoot are separated by "/" sign)
        if (!grepl("/",pred_opp,fixed = TRUE)) {
           # Multiply prior by likelihood of observation to get posterior. Done here ot take advantage of For loop. 
          if(as.character(data[t,"a_action"]) == pred_opp) {
            lik_opp[k,t] <- (theta + (1-theta)/nopts)*opp_prior_vec[k]
          } else {
            lik_opp[k,t] <- ((1-theta)/nopts)*opp_prior_vec[k]
          }
        # Here we are in shootout and opponent predicted action is a vector of two possible moves (e.g "left/center")  
        } else {
          if(grepl(as.character(data[t,"a_action"]), pred_opp,fixed = TRUE)) {
            # Assume each action has likelihood of 50%
             lik_opp[k,t] <- 0.5*(theta + (1-theta)/nopts)*opp_prior_vec[k]
          } else {
            #the 2 is to standardise the theta dependent probs: P(ai plays pred | opp is level k)
            lik_opp[k,t] <- 2*((1-theta)/nopts)*opp_prior_vec[k]
          }
        }
        
      # Get which opponent strategy human current actn maps to (if index = 0 then human current action is br to level-0 comp_strat...)
      indices <- which(grepl(as.character(data[t,"h_action"]),br_hum,fixed = TRUE))

      # THEN likelihood of current human action is just prior on opponent vec (haven't updated priors with curr opp act yet)
      # if action is not predicted by any level-k OM, assume human chooses randomly with prob eps
      if(length(indices) == 0){
        lik_hum[t] <- eps/nopts
      # if same br action for multiple possible ai opponents, add probabilities 
      } else {
        lik_hum[t] <- sum(opp_prior_vec[indices])*(1-eps) + eps/nopts
      }
    }  
      # Standardising the probabilities and updating prior
      lik_opp[,t] <- lik_opp[,t]/sum(lik_opp[,t])
      opp_prior_vec <- lik_opp[,t]
    }
  # for debugging uncomment the line below
  #cat(as.character(pred_opp),as.character(data[t,"a_action"]),lik_opp[,t],"\n")
  }
  
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```

```{r}

test_data = subset(dat_exp2,human_id == "bZg1T3MNoItL6b7qAAAj")
exp2_naive_bayes(c(0.5,0.1),test_data,c("level0","level1","level2"),"-2loglik",TRUE)
```
## Fitting Bayesian model with transfer  
```{r}
exp2_TR_bayes <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_TR_bayes[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
  
  # TR_bayes_modelling[[id]] <- DEoptim(fn=naive_bayes, lower = c(0,0), upper = c(1,1), data=tdat, opp_strategy_vec = c("level0","level1","level2") ,return_value = "-2loglik", opp_mod_transfer = TRUE, control=list(trace = FALSE, parallelType=1,parVar = c("best_response")))

  exp2_TR_bayes[[id]] <- optim(c(0.1,0.1),fn=exp2_naive_bayes,gr = NULL, data=tdat,opp_strategy_vec = c("level0","level1","level2"),return_value = "-2loglik", opp_mod_transfer = TRUE,lower = c(0.01,0.01), upper = c(0.99,0.99), method="L-BFGS-B")
}
save(exp2_TR_bayes, file="exp2_TR_bayes.RData")

```

## Fitting Bayesian model with NO trnsfer 
```{r}

exp2_NT_bayes <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_NT_bayes[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
  
  # exp2_NT_bayes[[id]] <- DEoptim(fn=naive_bayes, lower = c(0.0,0.0), upper = c(1.0,1.0), data=tdat, opp_strategy_vec = c("level0","level1","level2"),return_value = "-2loglik", opp_mod_transfer = FALSE, control=list(trace = FALSE, parallelType=0,parVar = c("best_response")))

  exp2_NT_bayes[[id]] <- optim(c(0.1,0.1),fn=exp2_naive_bayes,gr = NULL, data=tdat,opp_strategy_vec = c("level0","level1","level2"),return_value = "-2loglik", opp_mod_transfer = FALSE, lower = c(0.01,0.01), upper = c(0.99,0.99), method="L-BFGS-B")
}

save(exp2_NT_bayes,file="exp2_NT_bayes.RData")
```


## Fitting Q_learning with state space of possible last round plays 
```{r}
exp2_QL_states <- function(par,data,return_value,gamma){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]

  #gamma <- 0.9 

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("left","center","right"), c("left","center","right"))
  states_SHOOT <- paste0(G3$Var1,G3$Var2)
  
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))

  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  
  Q_vals_SHOOT = matrix(-0.5,9,3)
  dimnames(Q_vals_SHOOT) = list(states_SHOOT, c("left","center","right"))
 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    nopts <- 3
    
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, shootout = states_SHOOT)
      lik_hum[t] <- 1/nopts
      # Randomly select state and previous actions for first round
      curr_state <- sample(state_vec, size = 1)
      h_act <- sample(colnames(Q_vals), size =1)
      ai_act <- sample(colnames(Q_vals), size =1)
      reward <- 0 
      
    } else {
      # Get past human action and associated reward 
      h_act <- as.character(data[t-1,"h_action"])
      ai_act <- as.character(data[t-1,"a_action"])
      reward <- as.numeric(data[t-1,"score"])
    }
    
    
    # cat(curr_state, " This is the previous state", "\n")
    new_state <- paste0(h_act,ai_act)
    # cat(new_state,"This is new state","\n")
    
    # Q_learning: update rule (time  = t-1)
    Q_vals[curr_state, h_act] <- Q_vals[curr_state, h_act] + alpha*( reward + gamma*max(Q_vals[new_state,]) - Q_vals[curr_state, h_act] )
    
    # Assume human chooses action probabilistically using softmax on Q values
    probs <- exp(Q_vals[new_state,]/beta)/sum(exp(Q_vals[new_state,]/beta))
    #if (data[t,"round"] == 20) { cat(probs,"\n") }
    
    # Get actual human action and compute likelihood
    h_act <- as.character(data[t,"h_action"])
    act_index <- match(h_act, colnames(Q_vals))
    lik_hum[t] <- probs[[act_index]]
      
    # Update state
    curr_state <- new_state 
    #}
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}



```

```{r}
exp2_QL_states(c(5,0.4),test_data, "-2loglik", 0.0)
```

```{r}
exp2_QLS_modelling <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_QLS_modelling[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
  # QL_states_modelling[[id]] <- optim(c(1,0.1),fn=Q_learn_states,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   exp2_QLS_modelling[[id]] <- DEoptim(fn=exp2_QL_states,lower = c(0,0), upper = c(20,1), data=tdat,"-2loglik", gamma = 0, control=list(trace = FALSE, parallelType=1))
}

save(exp2_QLS_modelling, file="exp2_QLS_modelling.RData")
```

```{r}
exp2_EWA_self <- function(par,data,return_value){

  lambda <- par[1]
  # Initiate N(0) = 1 as in Camerer and Ho 1997 paper. 

  
  # Define attraction vectors for each game 
  A_RPS = matrix(0.0,3)
  names(A_RPS) <- c("R","P","S")
  A_FWG = matrix(0.0,3)
  names(A_FWG) <- c("F","W","G")
  A_SHOOT = matrix(0.0,3)
  names(A_SHOOT) <- c("left","center","right")
  
  # # Define reward matrices from the prospective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_SHOOT <- t(matrix(c(-1,1,1,1,-1,1,1,1,-1),nrow=3, ncol=3))
  dimnames(reward_SHOOT) = list(c("left","center","right"), c("left","center","right"))
  
  # Initiate likelihood by trial vector 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Att <- switch(as.character(t_game), rps=A_RPS, fwg = A_FWG, shootout = A_SHOOT)
      reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, shootout = reward_SHOOT)
      game_data <- subset(data, game == t_game)
      lik_hum[t] <- 1/nopts
      N <- 1.0

      R_t <- rep(0.0,nopts)
      H_t <- R_t
      
    } else {
      
      indx <- data[t,"round"]
      # Get reward and past human action
      h_act_prev <- as.character(game_data[indx-1,"h_action"])
      a_act_prev <- as.character(game_data[indx-1,"a_action"])

      # Estimate phi(t)
      R_t <- as.numeric(names(Att) == a_act_prev)
      # cat("This is R(t)",R_t,"\n")
      
      H_t <- (H_t*(indx-2) + R_t)/(indx-1)
      # cat("this is H(t)",H_t,"\n")
      
      Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
      # cat("This is Phit(t)",Phi_t,"\t")
      
      # Estimate vector Delta(t)

      delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
      #cat("this is delta(t)",delta_t,"\n")
      
      for (i in 1:length(Att)) {
        action <- as.character(names(Att)[i])
        # cat("this is current strat",action,"\n")
        # cat("this is previous humna action",h_act_prev,"\n")
        
        # Attraction vector update rule 
        Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
      }
      #Update the value of N 
      N <- Phi_t*N + 1
      #cat(Att,'\n')
      
      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att)/sum(exp(lambda*Att))
      
      # Get actual human action and compute likelihood
      h_act <- as.character(game_data[indx,"h_action"])
      act_index <- match(h_act, names(Att))
      lik_hum[t] <- probs[[act_index]]
    }
  }
  
 if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
}

# exp2_EWA_self(1,test_data,"-2loglik")

```

## Fitting SEWA 
```{r}

exp2_SEWA_modelling <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_SEWA_modelling[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
#   exp2_SEWA_modelling[[id]] <- optim(10.0,fn=exp2_EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  exp2_SEWA_modelling[[id]] <- DEoptim(fn=exp2_EWA_self, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}

save(exp2_SEWA_modelling,file="exp2_SEWA_modelling.RData")
```

## MBM in experiment 2  
```{r}
MBM_2 <- function(par,data,return_value,gamma){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]

  #gamma <- 0.9 

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("left","right","center"), c("left","right","center"))
  states_SHOOT <- paste0(G3$Var1,G3$Var2)
  
  # Matrices to store Q_values in each state 
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))
  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  Q_vals_SHOOT = matrix(-0.5,9,3)
  dimnames(Q_vals_SHOOT) = list(states_SHOOT, c("left","right","center"))
  
  # Transition Matrices 
  Transit_RPS = matrix(0,9,9)
  dimnames(Transit_RPS) = list(states_RPS, states_RPS)
  Transit_FWG = matrix(0,9,9)
  dimnames(Transit_FWG) = list(states_FWG, states_FWG)
  Transit_SHOOT = matrix(0,9,9)
  dimnames(Transit_SHOOT) = list(states_SHOOT, states_SHOOT)
  
  
  lik_hum <- matrix(0.0,nrow(data))
  nopts <- 3

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, shootout = states_SHOOT)
      Transit <- switch(as.character(t_game),rps=Transit_RPS,fwg = Transit_FWG, shootout = Transit_SHOOT)
      lik_hum[t] <- 1/nopts
      
      # Randomly select prev_state and actions for first round
      curr_state <- sample(state_vec, size = 1)
      h_act <- sample(colnames(Q_vals), size =1)
      ai_act <- sample(colnames(Q_vals), size =1)
      reward <- 0 
      
    } else {
      # Get past human action and associated reward 
      h_act <- as.character(data[t-1,"h_action"])
      ai_act <- as.character(data[t-1,"a_action"])
      reward <- as.numeric(data[t-1,"score"])
    }
    
    
    # cat(curr_state, " This is the previous state", "\n")
    new_state <- paste0(h_act,ai_act)
    # cat(new_state,"This is new state","\n")
    
    
    # Update Transit matrix ....first create hot vector = 1 if new state, 0 otherwise 
    hot_vector <- state_vec == new_state
  
    
    # Transit probs update: TD learning on transition matrix, learning rate fixed at 0.1
    Transit[curr_state,] <- Transit[curr_state,] + 0.1* (hot_vector - Transit[curr_state,])
    #cat(Transit[curr_state,], "\n")
    
    # Update Q-values as current reward + a weighted (by trasnsit probs) average of future Q-values.
    Q_Row_max <- apply(Q_vals, 1, max, na.rm = TRUE)
    #cat(Transit[curr_state,] * Q_Row_max, "\n")
    Q_vals[curr_state, h_act] <- reward +  gamma*( Transit[curr_state,] %*% Q_Row_max  ) 
    
    
    # Assume human chooses action probabilistically using softmax on Q values
    probs <- exp(Q_vals[new_state,]/beta)/sum(exp(Q_vals[new_state,]/beta))
    #if (data[t,"round"] == 50) { cat(Q_vals,"\n") }
    
    # Get actual human action and compute likelihood
    h_act <- as.character(data[t,"h_action"])
    act_index <- match(h_act, colnames(Q_vals))
    lik_hum[t] <- probs[[act_index]]
      
    # Update state
    curr_state<- new_state 
    #}
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}


```

```{r}

#MBM_2(c(2, 0.1), test_data,"-2loglik", gamma = 0.9 )

MBM2_modelling <- list()
for(id in unique(dat$human_id)) {
  MBM_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  # MBM_modelling[[id]] <- optim(c(1,0.1),fn=QMBM,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   MBM2_modelling[[id]] <- DEoptim(fn=MBM_2,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0.9, control=list(trace = FALSE,parallelType=1))
}

save(MBM2_modelling, file="MBM2_modelling.RData")

```

## Putting all results together 
```{r}

# Load various modelling tables 

load("exp2_TR_bayes.RData")
load("exp2_NT_bayes.RData")
load("exp2_QLS_modelling.RData")
load("exp2_SEWA_modelling.RData")
load("MBM2_modelling.RData")




exp2_results <- data.frame()
for(id in unique(dat_exp2$human_id)) {
exp2_results <- rbind(exp2_results,
                       data.frame(
                         "ID" = id,
                         "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
                         "Random_BIC" = -2*(180*log(1/3)),

                         # Bayesian updating with/without transfer
                         "Bayes_Tr_BIC" = exp2_TR_bayes[[id]]$value + 2*log(180),
                         "Bayes_No_Tr_BIC" = exp2_NT_bayes[[id]]$value+ 2*log(180),
                         # Theta is the parameter governing AI stochasticity. Truth is 0.9
                         "theta_transfer" = exp2_TR_bayes[[id]]$par[1],
                         "theta_no_transfer" = exp2_NT_bayes[[id]]$par[1],
                         # Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
                         "eps_transfer" = exp2_TR_bayes[[id]]$par[2],
                         "eps_no_transfer" = exp2_NT_bayes[[id]]$par[2],


                         # Q-learning with last round states
                         "QL_states_BIC" = exp2_QLS_modelling[[id]]$optim$bestval + 2*log(180),
                         # beta ->  inverse temperature parameter in softmax choice function
                         "QL_states_Beta" = exp2_QLS_modelling[[id]]$optim$bestmem[1],
                         # alpha -> learning rate in QL update
                         "QL_states_alpha" = exp2_QLS_modelling[[id]]$optim$bestmem[2],
                         
                         # Self-Tuning EWA (only 1 parameter)
                          # BIC
                         "EWA_self_2LL" = exp2_SEWA_modelling[[id]]$optim$bestval,
                         "EWA_self_BIC" = exp2_SEWA_modelling[[id]]$optim$bestval + 1*log(180),
                         #Lambda is a parameter of the softmax choice function (inverse Temperature)
                         "EWA_self_Lambda" = exp2_SEWA_modelling[[id]]$optim$bestmem[1],
                         
                         # Model Based Model
                          # BIC
                         "MBM_2LL" = MBM2_modelling[[id]]$optim$bestval,
                         "MBM2_BIC" = MBM2_modelling[[id]]$optim$bestval + 2*log(180),
                          # Alpha is learning rate, beta is temperature in softmax
                         "MBM2_alpha" = MBM2_modelling[[id]]$optim$bestmem[1],
                         "MBM2_beta" = MBM2_modelling[[id]]$optim$bestmem[2]

                         ))
}

write.csv(exp2_results,file="exp2_results.csv",row.names = FALSE)

```

```{r}
exp2_table_results <- table(exp2_results[, "condition"],c("random","Bayes Tr","Bayes No Tr", "QL_states","S_EWA")[apply(exp2_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","EWA_self_BIC")],1,which.min)])

 write.csv(exp2_table_results,file="exp2_table_results.csv",row.names = TRUE)
 kable(exp2_table_results)
```

```{r}
exp2_model_comp <- data.frame()
for(id in unique(dat_exp2$human_id)) {
  tdat <- subset(dat_exp2,human_id == id)
  tot_score <- sum(tdat$score)
  tot_time <- sum(tdat$human_rt)
  early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg"))
  #early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg" | game =="numbers") )
  tr_score <- sum(early_dat$score)
  id_results <- subset(exp2_results, ID == id)
  min_BIC <- apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","EWA_self_BIC")],1,min)
  
  best_model <- c("random","Bayes Tr","Bayes NT", "QL_states","S_EWA")[apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","EWA_self_BIC")],1,which.min)]
  # 
 exp2_model_comp <- rbind(exp2_model_comp ,
                       data.frame(
                         "human_id" = id,
                         "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
                         "Early_game_score" = tr_score,
                         "Total_score" = tot_score,
                         "Best_model" = best_model,
                         "Total_time" = sum(tdat$human_rt),
                         "TR_minus_NT_BIC" = id_results[,"Bayes_Tr_BIC"] - id_results[,"Bayes_No_Tr_BIC"],
                         "Rand_minus_best_BIC" =  id_results[,"Random_BIC"] - min_BIC

                       ))
}

write.csv(exp2_model_comp,file="exp2_model_comp.csv",row.names = FALSE)
```

# Correlate difference between BICs of Bayes transfer and no transfer with early rounds score (evidence for transfer) 
```{r}

cor.test(exp2_model_comp$TR_minus_NT_BIC, exp2_model_comp$Early_game_score, method="spearman")

```


#Correlation between early game score and difference between best model and random BIC
```{r}
cor.test(exp2_model_comp$Rand_minus_best_BIC, exp2_model_comp$Early_game_score, method="spearman")

```

# Histogram of best fitting models 
```{r}
barplot(table(exp2_model_comp$Best_model))
```


## Let's compare total scores of each participant by the model of best fit, see if Bayes+transfer total scores are higher than Bayes + no transfer  

```{r}

# ggboxplot(exp2_model_comp, x = "model", y = "Total_score",
#           color = "model", palette = c("#00AFBB", "#E7B800", "#FC4E07"),
#           order = c("Random", "No Transfer", "Transfer"),
#           ylab = "Total Score", xlab = "Model with best fit")

exp2_model_comp$model <- recode(exp2_model_comp$Best_model,"Bayes Tr" = "Tranfer","Bayes NT" = "No Transfer", "QL_states" = "Q_Learning",  .default = "Random")
model <- factor(exp2_model_comp$model)
condition <- factor(exp2_model_comp$condition)

# Total score by best predictive model 
tapply(exp2_model_comp$Total_score, model, mean)

# Compute the analysis of variance
res.aov <- aov(Total_score ~ model + condition, data = exp2_model_comp)
# Summary of the analysis
summary(res.aov)
TukeyHSD(res.aov)
```

# Total time by best predictive model
```{r}
tapply(exp2_model_comp$Total_time, model, mean)

time.aov <- aov(Total_time ~ model, data = exp2_model_comp)
# Summary of the analysis
summary(time.aov)
TukeyHSD(time.aov)

``` 
