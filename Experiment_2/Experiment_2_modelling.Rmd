---
title: "Experiment_2_modelling"
output: html_document
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)

```

```{r}
library(tidyr)
library(dplyr)
library(DEoptim)
library(optimParallel)
library(ggpubr)

# dat_exp2 <- read.csv("MyData.csv")
# 
# # transform 'winner' variable in numeric score
# dat_exp2$score <- recode(dat_exp2$winner, human = 1, tie = 0, AI = -1)
# 
# # Create a new variable stage.f as a factor for the stages 1,2 ,3 4 in game
# dat_exp2$stage.f <- factor(dat_exp2$stage, labels = c("Stage 1","Stage 2","Stage 3", "Stage 4"),levels=c(1,2,3,4))
# 
# # create a new variable "game.f" as a factor variable of games
# dat_exp2$game.f <- factor(dat_exp2$game, labels = c("RPS","FWG","SHOOT"),levels=c("rps","fwg","shootout"))
# 
# # recode actions to make them equal to the codes in these files
# dat_exp2$h_action <- recode(dat_exp2$human_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G")
# dat_exp2$a_action <- recode(dat_exp2$ai_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G")
# 
# write.csv(dat_exp2, row.names=FALSE,file="data_exp2.csv")

```

```{r}
# Read in cleaned data file
dat_exp2 <- read.csv("data_exp2.csv")
test_data = subset(dat_exp2,human_id == "bZg1T3MNoItL6b7qAAAj")
```

```{r}
# Function to produce best reponse to ai_action in exp2
exp2_best_response <- function(game,ai_action){
  if (game == "rps"){
    if (ai_action == "R") {return("P")}
    else if (ai_action == "P") {return("S")}
    else if (ai_action == "S") {return("R")}
    
  } else if (game == "fwg"){ 
    if (ai_action == "F") {return("W")}
    else if (ai_action == "W") {return("G")}
    else if (ai_action == "G") {return("F")}
    
  } else if (game == "shootout") {
    if (ai_action == "left") {return("center/right")}
    else if (ai_action == "center") {return("left/right")}
    else if (ai_action == "right") {return("left/center")}
    else if (ai_action == "left/right") {return("center")}
    else if (ai_action == "left/center") {return("right")}
    else if (ai_action == "center/right") {return("left")}
  }
}
```


## Fitting Q_learning with state space of possible last round plays 
```{r, cache=TRUE}
# exp2_QL_states <- function(par,data,return_value,gamma, within_transfer = FALSE){
#   # Par[1] -> beta= inverse temperature parameter in softmax choice function
#   # Par[2] -> lambda = learning rate (one per game?)
#   # data : participant and ai choice data.
#   # return_value=c("-2loglik","likelihood_by_trial")
#   # Gamma is discount factor for future rewards 
#   # Returns Q-values per trial and predicts choice using softmax
#   beta <- par[1]
#   alpha <- par[2]
# 
#   #gamma <- 0.9 
# 
#   #Define matrix of state spaces for each game 
#   G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
#   states_RPS <- paste0(G1$Var1,G1$Var2)
# 
#   G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
#   states_FWG <- paste0(G2$Var1,G2$Var2)
# 
#   G3 <- expand.grid(c("left","center","right"), c("left","center","right"))
#   states_SHOOT <- paste0(G3$Var1,G3$Var2)
#   
#   Q_vals_RPS = matrix(-0.5,9,3)
#   dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))
# 
#   Q_vals_FWG = matrix(-0.5,9,3)
#   dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
#   
#   Q_vals_SHOOT = matrix(-0.5,9,3)
#   dimnames(Q_vals_SHOOT) = list(states_SHOOT, c("left","center","right"))
#  
#   lik_hum <- matrix(0.0,nrow(data))
# 
#   for(t in 1:nrow(data)) {
#     t_game <- data[t,"game"]
#     nopts <- 3
#     
#     if(data[t,"round"] == 1) {
#       
#       # Within game transfer Code 
#       if (!within_transfer){
#               Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
#       } else {
#         if (data[t,"stage"] == 1){
#           Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
#         } else if (data[t,"stage"] == 2) {
#           Q_vals_opp1 <- Q_vals
#           Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
#         } else if (data[t,"stage"] == 3) {
#           Q_vals_opp2 <- Q_vals 
#           Q_vals <- Q_vals_opp1
#         } else if (data[t,"stage"] == 4) {
#           Q_vals <- Q_vals_opp2
#         }
#       }
#    
# 
#       #cat(Q_vals,"\n") 
#       state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, shootout = states_SHOOT)
#       # first round is uniform prediction
#       lik_hum[t] <- 1/nopts
#       # Randomly select state and previous actions for first round
#       curr_state <- sample(state_vec, size = 1)
#       h_act <- sample(colnames(Q_vals), size =1)
#       ai_act <- sample(colnames(Q_vals), size =1)
#       reward <- 0 
#       
#     } else {
#       # Get past human action and associated reward 
#       h_act <- as.character(data[t-1,"h_action"])
#       ai_act <- as.character(data[t-1,"a_action"])
#       reward <- as.numeric(data[t-1,"score"])
#     }
#     
#     
#     # cat(curr_state, " This is the previous state", "\n")
#     new_state <- paste0(h_act,ai_act)
#     # cat(new_state,"This is new state","\n")
#     
#     # Q_learning: update rule (time  = t-1)
#     Q_vals[curr_state, h_act] <- Q_vals[curr_state, h_act] + alpha*( reward + gamma*max(Q_vals[new_state,]) - Q_vals[curr_state, h_act] )
#     
#     # Assume human chooses action probabilistically using softmax on Q values
#     probs <- exp(Q_vals[new_state,]/beta)/sum(exp(Q_vals[new_state,]/beta))
#     #if (data[t,"round"] == 1) { cat(probs,"\n") }
#     
#     # Get actual human action and compute likelihood
#     h_act <- as.character(data[t,"h_action"])
#     act_index <- match(h_act, colnames(Q_vals))
#     lik_hum[t] <- probs[[act_index]]
#       
#     # Update state
#     curr_state <- new_state 
#     #}
#   }
#   if(return_value == "-2loglik") {
#     ret <- -2*sum(log(lik_hum))
#     if(is.infinite(ret) || is.nan(ret)) {
#       return(1e+300)
#     } else {
#       return(ret)
#     }
#   }
#   if(return_value == "likelihood_by_trial") return(lik_hum)
#   
# }

```
## Attempt 2 : QL with states and option to activate transfer 
```{r, cache=TRUE}
exp2_QL_states <- function(par,data,return_value,gamma, within_transfer = FALSE){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]
  nopts <- 3
  
  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("left","center","right"), c("left","center","right"))
  states_SHOOT <- paste0(G3$Var1,G3$Var2)
  
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))

  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  
  Q_vals_SHOOT = matrix(-0.5,9,3)
  dimnames(Q_vals_SHOOT) = list(states_SHOOT, c("left","center","right"))
 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(data[t,"round"] == 1) {
      
      # Within game transfer Code 
      if (!within_transfer){
              Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
      } else {
        if (data[t,"stage"] == 1){
          Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
        } else if (data[t,"stage"] == 2) {
          Q_vals_opp1 <- Q_vals
          Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
        } else if (data[t,"stage"] == 3) {
          Q_vals_opp2 <- Q_vals 
          Q_vals <- Q_vals_opp1
        } else if (data[t,"stage"] == 4) {
          Q_vals <- Q_vals_opp2
        }
      }
   
      #cat(Q_vals,"\n") 
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, shootout = states_SHOOT)
      # first round is uniform prediction
      lik_hum[t] <- 1/nopts
      
    } else {
      # Get past human action and associated reward 
      h_act_prev <- as.character(data[t-1,"h_action"])
      ai_act_prev <- as.character(data[t-1,"a_action"])
      curr_state <- paste0(h_act_prev,ai_act_prev)
      
      h_act <- as.character(data[t,"h_action"])
      ai_act <- as.character(data[t,"a_action"])
      reward <- as.numeric(data[t,"score"])
      new_state <- paste0(h_act,ai_act)
      
      # Assume human chooses action probabilistically using softmax on Q values
      probs <- exp(Q_vals[curr_state,]/beta)/sum(exp(Q_vals[curr_state,]/beta))
      # Get actual human next action and compute likelihood
      act_index <- match(h_act, colnames(Q_vals))
      lik_hum[t] <- probs[[act_index]]
      
      # Q_learning: update rule (time = t)
      Q_vals[curr_state, h_act] <- Q_vals[curr_state, h_act] + alpha*( reward + gamma*max(Q_vals[new_state,]) - Q_vals[curr_state, h_act])
    }
    
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```


```{r}
exp2_QL_states(c(5,0.4),test_data, "likelihood_by_trial", 0.0, FALSE)
```

```{r, cache=TRUE}
exp2_QLS_modelling <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_QLS_modelling[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
  # QL_states_modelling[[id]] <- optim(c(1,0.1),fn=Q_learn_states,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   exp2_QLS_modelling[[id]] <- DEoptim(fn=exp2_QL_states,lower = c(0,0), upper = c(20,1), data=tdat,"-2loglik", gamma = 0, control=list(trace = FALSE, parallelType=1))
}

save(exp2_QLS_modelling, file="exp2_QLS_modelling.RData")
```

```{r, cache=TRUE}
exp2_QLS_within_Tr <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_QLS_within_Tr[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
  # QL_states_modelling[[id]] <- optim(c(1,0.1),fn=Q_learn_states,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   exp2_QLS_within_Tr[[id]] <- DEoptim(fn=exp2_QL_states,lower = c(0,0), upper = c(20,1), data=tdat,"-2loglik", gamma = 0, within_transfer =TRUE, control=list(trace = FALSE, parallelType=1))
}

save(exp2_QLS_within_Tr , file="exp2_QLS_within_Tr.RData")
```


##Self tuning EWA model (simple, no states)

```{r}
exp2_EWA_self <- function(par,data,return_value){

  lambda <- par[1]
  # Initiate N(0) = 1 as in Camerer and Ho 1997 paper. 

  
  # Define attraction vectors for each game 
  A_RPS = matrix(0.0,3)
  names(A_RPS) <- c("R","P","S")
  A_FWG = matrix(0.0,3)
  names(A_FWG) <- c("F","W","G")
  A_SHOOT = matrix(0.0,3)
  names(A_SHOOT) <- c("left","center","right")
  
  # # Define reward matrices from the prospective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_SHOOT <- t(matrix(c(-1,1,1,1,-1,1,1,1,-1),nrow=3, ncol=3))
  dimnames(reward_SHOOT) = list(c("left","center","right"), c("left","center","right"))
  
  # Initiate likelihood by trial vector 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    nopts <- 3
    # Resets at the beginning of each game and stage
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Att <- switch(as.character(t_game), rps=A_RPS, fwg = A_FWG, shootout = A_SHOOT)
      reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, shootout = reward_SHOOT)
      game_data <- subset(data, game == t_game)
      lik_hum[t] <- 1/nopts
      N <- 1.0

      # initialize H_t by looking at first action 
      a_act <- as.character(game_data[1,"a_action"])
      H_t <- as.numeric(colnames(Att) == a_act)
      
      
    } else {
      
      # rounds gets initialised at beginning of eahc stage so we can treat them as separate games and use
      # round as index 
      indx <- data[t,"round"]
      
      # Get reward and past human action
      h_act_prev <- as.character(game_data[indx-1,"h_action"])
      a_act_prev <- as.character(game_data[indx-1,"a_action"])
      curr_state <- paste0(h_act_prev,a_act_prev)
      
      h_act <- as.character(game_data[indx,"h_action"])
      a_act <- as.character(game_data[indx,"a_action"])
      new_state <- paste0(h_act,a_act)
      
      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att[curr_state,])/sum(exp(lambda*Att[curr_state,]))
      
      # Get actual human action and compute likelihood
      act_index <- match(h_act, colnames(Att))
      lik_hum[t] <- probs[[act_index]]
      
      # Update rule:
      # Estimate R(t) (recent history) then H(t) (history) then phi(t) (change detector)
      R_t <- as.numeric(colnames(Att) == a_act)
      #cat("This is R(t)",R_t,"\n")
      
      H_t <- (H_t*(indx-1) + R_t)/(indx)
      #cat("this is H(t)",H_t,"\n")
      
      Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
      #cat("This is Phit(t)",Phi_t,"\t")
      
      # Estimate vector Delta(t)
      delta_t <- as.numeric(reward[,a_act] >= as.numeric(game_data[indx,"score"]))
      #cat("this is delta(t)",delta_t,"\n")
      
      # This is a vectorised ST_EWA update rule, easier to follow the loop below in comments
      state_indx <- match(curr_state,state_vec)
      Att[state_indx,] <- (Phi_t*N*Att[state_indx,]  + ( delta_t + (1- delta_t)*(colnames(Att) == h_act))*as.numeric(reward[colnames(Att),a_act])) / (Phi_t*N + 1)
      
      ##############
      # for (i in 1:nopts) {
      #   action <- as.character(colnames(Att)[i])
      #   state_indx <- match(curr_state,state_vec)
      #   #cat("this is the current state index", state_indx, "\n")
      # 
      #   # Attraction vector update rule
      #   Att[state_indx,i] <- (Phi_t*N*Att[state_indx,i]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act))*as.numeric(reward[action,a_act])) / (Phi_t*N + 1)
      # 
      # }
      # 
      #cat("this is the current Att vector", Att, "\n")
      ###################
      
      #Update the value of N 
      N <- Phi_t*N + 1
      
      #Update the state 
      curr_state <- new_state 
    }
  }
  
 if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
}
```



## Fitting SEWA 
```{r, cache=TRUE}

exp2_SEWA_modelling <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_SEWA_modelling[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
#   exp2_SEWA_modelling[[id]] <- optim(10.0,fn=exp2_EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  exp2_SEWA_modelling[[id]] <- DEoptim(fn=exp2_EWA_self, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}

save(exp2_SEWA_modelling,file="exp2_SEWA_modelling.RData")
```


# Self-Tuning EWA model with states ( and possibly transfer) 

```{r}

exp2_STEWA_STATES <- function(par,data,return_value, transfer = TRUE){

  lambda <- par[1]
  nopts <- 3

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("left","center","right"), c("left","center","right"))    # Left, Center, Right 
  states_SHOOT <- paste0(G3$Var1,G3$Var2)
  
  A_RPS = matrix(-0.5,9,3)
  dimnames(A_RPS) = list(states_RPS, c("R", "P", "S"))

  A_FWG = matrix(-0.5,9,3)
  dimnames(A_FWG) = list(states_FWG, c("F", "W", "G"))
  
  A_SHOOT = matrix(-0.5,9,3)
  dimnames(A_SHOOT) = list(states_SHOOT, c("left","center","right"))

  
  # # Define reward matrices from the perspective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_SHOOT <- t(matrix(c(-1,1,1,1,-1,1,1,1,-1),nrow=3, ncol=3))
  dimnames(reward_SHOOT) = list(c("left","center","right"), c("left","center","right"))
  
  # Initiate likelihood by trial vector 
  lik_hum <- matrix(0.0,nrow(data))
  
   for(t in 1:nrow(data)) {
     # Get which game we are playing
     t_game <- data[t,"game"]
    
    if (data[t,"round"] == 1)  {
    
      # chance likelihood :we have no data on previous round.
      lik_hum[t] <- 1/nopts
      
      
      if (!transfer) {
        # switch and Reset attractions and reward matrices
        Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, shootout = A_SHOOT)
        state_vec <- switch(as.character(t_game), rps=states_RPS, fwg = states_FWG, shootout = states_SHOOT)
        reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, shootout = reward_SHOOT)
        
        # Get first ai action 
        a_act <- as.character(data[t,"a_action"])
        # initialize H_t by looking at first action
        H_t <- as.numeric(colnames(Att) == a_act)
        
        # Asumme N(0) = 1 for now...see discussion in paper
        N_t <- 1.0
        indx <- 1
      
        
      #Code for transfer , we're still in round 1 if loop  
      } else {
        
        if (data[t,"stage"] == 1) {

          # switch and Reset attractions and reward matrices
          Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, shootout = A_SHOOT)
          state_vec <- switch(as.character(t_game), rps=states_RPS, fwg = states_FWG, shootout = states_SHOOT)
          reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, shootout = reward_SHOOT)
          # Get first ai action 
          a_act <- as.character(data[t,"a_action"])
          # initialize H_t by looking at first action
          H_t <- as.numeric(colnames(Att) == a_act)
          #initialize N_t and index
          N_t <- 1.0
          indx <- 1
          
        #round 1 stage 2
        } else if (data[t,"stage"] == 2) {
          Att_opp_1 <- Att
          H_opp_1 <- H_t
          N_opp_1 <- N_t
          indx  <- 1
          #reset Attractions
          Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, shootout = A_SHOOT)
          
        # round 1 stage 3  
        } else if (data[t,"stage"] == 3) {
          # we're in end of stage 2, so keep attractions and counts for stage 4
          Att_opp_2 <- Att
          H_opp_2 <- H_t
          N_opp_2 <- N_t
          indx <- 20
          # Get attractions and counts from stage 1
          Att <- Att_opp_1
          H_t <- H_opp_1
          N_t <- N_opp_1
          
        } else if (data[t,"stage"] == 4) {
          # get attractions and counts from end of stage 2
          Att <- Att_opp_2
          H_t <- H_opp_2
          N_t <- N_opp_2
          indx <- 20
        }
      }

      # Rounds 2 to end 
    } else { 
      
      #cat("Attraction matrix", Att, "\n")
      
      # Get reward and past human action
      h_act_prev <- as.character(data[t-1,"h_action"])
      a_act_prev <- as.character(data[t-1,"a_action"])
      curr_state <- paste0(h_act_prev,a_act_prev)
      
      h_act <- as.character(data[t,"h_action"])
      a_act <- as.character(data[t,"a_action"])
      new_state <- paste0(h_act,a_act)
      
      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att[curr_state,])/sum(exp(lambda*Att[curr_state,]))
      
      # Get actual human action and compute likelihood
      act_index <- match(h_act, colnames(Att))
      lik_hum[t] <- probs[[act_index]]
      
      # Update rule:
      # Estimate R(t) (recent history) then H(t) (history) then phi(t) (change detector)
      R_t <- as.numeric(colnames(Att) == a_act)
      #cat("This is R(t)",R_t,"\n")
      
      H_t <- (H_t*(indx-1) + R_t)/(indx)
      #cat("this is H(t)",H_t,"\n")
      
      Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
      #cat("This is Phit(t)",Phi_t,"\t")
      
      # Estimate vector Delta(t)
      delta_t <- as.numeric(reward[,a_act] >= as.numeric(data[t,"score"]))
      #cat("this is delta(t)",delta_t,"\n")
      
      # This is a vectorised ST_EWA update rule, easier to follow the loop below in comments
      state_indx <- match(curr_state,state_vec)
      Att[state_indx,] <- (Phi_t*N_t*Att[state_indx,]  + ( delta_t + (1- delta_t)*(colnames(Att) == h_act))*as.numeric(reward[colnames(Att),a_act])) / (Phi_t*N_t + 1)
      
      ##############
      # for (i in 1:nopts) {
      #   action <- as.character(colnames(Att)[i])
      #   state_indx <- match(curr_state,state_vec)
      #   #cat("this is the current state index", state_indx, "\n")
      # 
      #   # Attraction vector update rule
      #   Att[state_indx,i] <- (Phi_t*N*Att[state_indx,i]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act))*as.numeric(reward[action,a_act])) / (Phi_t*N + 1)
      # 
      # }
      # 
      #cat("this is the current Att vector", Att, "\n")
      ###################
      
      #Update the value of N 
      N_t <- Phi_t*N_t + 1
      
      #Update the state 
      curr_state <- new_state 
      
    }
   }  
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```

```{r}
exp2_STEWA_STATES(1,test_data,"-2loglik", TRUE)

```
## Fitting ST_EWA_states WITHOUT transfer 
```{r, cache=TRUE}
# Set seed to random number to check you get same results
set.seed(43)

exp2_STEWA_NT <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_STEWA_NT[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
#   exp2_STEWA_NT[[id]] <- optim(10.0,fn=exp2_STEWA_STATES,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  exp2_STEWA_NT[[id]] <- DEoptim(fn=exp2_STEWA_STATES, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", FALSE, control=list(trace = FALSE,parallelType=1))
}

save(exp2_STEWA_NT,file="exp2_STEWA_NT.RData")
```

## Fitting ST_EWA_states WITH transfer 
```{r, cache=TRUE}

exp2_STEWA_Tr <- list()
for(id in unique(dat_exp2$human_id)) {
  exp2_STEWA_Tr[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
#   exp2_STEWA_Tr[[id]] <- optim(10.0,fn=exp2_STEWA_STATES,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  exp2_STEWA_Tr[[id]] <- DEoptim(fn=exp2_STEWA_STATES, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", TRUE, control=list(trace = FALSE,parallelType=1))
}

save(exp2_STEWA_Tr,file="exp2_STEWA_Tr.RData")
```



## MBM in experiment 2  
```{r}
MBM_2 <- function(par,data,return_value,gamma){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]

  #gamma <- 0.9 

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("left","right","center"), c("left","right","center"))
  states_SHOOT <- paste0(G3$Var1,G3$Var2)
  
  # Matrices to store Q_values in each state 
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))
  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  Q_vals_SHOOT = matrix(-0.5,9,3)
  dimnames(Q_vals_SHOOT) = list(states_SHOOT, c("left","right","center"))
  
  # Transition Matrices 
  Transit_RPS = matrix(0,9,9)
  dimnames(Transit_RPS) = list(states_RPS, states_RPS)
  Transit_FWG = matrix(0,9,9)
  dimnames(Transit_FWG) = list(states_FWG, states_FWG)
  Transit_SHOOT = matrix(0,9,9)
  dimnames(Transit_SHOOT) = list(states_SHOOT, states_SHOOT)
  
  
  lik_hum <- matrix(0.0,nrow(data))
  nopts <- 3

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, shootout = Q_vals_SHOOT)
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, shootout = states_SHOOT)
      Transit <- switch(as.character(t_game),rps=Transit_RPS,fwg = Transit_FWG, shootout = Transit_SHOOT)
      lik_hum[t] <- 1/nopts
      
      # Randomly select prev_state and actions for first round
      curr_state <- sample(state_vec, size = 1)
      h_act <- sample(colnames(Q_vals), size =1)
      ai_act <- sample(colnames(Q_vals), size =1)
      reward <- 0 
      
    } else {
      # Get past human action and associated reward 
      h_act <- as.character(data[t-1,"h_action"])
      ai_act <- as.character(data[t-1,"a_action"])
      reward <- as.numeric(data[t-1,"score"])
    }
    
    
    # cat(curr_state, " This is the previous state", "\n")
    new_state <- paste0(h_act,ai_act)
    # cat(new_state,"This is new state","\n")
    
    
    # Update Transit matrix ....first create hot vector = 1 if new state, 0 otherwise 
    hot_vector <- state_vec == new_state
  
    
    # Transit probs update: TD learning on transition matrix, learning rate fixed at 0.1
    Transit[curr_state,] <- Transit[curr_state,] + 0.1* (hot_vector - Transit[curr_state,])
    #cat(Transit[curr_state,], "\n")
    
    # Update Q-values as current reward + a weighted (by trasnsit probs) average of future Q-values.
    Q_Row_max <- apply(Q_vals, 1, max, na.rm = TRUE)
    #cat(Transit[curr_state,] * Q_Row_max, "\n")
    Q_vals[curr_state, h_act] <- reward +  gamma*( Transit[curr_state,] %*% Q_Row_max  ) 
    
    
    # Assume human chooses action probabilistically using softmax on Q values
    probs <- exp(Q_vals[new_state,]/beta)/sum(exp(Q_vals[new_state,]/beta))
    #if (data[t,"round"] == 50) { cat(Q_vals,"\n") }
    
    # Get actual human action and compute likelihood
    h_act <- as.character(data[t,"h_action"])
    act_index <- match(h_act, colnames(Q_vals))
    lik_hum[t] <- probs[[act_index]]
      
    # Update state
    curr_state<- new_state 
    #}
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}


```

```{r, cache=TRUE}

#MBM_2(c(2, 0.1), test_data,"-2loglik", gamma = 0.9 )

MBM2_modelling <- list()
for(id in unique(dat_exp2$human_id)) {
  MBM2_modelling[[id]] <- list()
  tdat <- subset(dat_exp2,human_id == id)
  # MBM_modelling[[id]] <- optim(c(1,0.1),fn=QMBM,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   MBM2_modelling[[id]] <- DEoptim(fn=MBM_2,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0.9, control=list(trace = FALSE,parallelType=1))
}

save(MBM2_modelling, file="MBM2_modelling.RData")

```

## Putting all results together 
```{r}

load("exp2_QLS_modelling.Rdata")
load("exp2_QLS_within_Tr.Rdata")
load("exp2_STEWA_NT.Rdata")
load("exp2_STEWA_Tr.Rdata")
load("MBM2_modelling.Rdata")




exp2_results <- data.frame()

for(id in unique(dat_exp2$human_id)) {
exp2_results <- rbind(exp2_results,
                       data.frame(
                         "ID" = id,
                         "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
                         "Random_BIC" = -2*(180*log(1/3)),

                         ########## MS results for Bayesian models
                          # Bayesian updating with between and within game transfer
                         "Distinct_game_BIC" = Bayes_distinct_game[[id]]$optim$bestval + 2*log(180),
                         "theta_transfer" = Bayes_distinct_game[[id]]$optim$bestmem[1],
                         "lambda_transfer" = Bayes_distinct_game[[id]]$optim$bestmem[2],

                          # Bayesian updating with within but no btwn game transfer
                         "Distinct_stage_BIC" = Bayes_distinct_stage[[id]]$optim$bestval + 2*log(180),
                         "theta_no_btw_tr" = Bayes_distinct_stage[[id]]$optim$bestmem[1],
                         "lambda_no_btw_tr" = Bayes_distinct_stage[[id]]$optim$bestmem[2],

                          #  No within or between game transfer
                         "Bayes_distinct_no" = Bayes_distinct_no[[id]]$optim$bestval + 2*log(180),
                         "theta_no_tr" =  Bayes_distinct_no[[id]]$optim$bestmem[1],
                         "lambda_no_tr" =  Bayes_distinct_no[[id]]$optim$bestmem[2],

                          # No distinction between players, transfer within and between
                         # "Bayes_same_game" = Bayes_same_game[[id]]$optim$bestval + 1*log(180),
                         # "theta_naive" = Bayes_same_game[[id]]$optim$bestmem[1],
                         #  ###########

                         ########### Q-learning with last round states (No within transfer)
                         "QL_states_BIC" = exp2_QLS_modelling[[id]]$optim$bestval + 2*log(180),
                         # beta ->  inverse temperature parameter in softmax choice function
                         "QL_states_Beta" = exp2_QLS_modelling[[id]]$optim$bestmem[1],
                         # alpha -> learning rate in QL update
                         "QL_states_alpha" = exp2_QLS_modelling[[id]]$optim$bestmem[2],
                         ############
                         
                         ############# QLS assuming within game transfer
                         "QLS_within_Tr_BIC" = exp2_QLS_within_Tr[[id]]$optim$bestval + 2*log(180),
                         # beta ->  inverse temperature parameter in softmax choice function
                         "QLS_within_Tr_Beta" = exp2_QLS_within_Tr[[id]]$optim$bestmem[1],
                         # alpha -> learning rate in QL update
                         "QLS_within_Tr_alpha" = exp2_QLS_within_Tr[[id]]$optim$bestmem[2],
                         #############
                         
                         ########## Self-Tuning EWA states NO transfer(only 1 parameter)
                          # BIC
                         "STEWA_NT_2LL" = exp2_STEWA_NT[[id]]$optim$bestval,
                         "STEWA_NT_BIC" = exp2_STEWA_NT[[id]]$optim$bestval + 1*log(180),
                         #Lambda is a parameter of the softmax choice function (inverse Temperature)
                         "STEWA_NT_Lambda" = exp2_STEWA_NT[[id]]$optim$bestmem[1],
                         ###########

                          ########## Self-Tuning EWA states WITH transfer (only 1 parameter)
                          # BIC
                         "STEWA_Tr_2LL" = exp2_STEWA_Tr[[id]]$optim$bestval,
                         "STEWA_Tr_BIC" = exp2_STEWA_Tr[[id]]$optim$bestval + 1*log(180),
                         #Lambda is a parameter of the softmax choice function (inverse Temperature)
                         "STEWA_Tr_Lambda" = exp2_STEWA_Tr[[id]]$optim$bestmem[1],
                         #############
                         
                         ########### Model Based Model
                          # BIC
                         "MBM_2LL" = MBM2_modelling[[id]]$optim$bestval,
                         "MBM2_BIC" = MBM2_modelling[[id]]$optim$bestval + 2*log(180),
                          # Alpha is learning rate, beta is temperature in softmax
                         "MBM2_alpha" = MBM2_modelling[[id]]$optim$bestmem[1],
                         "MBM2_beta" = MBM2_modelling[[id]]$optim$bestmem[2]

                         ))
}

write.csv(exp2_results,file="exp2_results.csv",row.names = FALSE)

```



```{r}
# WITH MS results for bayesian models 
# exp2_table_results <- table(exp2_results[, "condition"],c("random","TOM_BT","TOM_NBT", "TOM_NT","ToM_Naive", "QL_states","QLS_within","S_EWA","MBM")[apply(exp2_results[,c("Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no","Bayes_same_game","QL_states_BIC","QLS_within_Tr_BIC","EWA_self_BIC","MBM2_BIC")],1,which.min)])
# 
#  write.csv(exp2_table_results,file="exp2_table_results.csv",row.names = TRUE)
#  kable(exp2_table_results)
```

```{r}
exp2_model_comp <- data.frame()
for(id in unique(dat_exp2$human_id)) {
  tdat <- subset(dat_exp2,human_id == id)
  tot_score <- sum(tdat$score)
  tot_time <- sum(tdat$human_rt)
  early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg"))
  #early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg" | game =="numbers") )
  tr_score <- sum(early_dat$score)
  id_results <- subset(exp2_results, ID == id)

  # MS BAYES MODELS
  min_BIC <- apply(id_results[,c("Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no", "QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")],1,min)
  
  best_model <- c("random","ToM_BT","ToM_NBT", "ToM_NT","QL_states_NT","QLS_within_Tr","STEWA_NT","STEWA_Tr")[apply(id_results[,c("Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no","QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")],1,which.min)]
  
  exp2_model_comp <- rbind(exp2_model_comp ,
                       data.frame(
                         "human_id" = id,
                         "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
                         "Early_game_score" = tr_score,
                         "Total_score" = tot_score,
                         "Best_model_2" = best_model,
                         "Total_time" = sum(tdat$human_rt),
                         # "BT_minus_NBT_BIC" = id_results[,"Btwn_TR_BIC"] - id_results[,"No_Btwn_Tr_BIC"],
                         "BT_minus_NBT_BIC" = id_results[,"Distinct_game_BIC"] - id_results[,"Distinct_stage_BIC"],
                         "Rand_minus_best_BIC" =  id_results[,"Random_BIC"] - min_BIC

                       ))
}

write.csv(exp2_model_comp,file="exp2_model_comp.csv",row.names = FALSE)


```

## Calucalting BIC weights 
```{r}

exp2_comp_BICs <- exp2_results[c("ID","Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no", "QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")]

exp2_BIC_weights <- exp2_comp_BICs["ID"]
exp2_BIC_weights[,2:ncol(exp2_comp_BICs)] <- t(apply(exp2_comp_BICs[,2:ncol(exp2_comp_BICs)], 1, function(i) exp(-0.5*(i-min(i)) )))

exp2_BIC_weights[,-1] <- t(apply(exp2_BIC_weights[,-1], 1, function(i) round(i/sum(i),2)))
colnames(exp2_BIC_weights) <- c("ID", "random","ToM_BT","ToM_NBT", "ToM_NT","QL_states_NT","QLS_within_Tr","STEWA_NT","STEWA_Tr")

BIC_comp_table <- as.data.frame(t(colMeans(exp2_BIC_weights[,2:ncol(exp2_BIC_weights)])))

colnames(BIC_comp_table) <- c("random","ToM_BT","ToM_NBT", "ToM_NT","QL_states_NT","QLS_within_Tr","STEWA_NT","STEWA_Tr")
# 
BIC_comp_table

table(exp2_model_comp$Best_model_2)

```

# Correlate difference between BICs of Bayes transfer and no transfer with early rounds score (evidence for transfer) 
```{r}

cor.test(exp2_model_comp$BT_minus_NBT_BIC, exp2_model_comp$Early_game_score, method="spearman")

```


#Correlation between early game score and difference between best model and random BIC
```{r}
cor.test(exp2_model_comp$Rand_minus_best_BIC, exp2_model_comp$Early_game_score, method="spearman")

```

# Histogram of best fitting models 
```{r}
barplot(table(exp2_model_comp$Best_model))
```


## Let's compare total scores of each participant by the model of best fit, see if Bayes+transfer total scores are higher than Bayes + no transfer  

```{r}

# ggboxplot(exp2_model_comp, x = "model", y = "Total_score",
#           color = "model", palette = c("#00AFBB", "#E7B800", "#FC4E07"),
#           order = c("Random", "No Transfer", "Transfer"),
#           ylab = "Total Score", xlab = "Model with best fit")

# exp2_model_comp$model <- recode(exp2_model_comp$Best_model,"ToM_BT" = "btwn Tranfer","ToM_NBT" = "No btwn Transfer", "ToM_NT" = "No within or btwn Tr","QL_states" = "Q_Learning",  .default = "Random")

exp2_model_comp$model <- recode(exp2_model_comp$Best_model,"ToM_BT" = "btwn Tranfer","ToM_NBT" = "No btwn Transfer", "ToM_NT" = "No within or btwn Tr","QL_states" = "Q_Learning",  .default = "Random")


model <- factor(exp2_model_comp$model)
condition <- factor(exp2_model_comp$condition)

# Total score by best predictive model 
tapply(exp2_model_comp$Total_score, model, mean)

# Compute the analysis of variance
res.aov <- aov(Total_score ~ model + condition, data = exp2_model_comp)
# Summary of the analysis
summary(res.aov)
TukeyHSD(res.aov)
```

# Total time by best predictive model
```{r}
tapply(exp2_model_comp$Total_time, model, mean)

time.aov <- aov(Total_time ~ model, data = exp2_model_comp)
# Summary of the analysis
summary(time.aov)
TukeyHSD(time.aov)

``` 
