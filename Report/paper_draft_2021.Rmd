---
title             : "Transfer of Learned Opponent Models in Zero Sum Games"
shorttitle        : "Upgrade Report"

author: 
  - name          : "Ismail Guennouni"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Department of Experimental Psychology, University College London, 26 Bedford Way, London WC1H 0AP, United Kingdom"
    email         : "i.guennouni.17@ucl.ac.uk"
  - name          : "Maarten Speekenbrink"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Department of Experimental Psychology, University College London"

# author_note: |
  #<!-- Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line. -->

  #Enter author note here.

abstract: |
  Human learning transfer abilities take advantage of important cognitive building blocks such as an abstract representation of concepts underlying tasks and 
  causal models of the environment. One way to build abstract representations of the environment when the task involves interactions with others is to build 
  a model of the opponent that may inform what actions they are likely to take next. In this study, we explore opponent modelling and its transfer with the use of
  computer agents with human-like limited degrees of iterated reasoning. In two experiments, we find that participants
  deviate from Nash equilibrium play and learn to adapt to the opponent's strategy and exploit it. Moreover, we show that participants transfer their learning to new games and that
  this transfer is moderated by the level of sophistication of the opponent. Computational modelling shows that players start each game with a model-based
  learning strategy that facilitates between-game transfer of the opponent's strategy, but then switch to behaviour that is consistent with a model-free learning strategy in the latter stages of the interaction.
  
# keywords          : "keywords"
# wordcount         : "X"

bibliography      : ["Mendeley2.bib"]
csl               : "apa-6th-edition.csl"

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
linkcolor         : "blue"
mask              : no
draft             : no
fig_caption       : yes


documentclass     : "apa6"
# classoption       : a4paper
classoption       : man
output            : papaja::apa6_pdf
header-includes:
  - \shorttitle{}
---

```{r load-packages, include = FALSE}
library(papaja)
library(kableExtra)
require(knitr)
require(citr)
require(bookdown)

# using some functions dplyr, ggpubr, PairedData and sjPlot. Need to be loaded. 
library(plyr)
library(tidyr)
library(dplyr)
library(MASS)
library(ggpubr)
library(afex)
library(PairedData)
library(multcompView)
library(lsmeans)
library(magick)
library(depmixS4)

```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)

```


```{r, include=FALSE} 
options(tinytex.verbose = TRUE)
#add_wordcount_filter(NULL, error = FALSE)
```



# Introduction

Being able to transfer previously acquired knowledge to a new domain is one of the hallmarks of human intelligence. This ability relies on important cognitive building blocks, such as an abstract representation of concepts underlying tasks [@Lake2017]. One way to form these representations when the task involves interactions with others, is to build a model of the person we are interacting with that offers predictions of the actions they are likely to take next. There is evidence that people learn such models of their opponents when playing repeated economic games [@stahl1995players]. 

A model of the opponent can help increase performance in a particular game, but learning more general characteristics of an opponent may also help increase performance in other games. In this paper, we are specifically interested in the latter: How do people build and use models of their opponent to facilitate learning transfer? <!-- when engaged in situations involving an interaction with strategic considerations.--> Repeated games, in which players interact repeatedly with the same opponent and have the ability to learn about the opponent's strategies and preferences [@mertens1990repeated] are particularly useful to address this question. The early literature on learning transfer in repeated games has mostly focused on the proportion of people who play normatively optimal (e.g. Nash equilibrium play) or use salient actions (e.g. risk dominance) in later games, having had experience with a similar game environment previously [@ho1998iterated; @knez2000]. As is well-known, a Nash equilibrium means that all players in a game play such that no-one can unilaterally improve their performance by deviating from their strategy. When playing against an opponent with a Nash-optimal strategy, I can do no better than play according to the Nash-equilibrium strategy as well. However, when faced with a player who deviates from the Nash-optimal strategy, it may be better for me to exploit this and also deviate from this strategy, increasing my performance beyond what is expected at a Nash equilibrium. Of course, this comes with some risk, as my own deviation from Nash-optimal play may leave myself open to similar exploitation. 

<!-- his doesn't allow for the possibility of learning about the opponent's strategy and potentially exploiting it. !-->

Studies that focused on whether people can learn to exploit deviations from Nash-equilibrium play have mostly looked at the ability of players to detect and exploit action-based learning rules [@spiliopoulos2013strategic; @shachat2004we]. These studies used computer opponents with fixed strategies (no adaptation to their human opponent) consisting of playing each action with a fixed probability (a mixed strategy). Findings showed .... However, the use of mixed strategies may have limited people's ability to form accurate opponent models. Rather than thinking of their opponents and drawing a random action from a (non-uniform) probability distribution, people may rather think of their opponents as applying a form of iterated reasoning to determine their next action. The type of reasoning we refer to takes the form of ``I believe that you believe that I believe ...'' and has been shown to correspond to people's casual introspection (REFERENCE). It also underlies successful models in behavioural economics, such as Level-$k$ and Cognitive Hierarchy models, which posit that people assume their opponent has a limited level of iterative reasoning, with the player best responding to their (modelled) opponent. 

In Level-$k$ theory, a level-0 player uses a fixed strategy without explicitly considering the strategy of their opponent. A level-1 player assumes their opponent is a level-0 player, and chooses actions to best respond to the strategy of their opponent, without considering what their opponent might believe that they will play. A level-2 player, on the other hand, takes their opponent's belief about their actions into account, assuming they face a level-1 player, and choosing actions to best respond to the actions of that player. Cognitive Hierarchy theory works on similar principles, but rather than assuming an opponent always adopts a particular level-$k$ strategy, they assume their opponent adopts each pf the level-$k$ strategies with a particular probability (i.e., the strategy is a mixture over pure level-$k$ strategies).

Iterative reasoniing strategies can explain non-equilibrium play in a range of games, such as ... In the present study, we endow our computer opponents with limited ability for iterative reasoning and assess whether (1) human players adapt their strategy to exploit this limited reasoning of their opponent, and (2) whether they are able to generalize a learned opponent model to other games. In two experiments, participants face the same opponent (Experiment 1) or same two opponents (Experiment 2) in three consecutive games: the well-known Rock-Paper-Scissors game, a structurally similar Fire-Water-Grass game, and a less similar Numbers (Experiment 1) or Shootout (Experiment 2) game. To foreshadow our results, we find evidence that participants transfer the learned strategy of their opponent to other games, but that this transfer is moderated by the sophistication of the opponent and the similarity of the games. Moreover, using computational modelling, we find evidence that participants switch from relying on an opponent model in the early stages of the games, to a morehabiltual and cognitively less demanding strategy in the later stages of the games.

<!--
with that were generally programmed not to change their strategies over the course of the task, allowing better experimental control. However, they have mostly looked at the ability of players to detect and exploit action-based learning rules [@spiliopoulos2013strategic; @shachat2004we]. The strategies implemented by the computer opponents had a style of play that was not "human-like" in the sense that humans are not very good at playing specific mixed strategies with precision, or at detecting patterns from long sequences of past play. Thus, in this study, we aim to explore opponent modelling and its transfer with the use of computer agents endowed with human-like limited degrees of iterated reasoning. The agents are either a level-1 or level-2 player, mimicking ``I know that you know that I know'' type reasoning, and the limited recursion depth they exhibit [@goodie2012levels]. A level 1 player adapts their play to what they believe their opponent will play, without considering what their opponent might believe they will play. A level 2 player, on the other hand, takes their opponent's belief about their actions into account, assuming they face a level 1 player, and choosing actions to beat the actions of that player. The choice of this type of strategy is also motivated by evidence that humans strategically use information from last round play of their opponents in zero sum games [@batzilis; @wang2014social]. 


We have conducted two experiments where participants interact with a computer opponent. In each experiment, we measure transfer of learning about the opponent's strategy between games with varying degrees of similarity. The first two games we use are structurally identical except for action labels. In the first experiment, the third game is strategically similar to the first two but descriptively different, while in a second experiment, we introduce a third game that is dissimilar to the first two in terms of payoff matrix and strategic structure. In the first experiment, participants face the same opponent throughout the three games, and the opponents are randomised to be either level-1 or level-2 players. In the second experiment, participants faced both level-1 and level-2 opponent sequentially, with the order in which they are faced randomised across participants.



<!-- Being able to transfer previously acquired knowledge to a new domain is one of the hallmarks of human intelligence. Humans are naturally endowed with the ability to extract relevant features from a situation, identify the presence of these features in a novel setting and use previously acquired knowledge to adapt to previously unseen challenges using acquired knowledge. More formally, @perkins1992transfer define transfer of learning as the application of skills, knowledge, and/or attitudes that were learned in one situation to another learning situation. This typically human skill has so far eluded modern AI agents. Deep neural networks for instance can do very well on image recognition tasks and can even reach super-human performance levels on video and strategic board games. Yet they struggle to learn as fast or as efficiently as humans do, and more importantly they have a very limited ability to generalize and transfer knowledge to new domains. @Lake2017 argue that human learning transfer abilities take advantage of important cognitive building blocks such as an abstract representation of concepts underlying tasks and compositionally structured causal models of the environment. -->

<!-- One way to build abstract representations of the environment when the task involves interactions with others is to build a model of the person we are interacting with that may inform what actions they are likely to take next. Once we learn something about them, we can use this knowledge to inform how to best behave in novel situations. This may lead to very efficient generalization of knowledge, even to situations that are dissimilar to the history of interaction, assuming what we have learned about others is an abstract representation that is not too dependent on the environment of the initial interaction. There is evidence that people learn models of their opponents when they play repeated economic games [@stahl1995players], engage in bilateral negotiations [@baarslag2016learning], or simply try to exploit a non random player in chance games such as Rock-Paper-Scissors [@de2012higher]. In this paper, we are specifically interested in the way in which people build and use models of their opponent to facilitate learning transfer, when engaged in situations involving an interaction with strategic considerations. These situations arise frequently such as in negotiations, auctions, strategic planning and all other domains in which theory of mind [@premack1978does] abilities play a role in determining human behaviour.  In order to explore learning transfer in these strategic settings, it is generally useful to study simple games as a model of more complex interactions. More specifically, we need a framework that allows the study of whether and how a player takes into consideration, over time, the impact of its current and future actions on the future actions of the opponent and the future cumulative rewards. Repeated games, in which players interact repeatedly with the same opponent and have the ability to learn about the opponent's strategies and preferences [@mertens1990repeated] are particularly adapted to the task of opponent modelling. -->

<!-- Early literature on learning transfer in games has mostly focused on measuring the proportion of people who play normatively optimal (Nash Equilibria) or salient actions (e.g Risk Dominance) in later games, having had experience with a similar game environment previously. For instance, @ho1998iterated measure transfer as the proportion of players who choose the Nash Equilibrium in later p-beauty contest games, after training on similar games. They find there is no evidence of immediate transfer (Nash equilibrium play in the first round of the new game) but positive structural learning transfer as shown by the faster convergence to equilibrium play by experienced vs non experienced players. @knez2000 test learning transfer in players exposed to two games with multiple equilibria sequentially and explore the ability of players to coordinate their actions to choose a particular equilibrium in subsequent games having reached it in prior ones. They distinguish between games that are similar in a purely descriptive way, meaning similar choice labels, identity of players, format and number of action choices; and games that are similar in a strategic sense, meaning similar payoffs from combination of actions, identical equilibrium properties or significant social characteristics of payoffs such as possibility of punishment, need for fairness and cooperative vs competitive settings. They find that transfer of learning (successful coordination) occurs more readily in the presence of both descriptive and strategic similarity. If the games were only strategically similar, then the transfer was much weaker.   -->

<!-- @Juvina2013 made a similar distinction between what they deemed surface and deep similarities and find that both contribute to positive learning transfer. However, they show that surface similarity is not necessary for deep transfer and can either aid or block this type of transfer depending on whether it leads to congruent or incongruent actions in later games. In a series of experiments using economic signalling games Cooper & Kagel [-@cooper2003lessons; -@Cooper2008] found that participants who have learned to play according to a Nash Equilibrium in one game can transfer this to subsequent games, even though the actions consistent with Nash Equilibrium in later games are different.  They show that this transfer is driven by the emergence of sophisticated players who are able to represent the strategic implications of their actions and reason about the consequences of a different payoff structure on an opponent's actions. -->


<!-- In exploring opponent modelling and learning transfer, most studies adopted two types of opponents in the experimental setting.  Either human participants were matched with other participants or they played against a computer algorithm. Computer opponents were generally programmed not to change their strategies over the course of the task, allowing better experimental control. One of the commonalities in studies of how humans adapt to computerised opponents is that they have mostly looked at the ability of players to detect and exploit action-based learning rules. The strategies implemented by the computer opponents had a style of play that was not "human-like" in the sense that humans are not very good at playing specific mixed strategies with any precision, or at detecting patterns from long sequences of past play due to cognitive constraints. It is therefore important to have agents that "play like humans", and one way of achieving that is to embed theses agents with human-like theory of mind abilities.  @simon1972theories explains that humans have limited cognitive capacities and as such cannot be expected to solve computationally complex problems such as finding Nash equilibria. Instead, they will try to 'satisfice' by choosing a strategy that is adequate in a simplified model of the environment, rather than an optimal one. This concept finds its natural application in 'level-k' theory, first adopted by @stahl1995players. It posits that deviations from Nash equilibrium solutions in simple games are explained by the fact that humans have a heterogeneous degree of strategic sophistication. At the bottom of the ladder, level-0 players are non-strategic and play either randomly or use a salient strategy in the game environment @arad201211. Level-1 players are next up the ladder of strategic sophistication and will assume all their opponents belong to the level-0 category and as such will best respond to them given this assumption. Likewise, a level-2 player will choose actions that are the best response given the belief that all opponents are exactly one level below, and so on.  -->

<!-- In this study, we propose to explore opponent modelling and its transfer with the use of computer agents possessing human-like theory of mind abilities with limited degrees of iterated reasoning. The agents will either be a level-1 or level-2 player, mimicking human theory of mind abilities and the limited recursion depth they exhibit [@goodie2012levels].  Our choice of using computer opponents instead of matching groups of participants makes it easier to disentangle the process of learning about the opponent from that of learning about the game structure and payoffs. When playing against other human opponents, players are learning about the game as well as trying to infer the potential dynamic strategy of the opponent simultaneously. Thus, it is harder to focus on an individual and how her strategies are changing and adapting to the opponent's play if we cannot experimentally control the behaviour of the opponent. The use of computer opponents to elicit learning behavior has been explored in the literature with encouraging results.  For instance, @spiliopoulos2013strategic made humans play constant sum games against 3 computer opponents, designed to take advantage of known patterns in human play such as imperfect randomization and heuristics use. He found that human participants do adapt to the opponent they are facing. @shachat2004we made human participants face computer opponents playing various mixed strategies in a zero-sum asymmetric matching pennies game. They found that the players changed their strategies towards exploiting the deviations of the opponent from the Mixed Strategy Nash Equilibrium (MSNE), and that this exploitation was very likely if the deviation from the MSNE play was high. -->


# Experiment 1

In the first experiment, we .... [intro to first exp]

## Methods


### Participants and Design
A total of 52 (28 female, 24 male) participants were recruited on the Prolific Academic platform. The mean age of participants was 31.2 years. Participants were paid a fixed fee of £2.5 plus a bonus dependent on their performance which averaged £1.06. The experiment used a 2 (computer opponent: level 1 or level 2) by 3 (games: rock-paper-scissors, fire-water-grass, numbers) design, with repeated measures on the second factor. Participants were randomly assigned to one of the two levels of the first factor. 

### Tasks
In the first experiment, participants played the three games against their computer opponent. These games were rock-paper-scissors, fire-water-grass, and the numbers game. A typical rock-paper-scissors game (hereafter RPS) is a 3x3 zero sum game, with a cyclical hierarchy between the two player's actions: rock blunts scissors, paper wraps rock, and scissors cut paper. If one player chooses an action which dominates their opponent's action, the player wins (receives a reward of 1) and the other player loses (receives a reward of -1). Otherwise it is a draw and both players receive a reward of 0. RPS has a unique mixed-strategy Nash equilibrium, which consists of each player in each round randomly selecting from the three options with uniform probability. The Fire-Water-Grass (FWG) game is identical to RPS in all but action labels: Fire burns grass, water extinguishes fire, and grass absorbs water. We use this game as we are interested in whether learning is transferred in a fundamentally similar game where the only difference is in the name of the possible actions. This should make it relatively easy to generalize knowledge of the opponent's strategy, provided this knowledge is on a sufficiently abstract level, such as knowing the opponent is a level-1 or level-2 player. Crucially, learning simple contingencies such as ``If I played Rock on the previous round, playing Scissors next will likely result in a win'', <!-- as might be learned by a simple reinforcement learning algorithm,--> is not generalizable to this similar game, as these contingencies are tied to the labels of the actions. The Numbers game is a generalization of RPS. In the variant we use, 2 participants concurrently pick a number between 1 and 5. To win in this game, a participant needs to pick a number exactly 1 higher than the number chosen by their opponent. For example, if a participant thinks their opponent will pick 3, they ought to choose 4 to win the round. To make the strategies cyclical as in RPS, the game stipulates that the lowest number (1) beats the highest number (5), so if the participant thinks the opponent will play 5, then the winning choice is to pick 1. This game has a structure similar to RPS in which every action is dominated by exactly one other action. All other possible combinations of choices are considered ties. Similar to RPS and FWG, the mixed-strategy Nash equilibrium is to randomly play each action with equal probability.

The computer opponent was programmed to use either a level-1 or level-2 strategy in all the games. A level-1 player is defined as a player who best responds to a level-0 player. A level-0 player plays in a non-strategic way and does not consider their opponent's actions. Here, we assume a level-0 player simply repeats their previous action. There are other ways to define a level-0 player. For instance, as repeating their action if it resulted in a win and choosing randomly from the remaining actions otherwise, or choosing randomly from all actions. As a best response to a uniformly random action is itself a random action, defining a level-0 player in such a way would make a level-1 opponent's strategy much harder to discern. Because we are mainly interested in generalization of knowledge of an opponent's strategy to other games, which rests on good knowledge of this strategy, we opted for this more deterministic formulation of a level-0 player (whilst also introducing some randomness in the computer opponent's play). A level-2 computer opponent, will assume in turn that the participant is a level-1 opponent, playing according to the strategy just described. We also introduced some noise over the actions of computer opponents making them play randomly in 10\% of all trials. Note that at all levels, the strategies are contingent on the actions taken in the previous round. The choice of this type of strategy is consistent with evidence that humans strategically use information from last round play of their opponents in zero sum games [@batzilis; @wang2014social].

\begin{table}[bth!]
\centering
\begin{tabular}{||c c | c c||} 
 \hline
 Human action $t-1$ & Computer action $t-1$  & Computer level-1 action $t$ & Computer level-2 action $t$ \\ [0.5ex] 
 \hline\hline
 Paper & Rock & Scissors & Scissors \\ 
 Scissors  & Scissors & Rock & Paper \\
 Rock & Paper & Paper & Rock \\
 ... & ... & ... & ... \\ [1ex] 
 \hline
\end{tabular}
\caption{Example of how a level-1 and level-2 computer agent plays in response to actions taken in the previous round.}
\label{table:1}
\end{table}

### Procedure

Participants were informed they would play three different games against the same computer opponent. Participants were told that the opponent cannot cheat and will choose its actions simultaneously with them, without prior knowledge of the participant's choice. After providing informed consent and reading the instructions, participants answered a number of comprehension questions. They then played the three games against their opponent in the order RPS, FGW, and Numbers. An example of the interface for the RPS game is provided in Figure\ \@ref(fig:feedback-rps-exp2). On each round, tHe human player chooses an action, and after a random delay, is shown the action chosen by the computer opponent, and the outcome of that round. A history of the chosen actions over all rounds is available in the centre panel. A total of 50 rounds of each game was played with the player's score displayed at the end of each game. The score was calculated as the number of wins minus the number of losses. Ties did not affect the score. In order to incentivise the participants to maximise the number of wins against the opponents, players were paid a bonus at the end of the experiment that was proportional to their final score. Each point is worth £0.02. After playing all the games, participants were asked questions about their beliefs about the computer opponent, related to whether they thought they learned their opponent's strategy, and how difficult they found playing against their particular opponent. They were then debriefed and thanked for their participation.


<!-- Participants played 3 games sequentially against the same computer opponent. The computer opponent either used a level-1 or level-2 strategy. Participants were informed they would play three different games against the same computer opponent. Each participant plays all three games consecutively and in the same order described above. Participants were told that the opponent cannot cheat and will choose its actions simultaneously without knowledge of the participant's choice. A total of 50 rounds of each game was played with the player's score displayed at the end of each game. The score was calculated as the number of wins minus the number of losses. Ties did not affect the score. In order to incentivise the participants to maximise the number of wins against the opponents, players were paid a bonus at the end of the experiment that was proportional to their final score. Each point is worth £0.02. An example of the interface for the rock-paper-scissors game is provided in Figure\ \@ref(fig:feedback-rps-exp2). -->



```{r feedback-rps-exp2, fig.cap = "Screenshot of the Rock-Paper-Scissors game in Experiment 2. Shown here is the feedback stage, after both the human (left) and computer (right) players have chosen their action. The interface was similar in Experiment 1, but excluded the history of game play in the center panel.", fig.align='center'}

knitr::include_graphics("images/feedback_rps.png")

```

### Results 

```{r load-exp1-data, include=FALSE}

dat1 <- read.csv("../data20180719.csv") 

# transform 'winner' variable in numeric score
dat1$score <- recode(dat1$winner, human = 1, tie = 0, ai = -1)
# create a new variable 'block' with round 1...25 = block 1 and round 26...50 as block 2
dat1$block <- factor(as.numeric(cut(dat1$round,2)),labels =c("first half", "second half"))

# create a new variable "game.f" as a factor variable of games
dat1$game.f <- factor(dat1$game, labels = c("RPS","FWG","Numbers"),levels=c("rps","fwg","numbers"))

#Group data by human_id and calculate mean score per block of each game.
dat2 <- dat1 %>% 
  group_by(human_id,condition,game,block,game.f) %>% 
      summarize(block_score = mean(score))

# Group data by game and ID
dat3 <- group_by(dat2, human_id,game.f,game) %>% summarise(game_score = mean(block_score))
# head(dat3,6)

# Subsetting scores data by game
rps <- subset(dat3,  game.f == "RPS",game_score)
fwg <- subset(dat3,  game.f == "FWG",game_score)
num <- subset(dat3,  game.f == "Numbers",game_score)
# head(rps,6)
```

```{r exp1-avg-scores-game, fig.cap = "Performance per game and block across conditions in Experiment 1. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align="center", fig.width=8, fig.height = 4}
# Plot average  scores per game

dat2 %>%
  mutate(game = factor(game)) %>%
  mutate(game = forcats::fct_recode(game, RPS = "rps", FWG = "fwg", Numbers = "numbers")) %>%
  mutate(game = factor(game, levels = c("RPS", "FWG", "Numbers"))) %>%
  mutate(condition = factor(condition, levels = c("Level1", "Level2"))) %>%
  mutate(condition = forcats::fct_recode(condition, "Level 1" = "Level1", "Level 2" = "Level2")) %>%
  ggplot(aes(x = game, y = block_score, colour = block)) + stat_summary(fun.data = "mean_ci", position = position_dodge2(width=.8), geom = "crossbar")  + geom_point(position=position_jitterdodge(), alpha=.4) + ylab("Score") + xlab("Game") + facet_grid(. ~ condition) + theme_apa() + geom_hline(yintercept = 0, lty=2)

#+ geom_boxplot(alpha=.4) + geom_point(position=position_jitterdodge(), alpha=.4) + ylab("Score") + xlab("Game") + facet_grid(. ~ condition) + theme_apa() + geom_hline(yintercept = 0, lty=2)

#geom_jitter(aes(colour=block, fill=block), width=.2) + facet_grid(. ~ condition)


# ggboxplot(x = "game", y = "block_score", colour="block", fill="block", add =c("mean_ci"), palette = c("#00AFBB", "#E7B800", "#FC4E07"), order = c("RPS", "FWG","NUMBERS"), ylab = "Score", xlab = "Game", legend.title = "Block") + geom_jitter(aes(color = block, fill = block), width = 0.2, alpha = 0.3 ) + facet_grid(. ~ condition)

```
On average, participants obtained the lowest score in the RPS game ($M = 0.289$, $SD = 0.348$), followed by Numbers ($M = 0.31$, $SD = 0.347$). Participants' performance was highest in the FWG game ($M = 0.454$, $SD = 0.354$). Scores in each game were significantly different from 0, the expected score of uniformly random play (RPS: $t(51) = 7.26$, $p < .001$;  FWG:  $t(51) = 10.04$ , $p < .001$; Numbers: $t(51) = 7.17$, $p < .001$). As uniformly random play is the Nash equilibrium, this indicates successful deviation from a Nash-optimal strategy. 

To assess learning within and between games, we used a 2 (condition: Level 1, Level 2) by 3 (game: RPS, FWG, Numbers) by 2 (block: first half, second half) repeated-measures ANOVA, with the first factor varying between participants. This showed a main effect of Game ($F(2,100) = 8.54$, $\eta^{2}  = 0.05$,  $p < .001$), indicating that average scores varied significantly over the games. Post-hoc pairwise comparisons showed that performance in the FWG game was significantly higher than in the RPS game ($t(100) =3.78$, $p < .001$) and the Numbers game ($t(100) = 3.32$ , $p = .002$). The score in RPS was not significantly different from the score in Numbers ($t(100) = 0.45$ , $p = .65$). The main effect of Block ($F(1,50) = 22.51$ ,  $\eta^{2} = 0.03$, $p < .001$) shows that the score in the first half of each game ($M = 0.29$) was significantly lower than in the second half ($M = 0.40$), which indicates within-game learning. The main effect of Condition ($F(1,50) = 5.44$, $\eta^{2} = 0.05$, $p = .024$) indicates that scores were higher against the level-1 player ($M = 0.43$) than against the level-2 player ($M = 0.27$). Thus, it appears that is was harder for participants to exploit the strategy of the more sophisticated level-2 opponent than the comparatively less sophisticated level-1 opponent. **MS: Need to check test results and report GG degrees of freedom, etc.**



<!-- Looking at the aggregate scores (See Figure\ \@ref(fig:exp1-avg-scores-game) ), the RPS game had the lowest average score across participants (M = 0.289, SD = 0.348) followed by NUMBERS (M = 0.31, SD = 0.347) and finally the FWG game had the highest average score (M = 0.454, SD = 0.354).  Aggregate average scores for each game were significantly different from 0 (hypothesised value of random play) using parametric one sample t-tests (RPS: t(51) = 7.26, $p$ < 0.001 ;  FWG:  t(51) = 10.04 , $p$ < 0.001 ; NUMBERS: t(51) = 7.17, $p$ < 0.001). -->
<!-- To analyse within and between game learning, we used a 2 (condition: level-1, level-2) by 3 (game: RPS, FWG, NUMBERS) by 2 (block: first half, second half) repeated measures ANOVA with the first factor varying between participants. There was a main effect of Game (F(2,100) = 8.54, $\eta^{2}$  = 0.05,  p < 0.001), showing that average scores varied significantly over the games. Post-hoc pairwise comparisons showed that performance in the FWG game was significantly higher than in the RPS game (t(100) =3.78, p = 0.0008), and the performance in the NUMBERS game was significantly lower than FWG game (t(100) = -3.32 , p = 0.0024). The score in RPS was not significantly different from the score in NUMBERS (t(100) = 0.45 , p = 0.65). The main effect of Block (F(1,50) = 22.51 , p < .001, $\eta^{2}$ = 0.03) shows that the average score in the first half of games (M = 0.29)  was significantly lower than in the second half  of the games played (M = 0.40), which translates to within-game learning. The main effect of Condition (F(1,50) = 5.44, p = .024, $\eta^{2}$ = 0.05) indicates that scores were higher against the level-1 player (M = 0.43) than against the level-2 player (M = 0.27). This indicates that it was harder for participants, on average, to exploit the strategy of the more sophisticated opponent (level-2) compared to that of the comparatively less sophisticated agent (level-1). -->

```{r exp1-3factor-plot, include= FALSE, fig.cap = "Performance in early rounds (2-6) per game and block across conditions in Experiment 1. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align = "center", fig.width=8, fig.height=4}

# Plot results by game and block (for all 3 games, learning is happening) 
dat2 %>%
  mutate(game = forcats::fct_recode(as.factor(game), RPS = "rps", FWG = "fwg", Numbers = "numbers")) %>%
  mutate(condition = factor(condition, levels = c("Level1", "Level2"))) %>%
  mutate(condition = forcats::fct_recode(condition, "Level 1" = "Level1", "Level 2" = "Level2")) %>%
  ggboxplot(x = "game", y = "block_score", palette = c("#00AFBB", "#E7B800"),order = c("RPS", "FWG","Numbers"), fill="block", ylab = "Percentage score", xlab = "Games") +  facet_grid(. ~ condition)

```


```{r ANOVAexp1, include = FALSE, warning=FALSE}

afex::aov_car(
  block_score ~ game.f*condition*block + Error(human_id/(game.f*block))
  , data=dat2
  , type = 3)
                                    

#apa_lm1 <- apa_print(exp1_early_anova)

#means.int2 <- lsmeans(exp1_early_anova, specs = c("game.f","condition"))
#trans26 <- summary(means.int2, infer = c(TRUE,TRUE),level = .95, adjust = "holm",ref=c("FWG","Numbers"))

```

<!-- Finally, the analysis showed a significant block by game interaction ( F(2,100) = 6.92 , p = .002, $\eta^{2}$ = 0.02), indicating that within-game learning differed between the games. Indeed, second half scores in RPS are significantly higher than first-half scores (t(150) = 5.59, p <.0001), while there was no significant difference between block scores for the other two games. This is indicative of the significant within-game learning happening in the first game when players have no experience against the opponent, as opposed to much lower within game performance improvement in the latter games when participants have had some experience playing against the opponent and start with higher scores indicative of transfer.  There was also a three-way interaction between condition, game, and block ( F(2,100) = 3.88, p = .023, $\eta^{2}$ = 0.01), which indicates, as seen in Figure\ \@ref(fig:exp1-3factor-plot) that within-game learning changes across games also depend on the sophistication of the opponent. For instance, there is more within game learning in the third game against level-2 opponents, since the initial scores are lower than against level-1 opponent. The explanation for this will become clearer when we discuss the factors moderating learning transfer in the next section. -->

### Learning transfer

<!-- As a measure for learning transfer, we focus on participants’ scores in the first 5 rounds excluding the initial round (rounds 2-6). We exclude the very first round as the computer opponent plays randomly here and there is no opportunity yet for the human player to exploit their opponent's strategy. A group of players with no experience of the game are expected to perform at chance level over the early rounds of a new game, as was the case in RPS. Positive scores in the early rounds would therefore reflect generalization of prior experience. For the FWG game, the score is significantly higher than 0 ( t(148.85) = 4.58 , p < 0.0001). This is also the case for the more dissimilar game : NUMBERS ( t(148.85) = 3.00, p = 0.0092). For the RPS game, the average score is not significantly different from 0 as this is the first game and no learning is possible (t(148.85) = 1.04 , p = 0.89). -->

As a measure for learning transfer, we focus on participants’ scores in the initial 5 rounds after the first round (rounds 2-6) of each game (see Figure \ref{fig:exp1-early-score-by-opp}). We exclude the very first round as the computer opponent plays randomly here and there is no opportunity yet for the human player to exploit their opponent's strategy. Players with no knowledge of their opponent's strategy are expected to perform at chance level in these early rounds. Positive scores in rounds 2-6 reflect generalization of prior experience. The FWG early score score is significantly higher than 0 ($t(148.85) = 4.584$, $p < .001$). This is also the case for the Numbers game ($t(148.85) = 3.00$, $p = .009$). We did not expect positive scores for the RPS game, as it was the first game played and there was no opportunity for learning about the opponent's strategy. Scores in this game was indeed not significantly different from 0 ($t(148.85) = 1.04$ , $p = .89$).


```{r exp1-early-rounds, include=FALSE}
dat_26 <- subset(dat1,round >1 & round <7, drop =TRUE)
exp1_dat2_6 <- dat_26 %>% 
  group_by(human_id,condition,game.f,confidence,difficulty) %>% 
      summarise(early_score = mean(score))

# Check group means and SDs by game and condition
group_by(exp1_dat2_6, game.f,condition) %>%
  summarise(
    count = n(),
    mean = mean(early_score, na.rm = TRUE),
    sd = sd(early_score, na.rm = TRUE)
    )

```

<!-- (ref:figure4-caption) Mean and standard error of early scores by game and opponent -->
```{r exp1-early-score-by-opp, fig.cap = "\\label{ref:figure4-caption}Performance in early rounds (2-6) per game and block across conditions in Experiment 1. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align ='center', fig.width=8, fig.height=4}
exp1_dat2_6 %>%
  mutate(game = factor(game.f)) %>%
  mutate(game = forcats::fct_recode(game, Numbers = "Numbers")) %>%
  mutate(game = factor(game, levels = c("RPS", "FWG", "Numbers"))) %>%
  mutate(condition = factor(condition, levels = c("Level1", "Level2"))) %>%
  mutate(condition = forcats::fct_recode(condition, "Level 1" = "Level1", "Level 2" = "Level2")) %>%
  ggplot(aes(x = game, y = early_score, colour=game)) + stat_summary(fun.data = "mean_ci", geom = "crossbar")  + geom_jitter(alpha=.4) + ylab("Early score") + xlab("Game") + facet_grid(. ~ condition) + theme_apa() + geom_hline(yintercept = 0, lty=2) + scale_color_discrete() + theme(legend.position = "none")

  
#p2 <- ggerrorplot(exp1_dat2_6, x = "game.f", y = "early_score", group = 1, color="game.f", desc_stat = "mean_se",palette = c("#00AFBB", "#E7B800", "#FC4E07"), order = c("RPS", "FWG","Numbers"), ylab = "Early rounds score", xlab = "Games")

#p2 + facet_grid(. ~ condition)

```



<!-- Next, we explore whether learning transfer is moderated by the type of opponent and game similarity. Figure\ \@ref(fig:exp1-score-by-opp) shows the mean scores for rounds 2-6 by game for both level-1 and level-2 facing players as well as the $95$ percent confidence interval for the mean. Graphically we can see that the pattern is dissimilar between level-1 and level-2 players, and we suspect transfer to be positively associated with similarity and negatively with degree of sophistication of the agent. To explore this, we run statistical tests on early round scores by game and opponent against the null hypothesis of 0 (no transfer). For level-1 facing players, there is evidence of learning transfer from RPS to both FWG ( t(150) = 3.96, p < 0.001) and NUMBERS (t(150) = 3.74, p < 0.001) . For level-2 facing players, there is evidence for transfer from RPS to the similar game FWG, albeit scores are lower than for level-1 player ( t(150) = 2.48, p = 0.01) but not to the dissimilar game of NUMBERS.  -->

Next, we explore whether learning transfer is moderated by the type of opponent and game similarity. We expected better transfer between more similar games (i.e. better transfer from RPS to FWG than from RPS/FWG to Numbers), and worse transfer for the more sophisticated level 2 agent. Figure \ref{fig:exp1-early-score-by-opp} indicates that the pattern over the games is indeed dissimilar between level-1 and level-2 opponents. To explore this, we used a 2 (condition: Level 1, Level 2) by 3 (game: RPS, FWG, Numbers) repeated measures ANOVA with the first factor varying between participants. There was a main effect of Game ($F(1.84,91.77) = 3.35$, $\eta^{2}  = 0.04$,  $p = .043$, Greenhouse-Geisser correction to degrees of freedom applied to correct for non-sphericity).  We then run statistical tests on early round scores by game and opponent against the null hypothesis of 0 (no transfer). For level-1 facing players, there is evidence of learning transfer from RPS to both FWG ($t(150) = 3.96$, $p < .001$) and Numbers ($t(150) = 3.74$, $p < .001$). For level-2 facing players, there is evidence for transfer from RPS to the similar game FWG, albeit scores are lower than for level-1 player ($t(150) = 2.48$, $p = .01$) but not to the dissimilar game of Numbers. 

### Discussion

The results of Experiment 1 indicate that participants were able to learn successful strategies which exploited the deviation from Nash-optimal play of their opponents. Moreover, they were able to transfer knowledge about their opponent to other games, but this was moderated by the type of game and opponent. There was evidence of transfer from the RPS game to the similar FWG game for both opponents. However, transfer to the dissimilar Numbers game was only evident for participants facing a level-1 opponent. <!-- These results indicate that learning transfer to the more dissimilar game (NUMBERS) we found earlier is exclusively driven by level-1 facing players, as average early round scores in the NUMBERS game of level-2 facing players are close to 0. Therefore, both participants facing level-1 and level-2 agents can transfer learning to the similar FWG game, but only those facing the less sophisticated opponent are able to generalise to the less similar NUMBERS game. -->

```{r ANOVA early exp1, include = FALSE, warning=FALSE}

exp1_early_anova <- afex :: aov_car(
  early_score ~ game.f*condition + Error(human_id/(game.f))
  , data=exp1_dat2_6
  , type = 3)
                                    
exp1_early_anova
#apa_lm1 <- apa_print(exp1_early_anova)

means.int2 <- lsmeans(exp1_early_anova, specs = c("game.f","condition"))
trans26 <- summary(means.int2, infer = c(TRUE,TRUE),level = .95, adjust = "holm",ref=c("FWG","Numbers"))

```
  
```{r table-early-scores, include = FALSE, results = "asis"}
# apa_table(trans26, caption = "Average early round scores by game and condition in first experiment")
```

<!-- ### Discussion Experiment 1  -->

<!-- Our results when averaging across conditions (previous section) showed that there was indeed evidence for transfer to the more dissimilar game (NUMBERS). We can see from splitting the participants by opponent faced that this transfer is exclusively driven by level-1 facing players, as average early round scores of level-2 facing players are close to nil in the NUMBERS game. Therefore, both participants facing level-1 and level-2 agents can transfer learning to the similar game, but only those facing the less sophisticated opponent are able to generalise to the less similar game.  -->

# Experiment 2

In Experiment 2, we aimed to obtain a stronger test of learning transfer. <!-- We ran a second experiment with various differentiated features to improve the opportunity to measure learning transfer.--> Instead of facing a single level-1 or level-2 opponent throughout all games, participants face both types of opponents sequentially in this experiment. To perform well against both opponents, participants would need to learn distinct strategies in each game. To reduce effects of increased memory load due to facing distinct opponents, we provided participants access to the history of play against an opponent within each game (see Figure \@ref(fig:feedback-rps-exp2)). Finally, we changed the third game to a penalty shootout game, with participants aiming to score a goal and opponents playing the role of goal keepers. Whilst this game has the same number of actions as the first two (aim left, center, or right), it is strategically dissimilar. Unlike the Numbers game in Experiment 1, the Shootout game does not have a cyclical hierarchy between actions, making it harder to win through a heuristic based on this cyclicity.

<!-- Because there were two distinct opponents, requiring potentially holding two opponent models in memory, we also made it easier to recall the results of past rounds by providing participants with the opportunity to see the history of the game since the beginning of each interaction. Figure\ \@ref(fig:feedback-rps-exp2) shows an example of showing interaction history in the RPS game. Finally, we changed the third game to a penalty shootout game, whith participants aiming to score a goal and opponents playing the role of goal keepers.  Whilst this game has the same number of actions as the first two, it is strategically dissimilar. Unlike the third game in the first experiment, it did not have a cyclical hierarchy between actions, making it harder to win by just following simple heuristics leveraging this cyclicality. If we see evidence for differential play against opponents, it would show participants adapting their strategies to the opponent they are facing, which is indicative of opponent modelling. -->

## Methods 

### Participants & Design 

A total of 48 participants (21 females, 28 males, 1 unknown) were recruited via the Prolific platform, none of which took part in Experiment 1. The average age was 30.2 years, and the mean duration to complete the task was 39 minutes. Participants were incentivised using a two-tier payment mechanism: a fixed fee of £2.5 for completing the experiment plus a performance linked bonus that averaged £1.32.

### Tasks

The three games the participants played were Rock-Paper-Scissors (RPS), Fire-Water-Grass (FWG), and the penalty Shootout game. The first two games were identical to the ones used in the first experiment. In the final game (Shootout) participants took the role of the a football (soccer) player in a penalty situation, with the computer opponent taking the role of the goalkeeper. Players had the choice between three actions: shooting the football to the left, right or centre of the goal. Similarly, the goalkeeper chooses between defending the left, right, or centre of the goal. If participants shoot in a different direction than where the goalkeeper defends, they win the round and the goalkeeper loses. Otherwise, the goalkeeper catches the ball and the player loses the round. There is no possibility of ties in this game. Figure\ \@ref(fig:screenshot-shootout) shows a snapshot of play in the shootout game. What makes this game different to the other games is that there are two ways to beat the opponent: if the shooter thinks their opponent is going to choose to defend ''right'' in the next round, they can win by either choosing to shoot ''left'' or ''center''. A level-1 shooter who thinks that their goalkeeper opponent will repeat their last action has thus two possible best responses. A level-1 goalkeeper, however, has only a single best response (defending where their opponent aimed in the last round). A level-2 goalkeeper, who believes their opponent is a level-1 shooter, will have two best responses however. We programmed the level-2 computer player to choose randomly between these two best responses.  

```{r screenshot-shootout, fig.cap = "Screenshot of the shootout game", fig.align='center'}

knitr::include_graphics("images/shootout.jpg")
```

### Procedure 

Participants played 3 games sequentially against both level-1 and level-2 computer opponents, rather than just one like in the first experiment. Like in the first experiment, the computer opponents retained the same strategy throughout the 3 games, however the participants faced each opponent twice in each game. Specifically, each game was divided into 4 stages numbered 1 to 4, consisting of 20, 20, 10, and 10 rounds respectively for a total of 60 rounds per game. Participants start by facing one of the opponents in stage one, then face the other in stage two. This is repeated in the same order in stages 3 and 4. Which opponent they faced first was counterbalanced. All participants engage in the same three games (namely RPS, FWG and Shootout) in this exact order, and were aware that the opponent was not able to know their choices beforehand but was choosing simultaneously with the player. 
In order to encourage participants to think about their next choice, a countdown timer of 3 seconds was introduced at the beginning of each round. During those 3 seconds, participants could not choose an action and had to wait for the timer to run out. A small random delay between 0.5 and 3 seconds was also introduced before the computer agent made their choice, as a way of simulating a real human opponent thinking time. After each round, participants were given detailed feedback about their opponent's action and whether they won or lost the round. Further information about the history of play in previous rounds was also provided and participants could scroll down to recall the full history of each interaction against an opponent in a particular stage of a game. The number of wins, losses and ties were clearly shown at the top of the screen for each game, and this scoreboard was reinitialised to zero at the onset of a new stage game.
As in the first experiment, all the games have a unique mixed-strategy Nash equilibrium consisting of uniformly random actions. If participants follow this strategy, or simply don't engage in learning how the opponent plays, they would score 0 on average against both level-1 and level-2 players. Evidence of sustained wins would indicate that participants have learned to exploit patterns in their opponents' play. 

## Results

```{r load_exp2_data , include=FALSE}
dat_int <- read.csv(file = "../Experiment_2/dat_int_exp2.csv")
```

Participants scores are depicted in Figure \ref{fig:exp2-score-by-opp}. The RPS game had the lowest average score per round ($M = 0.194$, SD = 0.345) followed by FWG ($M = 0.27$, SD = 0.394) and finally the Shootout game ($M = 0.289$, SD = 0.326).^[ A higher score in shootout is expected as there are 2 out of three possible winning actions, compared to one out of three in RPS and FWG. Indeed, a player not aiming to uncover the opponent’s strategy and thus choosing to play randomly should be expected to have on average score per round of 0 in both RPS and FWG, and 0.33 in the Shootout game. To make the scores more comparable, and because we are interested in player’s performance that is not due to chance, we will adjust all scores in the shootout game by subtracting the average score per round of a random strategy (0.33)] Using parametric t-tests on adjusted scores, we reject the null hypothesis of random play in all three games (RPS: $t(49) = 6.26$, $p < .001$; FWG: $t(49) = 7.25$, $p < .001$; Shootout: $t(49) = 13.61$, $p < .001$). 

```{r Anova exp 2, include= FALSE}
exp2_anova <- afex::aov_car(
  int_score ~ (game.f*interaction_lvl*condition.f*opp_type) + Error(human_id/(game.f*interaction_lvl*opp_type))
  , data = dat_int
  , type = 3
)

(ls0 <- lsmeans(exp2_anova, "game.f", by = "opp_type"))
(ls0 <- update(pairs(ls0, reverse = TRUE), by=NULL, adjust = "holm"))
apa_lm <- apa_print(exp2_anova)

```


```{r exp2-score-by-opp, include= FALSE, fig.cap = "\\label{fig:exp2-score-by-opp}Performance per game and interaction across opponents in Experiment 2. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align = "center", fig.width=8, fig.height=4}
# Plot results by game and block (for all 3 games, learning is happening) 
dat_int %>%
  mutate(game.f = forcats::fct_recode(game.f, Shootout = "SHOOT")) %>%
  mutate(game.f = factor(game.f, levels = c("RPS", "FWG", "Shootout"))) %>%
  mutate(opp_type = forcats::fct_recode(as.factor(opp_type), "Level 1" = "Level_1", "Level 2" = "Level_2")) %>%
  mutate(encounter = interaction_lvl) %>%
  ggplot(aes(x = game.f, y = int_score, colour = encounter)) + stat_summary(fun.data = "mean_ci", position = position_dodge2(width=.8), geom = "crossbar")  + geom_point(position=position_jitterdodge(), alpha=.4) + ylab("Score") + xlab("Game") + facet_grid(. ~ opp_type) + theme_apa() + geom_hline(yintercept = 0, lty=2)

# p3 <- ggerrorplot(dat_int, x = "game.f", y = "int_score", group = 1, color="game.f", desc_stat = "mean_ci",palette = c("#00AFBB", "#E7B800", "#FC4E07"), order = c("RPS", "FWG","SHOOT"), ylab = "Average Scores per game", xlab = "Games")
# 
# p3 + facet_grid(. ~ opp_type)

#p4 <- ggboxplot(dat_int, x = "game.f", y = "int_score", palette = c("#00AFBB", "#E7B800"),order = c("RPS", "FWG","SHOOT"), fill="interaction_lvl",ylab = "Percentage score", xlab = "Games", legend.title = "interaction level")

#p4 + facet_grid(. ~ opp_type)


```

```{r exp2-anova, results = "asis", echo=FALSE, warning=FALSE}
# apa_table( apa_lm$table, caption = "ANOVA results for experiment 2", escape = FALSE)
```

To explore whether learning occurred within and between games, we performed a two (Condition: level-1 first, level-2 first) by two (Opponent type: level-1 or level-2) by three (Game: RPS, FWG, Shootout) by two (Encounter: first or second) repeated-measures ANOVA on the average score per round, with the first factor varying between participants. This shows a main effect of Game ($F(1.85,88.7) = 11.81$, $\eta^{2} = 0.04$,  $p < .001$). Post-hoc pairwise comparisons between games (p-values adjusted using Holm method for multiple comparisons) indicate performance in the games increases steadily throughout the experiment, with FWG performance significantly higher than RPS ($t(96) =2.53$, $p = .025$), and performance in the Shootout game significantly higher than in FWG ($t(96) = 2.32$, $p = .025$). Whilst the ANOVA shows no main effect of Condition, Opponent type, or Encounter, there was a significant interaction between Game and Opponent type ($F(1.7, 81.82) = 5.31$ ,$\eta^{2} = 0.02$,  $p = .01$). Follow-up analysis shows that when facing level-1 agents, scores increase steadily after each game, with FWG score significantly higher than RPS ($t(191) = 2.70$, $p = .03$) and Shootout scores in turn significantly higher than FWG  ($t(191) = 3.05$, $p = .01$). There was no significant difference between average scores on any two games when facing level-2 agents however.

<!--Figure\ \@ref(fig:exp2-score-by-opp) shows boxplots of game scores, averaged across participants, by game and opponent type. We also distinguish between scores from the first time the players faced the opponent (first interaction) and the second time they did (second interaction). We see that when facing level-1 agents, scores increase steadily after each game, with FWG score significantly higher than RPS ($t(191) = 2.70$, $p = .03$) and Shootout scores in turn significantly higher than FWG  ($t(191) = 3.05$, $p = .01$). There was no significant difference between average scores on any two games when facing level-2 agents however. -->


### Learning transfer

```{r, include=FALSE}
dat <- read.csv(file = "../Experiment_2/dat_exp2.csv")
```



```{r, include = FALSE}

#looking at TRIALS 2 to 6 to test robustness of evidence for transfer of learning of opponent strategy #########

dat_26 <- subset(dat,(round >1 & round < 7) & (interaction_lvl == "first interaction" ), drop =TRUE)

exp2_dat2_6 <- dat_26 %>% 
  group_by(human_id,condition.f,game.f,opp_type,confidence,difficulty) %>% 
      summarise(early_score = mean(adj_score))

# Check group means and SDs
group_by(exp2_dat2_6, game.f) %>%
  summarise(
    count = n(),
    mean = mean(early_score, na.rm = TRUE),
    sd = sd(early_score, na.rm = TRUE)
    )
group_by(exp2_dat2_6, game.f,opp_type) %>%
  summarise(
    count = n(),
    mean = mean(early_score, na.rm = TRUE),
    sd = sd(early_score, na.rm = TRUE)
    )
```

```{r exp2-early-score-by-opp, fig.cap = "\\label{ref:figure4-caption}Performance in early rounds (2-6) per game and opponent in Experiment 2. Points are scores of individual participants and boxes reflect the 95\\% confidence intervals of the mean (center line equals the mean).", fig.align ='center', fig.width=8, fig.height=4}
exp2_dat2_6 %>%
  mutate(game = factor(game.f)) %>%
  mutate(game = forcats::fct_recode(game, Shootout = "SHOOT")) %>%
  mutate(game = factor(game, levels = c("RPS", "FWG", "Shootout"))) %>%
  mutate(opp_type = factor(opp_type, levels = c("Level_1", "Level_2"))) %>%
  mutate(opp_type = forcats::fct_recode(opp_type, "Level 1" = "Level_1", "Level 2" = "Level_2")) %>%
  ggplot(aes(x = game, y = early_score, colour=game)) + stat_summary(fun.data = "mean_ci", geom = "crossbar")  + geom_jitter(alpha=.4) + ylab("Early score") + xlab("Game") + facet_grid(. ~ opp_type) + theme_apa() + geom_hline(yintercept = 0, lty=2) + scale_color_discrete() + theme(legend.position = "none")

  

# plot scores per game 
#p2 <- ggerrorplot(exp2_dat2_6, x = "game.f", y = "early_score", group = 1, color="game.f", desc_stat = "mean_se",palette = c("#00AFBB", "#E7B800", "#FC4E07"), order = c("RPS", "FWG","SHOOT"), ylab = "Early rounds score", xlab = "Games", legend.title = "Games") 

#p2 + facet_grid(. ~ opp_type)

```


As a measure for learning transfer we will again compare scores only on rounds 2-6 of each game, excluding the very first round where play is necessarily random (Figure\ \@ref(fig:exp2-early-scores)). For both the FWG and Shootout games, score in the early rounds of the first interaction are significantly higher than 0 for both opponent types (Level-1 opponent: FWG: $t(270) = 4.99$, $p < .001$; Shootout: $t(270) = 6.66$, $p < .001$; Level-2 opponent: FWG: $t(270) = 4.40$, $p < .001$; Shootout: $t(270) = 3.21$, $p=.004$). **MS: Should also report ANOVA, as for Experiment 1**

### Discussion

The results of Experiment 2... **MS: give summary of main findings, as done for Experiment 1. You should not report new statistical tests in the discussion, which should go in the results section before.** 

Looking at learning transfer by type of opponent faced, we confirm the result from the first experiment that it is easier to transfer learning to the more dissimilar game (Shootout) when facing a level 1 opponent. Indeed, while the early scores of FWG for level-1 and level-2 facing players are not significantly different from each other, the score of the players facing the level-1 opponent is indeed almost 0.2 point per round higher than that of players facing level-2 opponents, and the difference is statistically significant ( t(144) = 2.45 , p = 0.01). These early scores have also been adjusted to account for the fact that the shootout game has higher average scores when playing randomly, and therefore this difference is really due to better learning transfer and not due to chance. 

# Computational modelling

To gain more insight into participants' strategies against their computer opponents, we constructed and tested several computational models of strategy learning. The baseline model assumes play is random, and each potential action is chosen with equal probability. Note that this corresponds to the Nash equilibrium strategy. The other models adapted their play to the opponent, either by reinforcing successful actions in each game (reinforcement learning), or by determining the type of opponent through Bayesian learning (Bayesian Cognitive Hierarchy models). We also include the Expected Weighted Attraction (EWA) model, which is a popular model in behavioural economics.

In the following, we will describe the models in more detail, and provide some intuition into how they they learn about the game and/or the opponent. We use the following notation: In each game $g \in \{\text{RPS},\text{FWG}, \text{Numbers}, \text{Shootout} \}$, on each trial $t$, the participant chooses an action $a_t \in \mathcal{A}_g$, and the opponent chooses action $o_t \in \mathcal{A}_g$, where $\mathcal{A}_g$ is the set of allowed actions in game $g$, e.g. $\mathcal{A}_\text{RPS} = \{R,P,S\}$. The participant then receives reward $r_t \in \{1,0,-1\}$, and the opponent receives $-r_t$. We use the state variable $s_t = \{a_{t-1},o_{t-1}\}$ to denote the actions taken in the previous round $t-1$ by the participant and opponent. 

## Reinforcement learning (RL) model

We first consider a model-free reinforcement learning algorithm, where actions that have led to positive rewards are reinforced, and the likelihood of actions that led to a negative reward is lowered. Since the computer players in this experiment based their play on the actions in the previous round, a suitable RL model for this situation is one which learns the value of actions contingent on plays in the previous round, i.e. by defining the state $s_{t}$ as above. The resulting RL model learns a $Q$-value [@watkins1992q] for each state-action pair:
$$ Q_{t+1}(s_{t},a_{t}) = Q_{t}(s_{t},a_{t}) + \alpha \left( r_{t}  - Q(s_{t},a_{t}) \right) ,$$  
where $Q(s_{t},a_{t})$ is the value of taking action $a$ when in state $s$ at time $t$, and $\alpha \in [0,1]$ the learning rate. For instance, $Q_t(\{R,S\},P)$ denotes the value of taking action  ''Paper'' this round if the player's last action was ''Rock'' and the opponent played ''Scissors''. Actions are taken according to a softmax rule:
$$P_{t}(a|s_t) = \frac{\exp \{ \lambda Q_{t}(a,s_t) \}}{\sum_{a' \in \mathcal{A}_g} \exp \{\lambda  Q_{t}(a',s_t) \}}, $$ 
where the inverse temperature parameter $\lambda$ determines the consistency of the strategu (the higher $\lambda$, the more often the action with the highest $Q$-value is chosen. While this RL model allows the players to compute the values of actions conditional on past play, crucially, it will not be able to transfer learning between games, as each game has a different state $\mathcal{S}_g$ and action space $\mathcal{A}_g$, and there is no simple way to map states and actions between games.

The RL model has two free parameters: the learning rate ($\alpha$) and the inverse temperature ($\lambda$). <!-- We've assumed $\gamma = 0$ as changes in this parameter didn't affect the other parameter estimations or ultimately the value of the likelihoods. -->


## Experience-weighted attraction (EWA) model

The self-tuning Experience Weighted Attraction (EWA) model [@ho2004economics] combines two seemingly different approaches, namely reinforcement learning and belief learning. Belief learning models are based on the assumption that players keep track of the frequency of past actions and best respond to that. By contrast, reinforcement learning does not explicitly take into account beliefs about other players, but simply increases the probability of repeating a more rewarding action. The self-tuning EWA model has been shown to perform better than either RL or belief learning alone in various repeated games and has the advantage of having only one free parameter, the inverse temperature of the softmax choice function. The EWA model is based on updating ``Attractions'' for each action over time. The attraction of action $a$ time $t$ is written $A_{t}(a)$ and is updated as 
$$ A_{t+1}(a) =  \frac{\phi \ N(t) \ A_{t}(a) + [ \delta + (1-\delta) \ I(a_t = a )] \ R(a,o_t) } {\phi \ N(t) + 1} $$
where $I(x)$ is an indicator function which takes the value 1 is its argument is true, and 0 otherwise, and $R(a,o_t)$ is the reward that would be obtained from playing action $a$ against opponent action $o_t$, which equals the actual obtained reward when $a = a_t$, and otherwise is a counterfactual reward that would have been obtained if a different action were taken. Unlike reinforcement learning, this uses knowledge of the rules of the game to allow reinforcing actions that were not taken. We can see that setting $\delta = 0$ leads to reinforcement of past actions, while positive and high delta parameters make the update rule take into account foregone pay-offs, which is similar to weighted fictitious play [@cheung1994learning]. While the assumption in expanding the update rule above is that $\phi$ and $\delta$ are free parameters [@camerer1997experience], the self-tuning aspect of the model comes from the fact that these are now self-tuned using the formulas expanded in [@ho2004economics]. $N(t)$ represents an experience weight and can be interpreted as the number of "observation-equivalents" of past experience. We initialise it to 1 so initial attractions and reinforcement from payoffs are weighted equally. **MS: Should describe how the parameters are self-tuned with equations **

As in the models above, actions are chosen based on a softmax decision rule:
$$P_t(a) = \frac{\exp \{\lambda  A_{t}(a) \} }{\sum_{a' \in \mathcal{A}_t} \exp \{ \lambda A_{t}(a') \} }$$
The self-tuning EWA has one free parameter: the inverse temperature of the softmax decision rule ($\lambda$). **MS: Are there no states in the EWA? If not, it is unsurprising that this model doesn't fit well **

\subsection{Bayesian Cognitive Hierarchy (BCH) model}

In what we call the Bayesian Cognitive Hierarchy (BCH) model, the participant attempts to learn the type of opponent they are facing through Bayesian learning. We assume the participant considers the opponent could be either a level 0, level 1, or level 2 player, and starts with a prior belief that each of these types is equally likely. They then use observations of the opponents actions to infer a posterior probability of each type:
$$P(\text{level}=k | \mathcal{D}_{t})  \propto  P(\mathcal{D}_{t}|\text{level}=k ) \times P(\text{level}=k)$$
where $\mathcal{D}_{t} = \{s_1,\ldots,s_t\}$ is the data available at time $t$. The likelihood is defined as
$$P(\mathcal{D}_{t}|\text{level}=k) = \prod_{j=1}^t \left( \theta \frac{1}{|\mathcal{A}_g|} + (1-\theta) f_k(o_j|s_{j})\right)$$
where $f_k(o_t|s_{t}) = 1$ if $o_t$ is the action taken by a level $k$ player when the previous round play was $s_t = (a_{t-1}, o_{t-1})$, and 0 otherwise. Note that the likelihood assumes (correctly) that there is a probability $\theta \in [0,1]$ that the opponent takes a random action. The posterior at time $t-1$ forms the prior at time $t$. We assume a participant chooses an action by using the softmax function over the best response to predicted actions:
$$\begin{aligned} B_t(a) &= \sum_{k = 0}^2 \sum_{o \in \mathcal{A}_g} b(a,o) P_k(o|s_{t})  P(\text{level}=k|\mathcal{D}_{t-1})\\
P_t(a) &= \frac{\exp\{\lambda B_t(a) \}}{\sum_{a' \in \mathcal{A}_g} \exp \{ \lambda B_t(a')\}} \end{aligned}$$
where $b(a,o) = 1$ if action $a$ is a best response to opponent's action $o$ (i.e. it leads to a win), and $P_k(o|a_{t-1},o_{t-1}) = \theta \frac{1}{|\mathcal{A}_g|} + (1-\theta) f_k(o|a_{t-1},o_{t-1})$ is the probability that a level $k$ agent takes action $o$, as also used in the likelihood above.

Unlike the models above, the BCH model allows for between-game transfer, as knowledge of the level of the opponent can be used to generate predictions in games that have not been played before. This generalization is done simply by using the posterior $P(\text{level} = k|\mathcal{D}_T)$ from the previous game as rhe prior distribution in the next game. However, the participant might also assume that the level of reasoning of their opponent does not generalize over games. This would mean starting with a "fresh" prior $P(\text{level} = k)$ at the start of each game. We hence distinguish between two versions of the BCH model. In the No-Between-Transfer (BCH\_NBT) variant, participants assume a uniform probability of the different levels at the start of each game (and hence do not transfer knowledge of their opponent between games). In the Between-Transfer model (BCH\_BT), participants use the posterior probability over the levels of their opponent as the prior at the start of a new game (i.e. complete transfer of the knowledge of their opponent). Both versions of the BCH model have two free parameters: the assumed probability that the opponent chooses a random action ($\theta$), and the temperature parameter of the softmax function ($\lambda$). 

## Estimation and model comparison

For both experiments, we fitted all models to individual participant data by maximum likelihood estimation. We use the Bayesian Information Criterion (BIC) to determine the best fitting model for each participant. <!--, the best fitting model for each participant was chosen and we compared the number of participants whose behavior was best explained by each model. -->

For Experiment 1, we fitted a total of 5 models: a baseline model assuming random play (Nash), the Bayesian Cognitive Hierarchy model allowing transfer between games (BCH\_BT) and without transfer between games (BCH\_NBT), as well as a Reinforcement Learning model with state space consisting of last round play (RL), and finally a self-tuning EWA model with the same state space (EWA).

In Experiment 2, because participants were interacting with each opponent twice within each game, we need to distinguish between two type of opponent model transfer. We can have transfer within games, between the first and second interaction with the opponent. We can also have transfer between games, as in e.g. transferring a learned opponent model from RPS to FWG. Therefore, we fitted a total of three versions of the Bayesian Cognitive Hierarchy model: BCH\_BT allows for both within and between game transfer (between game transfer without within game transfer is implausible); BCH\_NBT allows for within but not between game transfer; BCH\_NT allows for no transfer within or between games. For Reinforcement learning models, because RL models can't account for between game transfer due to change in state and action space, we can only have models that allowing for within game transfer (RL\_TR) or with no transfer within games (RL\_NT). Likewise, we fit both a self tuning EWA model with transfer between stages of the same game (EWA\_TR) or without transfer (EWA\_NT). Counting the base model with random play (Nash) we therefore fit a total of 8 models for Experiment 2. 


```{r load-comp-mod-distrib, include=FALSE}

exp1_comp_results <- read.csv(file="../exp1_all_results.csv")
exp1_comp_table <- table(exp1_comp_results[, "condition"],c("Nash","BCH_BT","BCH_NBT", "RL","EWA")[apply(exp1_comp_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")],1,which.min)])

 #write.csv(exp1_comp_table ,file="exp1_comp_table ",row.names = TRUE)
 kable(exp1_comp_table)
```



```{r exp1-cum-score-plots, include=FALSE}

exp1_dat = read.csv("../exp1_data.csv")

exp1_model_comp <- data.frame()
for(id in unique(exp1_dat$human_id)) {
  tdat <- subset(exp1_dat,human_id == id)
  tot_score <- sum(tdat$score)
  tot_time <- sum(tdat$human_rt)
  early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg"))
  #early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg" | game =="numbers") )
  tr_score <- sum(early_dat$score)
  id_results <- subset(exp1_comp_results, ID == id)
  min_BIC <- apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")],1,min)
  
  best_model <- c("Nash","BCH_BT","BCH_NBT", "RL","EWA")[apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")],1,which.min)]
  # 
  exp1_model_comp <- rbind(exp1_model_comp,
                       data.frame(
                         "human_id" = id,
                         "condition" = exp1_dat[exp1_dat$human_id==id,"condition"][1],
                         "Early_game_score" = tr_score,
                         "Total_score" = tot_score,
                         "Best_model" = best_model
                         #"Total_time" = sum(tdat$human_rt),
                       ))
}


datalist = list()
i = 0
new_dat <- setNames(data.frame(matrix(ncol = ncol(exp1_dat), nrow = 0)), colnames(exp1_dat))
for(id in unique(exp1_dat$human_id)) {
  i <- i+1
  tdat <- subset(exp1_dat,human_id == id)
  tdat$part_num <- i
  tdat <- within(tdat, acc_sum <- cumsum(tdat$score))
  datalist[[i]] <- tdat
}

# Merge all datasets into one 
new_dat <- dplyr::bind_rows(datalist)
# or new_dat <- data.table::rbindlist(datalist)

# Add column for time t
new_dat <- new_dat %>% dplyr::group_by(exp1_dat$human_id) %>% dplyr::mutate(t = row_number())

# Add best fitting model per participant
new_dat <- merge(new_dat, exp1_model_comp[, c("human_id", "Best_model")], by="human_id")



temp <- new_dat[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>% summarize(model_acc_sum = mean(acc_sum))



 

```

Figure\ \@ref(fig:exp1-comp-models)  shows the results for Experiment 1. We can see that the RL model clearly described most participants' behaviour best, followed by the random (Nash) model. Only a few participants were best described by one of the BCH models, or the EWA model. Looking at BIC weights, we confirm this as seen in  Figure \ \@ref(fig:xp1-BIC-weigths). RL models have high BIC weights when they best fit the participants, and very few instances have high BIC weights for models other then RL, which fits the picture drawn by the histogram.
```{r, include=FALSE}

#Only select models we're interested in 
exp1_comp_BICs <- exp1_comp_results[c("ID","Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")]

exp1_BIC_weights <- exp1_comp_BICs["ID"]
exp1_BIC_weights[,2:ncol(exp1_comp_BICs)] <- t(apply(exp1_comp_BICs[,-1], 1, function(i) exp(-0.5*(i-min(i)) )))
colnames(exp1_BIC_weights) <- c("ID","Nash","BCH_BT","BCH_NBT","RL","EWA")
exp1_BIC_weights[,-1] <- t(apply(exp1_BIC_weights[,-1], 1, function(i) round(i/sum(i),2)))

```

```{r exp1-comp-models, fig.cap = "Experiment 1 - Histogram of best fitting computational models by condition", out.width="\\textwidth", fig.align='center'}

#knitr::include_graphics("../Report/images/exp1_comp_models.png", dpi = 108)
par(las=2) # make label text perpendicular to axis
par(mar=c(5,8,4,2)) # increase y-axis margin.
barplot(exp1_comp_table,
        horiz =TRUE, # rotate barplot for better visibility 
        las=1, # change orientation x axis labels 
        cex.names=0.8, # text label size
        legend = rownames(exp1_comp_table),
        beside =TRUE,
        xlab="Number of participants",
        args.legend=list( # positioning of legend box 
        x  = 15,
        y  = 10,
        #x=ncol(exp1_comp_table) -3 ,
        #y=max(colSums(exp1_comp_table)) ,
        bty = "n")
 )

```

```{r, include= FALSE}
library(reshape2)
d <- melt(exp1_BIC_weights, id.vars="ID", variable.name = "Model",value.name = "BIC_weight")
```

(ref:figure9-caption) Model BIC weights for participants in Experiment 1.
```{r, xp1-BIC-weigths, fig.cap="(ref:figure9-caption)", fig.align="center"}
ggplot(d, aes(y = BIC_weight, x = 1)) + 
  geom_violin() +
  geom_jitter(width=.2, alpha=.6) + 
  facet_wrap(~Model) + theme_apa() + theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + xlab("") + ylab("BIC weight")
#  theme(legend.position = c(0.8, 0.3))



# Everything on the same plot
# ggplot(d, aes(ID,BIC_weight, col=Model)) + 
#   geom_point() + 
#   stat_smooth() +
#   facet_wrap(~Model) +
#   theme(axis.title.x=element_blank(),
#         axis.text.x=element_blank(),
#         axis.ticks.x=element_blank()) + 
#   theme(legend.position = c(0.8, 0.3))
  
# 
# Average_BIC_weights <- as.data.frame(t(round(apply(exp1_BIC_weights[,-1],2,mean),2)))
# row.names(Average_BIC_weights) <- c("Model BIC weights")
# Average_BIC_weights
# 
# #Count_Best_fit <- table(exp1_model_comp$Best_model)
# temp <- plyr::count(exp1_model_comp,"Best_model")
# Count_Best_fit <- tidyr::spread(temp,Best_model, freq)
# row.names(Count_Best_fit) <- c("Count best fit")
# Count_Best_fit
# 
# 
# table_exp_1 <- rbind.fill(Average_BIC_weights,Count_Best_fit)
# row.names(table_exp_1) <- c("Model BIC weights","Count best fit")
# table_exp_1 


```

```{r exp1weightstable, results="asis", include =FALSE, echo=FALSE, warning=FALSE}

######apa_table breaks knitr....not sure why.

#papaja::apa_table(table_exp_1, caption = "Experiment one Average BIC weights and number of participants best fit by model")

#knitr::kable(table_exp_1, caption = "Experiment 1 - Average BIC weights and number of participants best fit by model")

```



<!-- Next we compared the performance of players whose actions are best fit by each of our hypothesized models. Figure\ \@ref(fig:exp1-cumScores) shows the average cumulative performance of players across games, for participants grouped by which model best fits their behavior in experiment 1. We can see that participants whose actions are most consistent with learning a BCH opponent model in a Bayesian way had the best overall performance (without transfer), followed by RL, EWA. Random players had, understandably the lowest performance. -->

```{r exp1-cumScores, fig.cap = "Experiment 1 - Average cumulative scores of participants by best fitting model", fig.align='center'}
# p5 <- ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) + 
#    geom_line(aes(color= Best_model))  
# 
# p5 + labs(color = "Best fitting model", x = "Round Number", y="Accumulated score")
```


In Experiment 2, we can see from Figure\ \@ref(fig:exp2-comp-models) that the RL model was again more successful than the Bayesian Cognitive Hierarchy models (with or without transfer) in fitting participants' action choices. <!--  In Experiment 2 when participants faced both level-1 and level-2 agents sequentially, the Bayesian models (with or without transfer) did not fit players observed data as well. --> This is also reflected in BIC weights in Figure \@ref(fig:xp2-BIC-weigths). **MS: model labels in Figures should correspond to those in the text**

```{r, include = FALSE}

exp2_comp_results <- read.csv(file="../Experiment_2/exp2_results.csv")

exp2_comp_table <- table(exp2_comp_results[, "condition"],c("Nash","BCH BT","BCH NBT", "BCH NT", "RL NT","RL Tr","EWA NT","EWA Tr")[apply(exp2_comp_results[,c("Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no","QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")],1,which.min)])

 # write.csv(exp2_table_results,file="exp2_table_results.csv",row.names = TRUE)
exp2_comp_table
```

```{r exp2-comp-models, fig.cap = "Experiment 2 Histogram of best fitting computational models by condition", out.width="\\textwidth", fig.align='center'}

par(las=2) # make label text perpendicular to axis
par(mar=c(5,8,4,2)) # increase y-axis margin.
barplot(exp2_comp_table,
        horiz =TRUE, # rotate barplot for better visibility 
        las=1, # change orientation x axis labels 
        cex.names=0.8, # text label size
        beside =TRUE,
        xlab="Number of participants",
        #legend = c("level-1 opp first","level-2 opp first"),
        legend = paste0(rownames(exp2_comp_table) , " faced first"),
        args.legend=list( # positioning of legend box 
        x  = 15,
        y  = 10,
        # x=ncol(exp2_comp_table) + 6,
        # y=max(colSums(exp2_comp_table)) +1,
        bty = "n")
 )

```



```{r, include=FALSE}

dat_exp2 = read.csv("../Experiment_2/data_exp2.csv")

#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- ifelse(as.character(dat_exp2$game) == "shootout", dat_exp2$score - 0.333, dat_exp2$score)

# Build dataframe comparing models BIC for each participant 
exp2_model_comp <- data.frame()
for(id in unique(dat_exp2$human_id)) {
  tdat <- subset(dat_exp2,human_id == id)
  tot_score <- sum(tdat$score)
  early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg" | game =="numbers") )
  tr_score <- sum(early_dat$score)
  id_results <- subset(exp2_comp_results, ID == id)
  
  # MS BAYES MODELS
  min_BIC <- apply(id_results[,c("Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no","QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")],1,min)
  
  best_model <- c("Nash","BCH BT","BCH NBT", "BCH NT", "RL NT","RL Tr","STEWA NT","STEWA Tr")[apply(id_results[,c("Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no","QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")],1,which.min)]
  
  exp2_model_comp <- rbind(exp2_model_comp ,
                       data.frame(
                         "human_id" = id,
                         "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
                         "Early_game_score" = tr_score,
                         "Total_score" = tot_score,
                         "Best_model_2" = best_model,
                         "Total_time" = sum(tdat$human_rt),
                         # "BT_minus_NBT_BIC" = id_results[,"Btwn_TR_BIC"] - id_results[,"No_Btwn_Tr_BIC"],
                         "BT_minus_NBT_BIC" = id_results[,"Distinct_game_BIC"] - id_results[,"Distinct_stage_BIC"],
                         "Rand_minus_best_BIC" =  id_results[,"Random_BIC"] - min_BIC

                       ))
}



datalist2 = list()
i = 0

#Build empty dataframe with same names as original data
exp2_cum_score <- setNames(data.frame(matrix(ncol = ncol(dat_exp2), nrow = 0)), colnames(dat_exp2))
for(id in unique(dat_exp2$human_id)) {
  i <- i+1
  tdat <- subset(dat_exp2,human_id == id)
  tdat$part_num <- i
  # NB : USE ADJUSTED SCORE IN SHOOTOUT
  tdat <- within(tdat, acc_sum <- cumsum(tdat$adj_score))
  datalist2[[i]] <- tdat
}

# Merge all datasets into one 
exp2_cum_score <- dplyr::bind_rows(datalist2)
# or exp2_cum_score <- data.table::rbindlist(datalist)

# Add column for time t
exp2_cum_score <- exp2_cum_score %>% group_by(human_id) %>% mutate(t = row_number())

# Participant number as a factor 
# tdat$part_num <- as.factor(tdat$part_num)

# Add best fitting model per participant
exp2_cum_score <- merge(exp2_cum_score, exp2_model_comp[, c("human_id", "Best_model_2")], by="human_id")

temp2 <- exp2_cum_score[,c("t","acc_sum","Best_model_2","condition","part_num")]

dat_by_model_2 <- temp2 %>% group_by(Best_model_2,t) %>% 
  summarize(avg_acc_sum = mean(acc_sum))

```


```{r, include=FALSE}

#Only select models we're interested in 
exp2_comp_BICs <- exp2_comp_results[c("ID","Random_BIC","Distinct_game_BIC","Distinct_stage_BIC","Bayes_distinct_no","QL_states_BIC","QLS_within_Tr_BIC","STEWA_NT_BIC","STEWA_Tr_BIC")]


exp2_BIC_weights <- exp2_comp_BICs["ID"]
exp2_BIC_weights[,2:ncol(exp2_comp_BICs)] <- t(apply(exp2_comp_BICs[,-1], 1, function(i) exp(-0.5*(i-min(i)) )))
colnames(exp2_BIC_weights) <- c("ID","Nash","BCH BT","BCH NBT", "BCH NT", "RL NT","RL Tr","EWA NT","EWA Tr")
exp2_BIC_weights[,-1] <- t(apply(exp2_BIC_weights[,-1], 1, function(i) round(i/sum(i),2)))

# Get dataframe of average BIC weights by model 
Average_BIC_weights_2 <- as.data.frame(t(round(apply(exp2_BIC_weights[,-1],2,mean),2)))
row.names(Average_BIC_weights_2) <- c("Model BIC weights")


# Get dataframe of counts of participants fit by each model
temp <- plyr::count(exp2_model_comp,"Best_model_2")
Count_Best_fit_2 <- tidyr::spread(temp,Best_model_2, freq)
#Count_Best_fit_2


#Combine in one table
table_exp_2 <- rbind.fill(Average_BIC_weights_2,Count_Best_fit_2)
table_exp_2[is.na(table_exp_2)] <- 0
rownames(table_exp_2) <- c("BIC weights","Count best fit")

table_exp_2[2,] <- round(table_exp_2[2,],0)
table_exp_2

```

(ref:my-table2-caption) Experiment 2 BIC weights and number of participants best fit by model.
```{r exp2-weights-table, results="asis", echo=FALSE, include =FALSE}

# papaja::apa_table(
#   table_exp_2
#   , caption = "(ref:my-table2-caption)"
#   , escape = FALSE)

knitr::kable(table_exp_2, caption = "Experiment 2 - Average BIC weights and number of participants best fit by model")  %>% 
  kable_styling(latex_options = c("scale_down"))
```
```{r, include= FALSE}
library(reshape2)
d <- melt(exp2_BIC_weights, id.vars="ID", variable.name = "Model",value.name = "BIC_weight")
```

(ref:figure11-caption) BIC weights for each model and participant in Experiment 2.
```{r, xp2-BIC-weigths, fig.cap="(ref:figure11-caption)", fig.align="center", out.width="\\textwidth"}
ggplot(d, aes(y = BIC_weight, x = 1)) + 
  geom_violin() +
  geom_jitter(width=.2, alpha=.6) + 
  facet_wrap(~Model) + theme_apa() + theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank()) + xlab("") + ylab("BIC weight")

# # Everything on the same plot
# ggplot(d, aes(ID,BIC_weight, col=Model)) + 
#   geom_point() + 
#   stat_smooth() +
#   facet_wrap(~Model) +
#   theme(axis.title.x=element_blank(),
#         axis.text.x=element_blank(),
#         axis.ticks.x=element_blank()) + 
#         theme(legend.position = c(0.8, 0.17), 
#         legend.direction = "vertical",
#         legend.key.size = unit(0.4, "cm"),
#         legend.key.width = unit(0.6,"cm")) 
#         # + guides(fill=guide_legend(nrow=4,ncol=2))
```




```{r exp2-cumScores, fig.cap = "Experiment 2 Average cumulative scores of participants by best fitting model", include =FALSE, fig.align='center', out.width="\\textwidth"}

 p8 <- ggplot(data = dat_by_model_2, aes(x = t, y=avg_acc_sum, group = Best_model_2)) + 
   geom_line(aes(color= Best_model_2)) +
  scale_x_continuous(minor_breaks = seq(0 , 180, 10), breaks = seq(0, 180, 60))

p8 + labs(color = "Best fitting model", x = "Round Number", y="Accumulated score")

```

<!-- Plotting cumulative scores by best model for experiment 2, we see very similar results looking at Figure\ \@ref(fig:exp2-cumScores), in that participants whose behavior was best fit by a BCH model of learning the opponent strategy had the highest cumulative performance. Out of these BCH models, the one in which there is within-game but no between-game transfer (NBT) had the best cumulative performance (although it only fit 2 participants best), followed by a model in which both within and between transfer of opponent models is allowed (BT). The next best model from a performance perspective was a Q-learning model with states and within game transfer, followed by BCH models where players reset opponent models at each stage of each game (NT). As expected, random play was at the bottom of cumulative performance. -->




## Using Hidden Markov Model to explore strategy switching

The computational modelling indicates that most players are best fit by a reinforcement learning which learns good actions conditional upon the last round play.  This is at odds with the behavioural findings, where we found evidence of transfer in early rounds of each game. If indeed most participants adopt a reinforcement learning strategy, they should not be able to transfer their learning to the early rounds of a new game. In order to investigate this discrepancy further, we plot the likelihood by trial for each game and three strategies: RL, Bayesian Cognitive Hierarchy models with between-game transfer, and the random (Nash) strategy. Figure\ \@ref(fig:exp1-lik-by-tr) shows that in the later games of Experiment 1, the likelihood for the BCH models is higher than that of the other models in the initial rounds of the games. However, over time, the likelihood of the RL model increases and exceeds that of BCH model.
```{r, include= FALSE}

exp1_lik_by_trial  <- read.csv("../exp1_lik_by_trial.csv")

exp1_lik_by_trial <- exp1_lik_by_trial %>%
    dplyr::mutate(Nash_lik = ifelse( (game == "rps" | game == "fwg"),1/3, 1/5))

mean_lik <- exp1_lik_by_trial %>%
  dplyr::select(human_id,condition,game, round, Nash_lik, QLS_lik, Bayes_Tr_lik) %>%
  group_by(condition,game,round) %>%
  dplyr::summarise(mean_Nash_lik = mean(Nash_lik, na.rm = TRUE),
                   mean_QLS_lik = mean(QLS_lik, na.rm = TRUE),
                   mean_Tr_lik = mean(Bayes_Tr_lik, na.rm = TRUE))

mean_lik$game <- factor(mean_lik$game,levels=c("rps","fwg","numbers"))

data_long <- tidyr::gather(mean_lik, strategy, probability, mean_Nash_lik:mean_Tr_lik, factor_key=TRUE)


```
```{r exp1-lik-by-tr, fig.cap = "Experiment 1 Likelihood by trial by game and opponent faced", fig.align='center'}
# make sure the different games are ordered in the way they were played
# data_long$game <- factor(data_long$game,levels=c("rps","fwg","numbers"))
ggplot(data_long,aes(x=round,y=probability,colour=strategy)) +
  geom_line() +
  facet_grid(game~condition) +
  scale_colour_discrete(name = "Strategy", labels = c("Nash", "RL", "BCH_BT"))

```
The same pattern holds for Experiment 2 (Figure\ \@ref(fig:exp2-lik-by-tr)). Again, the BCH model with between-game transfer has the highest likelihood in the early stages of the later games (apart from stage 1 of the shootout game, where the RL model is better). In later rounds, the likelihood of the RL model exceeds that of the BCH model. <!-- second (most similar) game, however the likelihood of Q-learning with states models increases steadily to be the highest in the later stages of all games. In the third and more dissimilar game, we get a result that is different from experiment 1. In this instance, the likelihoods of the BCH models stay constant and close to their initial values.-->


```{r, include = FALSE }

exp2_lik_by_trial <- read.csv("../Experiment_2/exp2_lik_by_trial.csv")

exp2_lik_by_trial <-  exp2_lik_by_trial %>% dplyr::mutate(Nash_lik = 1/3)

mean_lik_2 <- exp2_lik_by_trial %>%
  dplyr::select(human_id,game,stage,round_condition, round, Nash_lik, QLS_lik_within, Bayes_Tr_lik) %>%
  dplyr::group_by(game,stage,round) %>%
  dplyr::summarise(mean_Nash_lik = mean(Nash_lik, na.rm = TRUE),
             mean_QLS_within_lik = mean(QLS_lik_within, na.rm = TRUE),
             mean_BCH_Tr_lik = mean(Bayes_Tr_lik, na.rm = TRUE))

mean_lik_2$game <- factor(mean_lik_2$game,levels=c("rps","fwg","shootout"))

data_long2 <- gather(mean_lik_2, strategy, probability, mean_Nash_lik:mean_BCH_Tr_lik, factor_key=TRUE)
```

```{r exp2-lik-by-tr, fig.cap = "Experiment2 likelihood by trial by game and opponent faced", fig.align ='center'}
# make sure the different games are ordered in the way they were played
# data_long2$game <- factor(data_long$game,levels=c("rps","fwg","shootout"))
ggplot(data_long2,aes(x=round,y=probability,colour=strategy)) +
  geom_line() +
  facet_grid(game~stage) +
  scale_colour_discrete(name = "Strategy", labels = c( "Nash", "RL + within Tr", "BCH + within & btw Tr"))

```

The fact that the likelihoods of the main strategies considered cross over in both experiments could be interpreted as indicative that participants switch between strategies as the games progress. According to this interpretation, participants base their responses in early rounds on the learned level of their opponents iterative reasoning, switching later to learned actions through reinforcement. <!--use a Bayesian Cognitive Hierarchy Indeed, in both experiments, following our results, it seems that in the earlier stages of the latter games, the BCH based strategies fitted observed action choices better than Q-learning based ones, with a reversal of the roles in later stages.-->

We use hidden Markov models to more formally test for strategy switching in participants' play. In these models, the three strategies (RL, BCH with between-game transfer, and Nash) correspond to latent states which determine the overt responses (actions chosen). The models allow for switching between the states over time, and such switches correspond to strategy switches. Hidden Markov models assume that an observable action at time $t$ depends on a latent state at time $t$. Second, it is assumed that the latent state at time $t$ depends on the latent state at the previous time $t-1$. The model is specified by the state-conditional action distributions (these are provided by the likelihood of the fitted models), an initial state distribution (the distribution over the strategies at the initial round), and the state-transition probabilities (probability of switching from one state/strategy to another). Initial state probabilities and the transition probabilities were estimated with the depmixS4 package [@R-depmixS4]. As a statistical test of strategy switching, we compare the hidden Markov model to a constrained version which assumes the probability of switching from one strategy to a different one is 0. This model thus assumes that when players start with a particular strategy, they continue using it throughout the experiment.

<!-- In the first model, we allow for a non-nil probability of players transitioning from one state (strategy) to another. In the second model, we assume that such switching does not happen, and as such assume implicitly that when players start with a particular strategy, they continue using it throughout the experiment. We then compare the likelihoods of each HMM model using a likelihood ratio test.

that this hidden process has a Markov property, meaning that given state $S_{t-1}$, the value of $S_{t}$ is independent of all states occurring before time $t-1$. We also assume that $S_{t}$ has a discrete probability distribution in that it take one of K discrete values. The model is therefore specified by initial probabilities of being in each state ${1,2,...,K}$ and transition probabilities for moving from state $i$ to state $j$. These probabilities are fit using observed actions generated from these hidden states.


, we fitted Hidden Markov Models in which the latent states are the 3 strategies used (Q-learning with state space consisting of previous round play, BCH based model with opponent model transfer, and a base model consisting of random play consistent with a Nash equilibrium strategy). Hidden Markov models are useful tools to explore structure in observed time series. They are named as such because of two properties: First, they make the assumption that any observable action at time t results from a process whose state at time t , named $S_{t}$ is "hidden" from the observer. Second, it also assumes that this hidden process has a Markov property, meaning that given state $S_{t-1}$, the value of $S_{t}$ is independent of all states occurring before time $t-1$. We also assume that $S_{t}$ has a discrete probability distribution in that it take one of K discrete values. The model is therefore specified by initial probabilities of being in each state ${1,2,...,K}$ and transition probabilities for moving from state $i$ to state $j$. These probabilities are fit using observed actions generated from these hidden states.

To investigate the possibility of strategy switching, we fit two different hidden Markov models with the depmixS4 R package. In the first model, we allow for a non-nil probability of players transitioning from one state (strategy) to another. In the second model, we assume that such switching does not happen, and as such assume implicitly that when players start with a particular strategy, they continue using it throughout the experiment. We then compare the likelihoods of each HMM model using a likelihood ratio test.
-->


```{r define-dummy-response, include=FALSE}
setClass("dummyResponse", contains="response")

setGeneric("dummyResponse", function(y, pstart = NULL, fixed = NULL, ...) standardGeneric("dummyResponse"))

setMethod("dummyResponse", 
    signature(y="ANY"), 
    function(y,pstart=NULL,fixed=NULL, ...) {
      y <- matrix(y,length(y))
  		x <- matrix(1)
  		parameters <- list()
  		npar <- 0
      mod <- new("dummyResponse",parameters=parameters,fixed=logical(0),x=x,y=y,npar=npar)
      mod
	}
)

setMethod("show","dummyResponse",
    function(object) {
        cat("Dummy for fixed likelihood Model \n")
    }
)

setMethod("dens","dummyResponse",
  function(object,log=FALSE) {
   if(log) log(as.numeric(object@y)) else as.numeric(object@y)
  }
)

setMethod("getpars","dummyResponse",
    function(object,which="pars",...) {
        switch(which,
            "pars" = {
                pars <- numeric(0)
            },
            "fixed" = {
                pars <- logical(0)
            }
        )
        return(pars)
    }
)

setMethod("setpars","dummyResponse",
    function(object, values, which="pars", ...) {
        npar <- npar(object)
        if(length(values)!=npar) stop("length of 'values' must be",npar)
        # determine whether parameters or fixed constraints are being set
		nms <- ""
		switch(which,
		  "pars"= {
		      },
		  "fixed" = {
		    }
		  )
      names(object@parameters) <- nms
      return(object)
    }
)

setMethod("fit","dummyResponse",
    function(object,w) {
		  return(object)
	}
)

setMethod("predict","dummyResponse", 
    function(object) {
        ret <- object@y
        return(ret)
    }
)
```

```{r, include=FALSE}

## EXPERIMENT 1 Data 

Nash_lik <- exp1_lik_by_trial$Nash_lik
RL_lik <- exp1_lik_by_trial$QLS_lik
Bayes_Tr_lik <- exp1_lik_by_trial$Bayes_Tr_lik
# QLS_lik[which(QLS_lik == 0)] <- .001
# QLS_lik[is.na(QLS_lik)] <- .33 # this is a hack; there shouldn't be any missing values



nsubject <- length(unique(exp1_lik_by_trial$human_id)) # number of participants
ngame <- 3 # number of games
ntrial <- c(50,50,50) # numer of trials in each game

rModels <- list(
  list(
	  dummyResponse(RL_lik)
	),
	list(
		dummyResponse(Nash_lik)
	),
  list(
    dummyResponse(Bayes_Tr_lik)
  )
)

trstart <- matrix(c(0.8,0.1,0.1,0.1,0.8,0.1,.1,.1,.8),ncol=3)
transition <- list()
transition[[1]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[1,],family=multinomial("identity"))
transition[[2]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[2,],family=multinomial("identity"))
transition[[3]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[3,],family=multinomial("identity"))

instart <- c(1/3,1/3,1/3)
inMod <- transInit(~1,nstates=3,pstart=instart,family=multinomial("identity"),data=data.frame(rep(1,nsubject*ngame)))

mod1 <- makeDepmix(response=rModels,transition=transition,prior=inMod,ntimes=rep(ntrial,nsubject))

fmod1 <- fit(mod1, emcontrol=em.control(random.start=FALSE))
```


```{r no-switch, include=TRUE}

# No switching. Force off diagonal initial elements of transtion matrix to 0
trstart <- matrix(c(1,0,0,0,1,0,0,0,1),ncol=3)
transition <- list()
transition[[1]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[1,],family=multinomial("identity"))
transition[[2]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[2,],family=multinomial("identity"))
transition[[3]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[3,],family=multinomial("identity"))

mod1_noswitch <- makeDepmix(response=rModels,transition=transition,prior=inMod,ntimes=rep(ntrial,nsubject))

fmod1_noswitch <- fit(mod1_noswitch, emcontrol=em.control(random.start=FALSE))

# p-value for comparison between a model with strategy switches and one without:

llratio(fmod1,fmod1_noswitch)

1-pchisq(-2*as.numeric(logLik(fmod1_noswitch)) - (-2*as.numeric(logLik(fmod1))),df=6)
```

```{r, include = FALSE}

exp1_lik_by_trial["post_RL"] <- forwardbackward(fmod1)$gamma[,1]
exp1_lik_by_trial["post_Nash"] <- forwardbackward(fmod1)$gamma[,2]
exp1_lik_by_trial["post_Bayes_Tr"] <- forwardbackward(fmod1)$gamma[,3]

mean_post_1 <- exp1_lik_by_trial %>%
  dplyr::select(human_id,game,condition, round, post_Nash, post_RL, post_Bayes_Tr) %>%
  dplyr::group_by(game,condition, round) %>%
  dplyr::summarise(Nash_posterior = mean(post_Nash, na.rm = TRUE),
                   RL_posterior = mean(post_RL, na.rm = TRUE),
            BCH_BT_posterior = mean(post_Bayes_Tr, na.rm = TRUE))

mean_post_1$game <- factor(mean_post_1$game,levels=c("rps","fwg","numbers"))

data_long_post1 <- gather(mean_post_1, strategy, probability, Nash_posterior, RL_posterior,BCH_BT_posterior, factor_key=TRUE)


```

```{r exp1-posteriors-plot, fig.cap = "Experiment1 posterior probability of strategies by game and opponent faced", fig.align ='center'}

ggplot(data_long_post1,aes(x=round,y=probability,colour=strategy)) +
  geom_line() +
  facet_grid(game~condition)

```

In Experiment 1, a likelihood-ratio test shows that the HMM model with switching fits significantly better than the non-switching one ($p < .001$). **MS: we should really use a bootstrapped Likelihood Ratio test here ** This provides further statistical evidence in favour of the hypothesis that participants switch between strategies. Figure\ \@ref(fig:exp1-posteriors-plot) depicts the average (across participants) posterior probabilities of each state (strategy), as a function of trial and opponent faced. As can be seen, there is evidence of strategy switching in the FWG and Numbers games: Initially, participants appear to use a random strategy (in the first round of a game, there is no way to predict the opponent's action), after which the BCH strategy becomes dominant. In the later rounds of the games, the RL strategy becomes dominant, however. <!--This is then when there is no past play. The posterior probability is the probability that an observation comes from a component distribution a posteriori, i.e. given the value of the observation. In the first experiment, we can see from the plots of FWG and numbers games for level-1 opponent that although the likelihoods are very close, the posterior probability of the Bayesian model with transfer is slightly higher than that of the RL model in the very early rounds, but decreases rapidly while the posterior probability of the QL-learning with states models keeps increasing. -->


```{r, include =FALSE}
## EXPERIMENT 2 DATA 
# we need to have "dat" available and lik_by_trial
RL_lik <- exp2_lik_by_trial$QLS_lik
Nash_lik <- exp2_lik_by_trial$Nash_lik
Bayes_Tr_lik <- exp2_lik_by_trial$Bayes_Tr_lik

nsubject <- length(unique(exp2_lik_by_trial$human_id)) # number of participants
ngame <- 12 # number of games
#ntrial <- c(60,60,60)

ntrial <- c(20,20,10,10,20,20,10,10,20,20,10,10) # number of trials in each game

rModels <- list(
  list(
	  dummyResponse(RL_lik)
	),
	list(
		dummyResponse(Nash_lik)
	),
  list(
    dummyResponse(Bayes_Tr_lik)
  )
)

trstart <- matrix(c(0.8,0.1,0.1,0.1,0.8,0.1,.1,.1,.8),ncol=3)
transition <- list()
transition[[1]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[1,],family=multinomial("identity"))
transition[[2]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[2,],family=multinomial("identity"))
transition[[3]] <- transInit(~1,nstates=3,data=data.frame(1),pstart=trstart[3,],family=multinomial("identity"))

instart <- c(1/3,1/3,1/3)
inMod <- transInit(~1,nstates=3,pstart=instart,family=multinomial("identity"),data=data.frame(rep(1,nsubject*ngame)))

mod <- makeDepmix(response=rModels,transition=transition,prior=inMod,ntimes=rep(ntrial,nsubject))

fmod <- fit(mod, emcontrol=em.control(random.start=TRUE))
```

```{r}

exp2_lik_by_trial["post_RLw"] <- forwardbackward(fmod)$gamma[,1]
exp2_lik_by_trial["post_Nash"] <- forwardbackward(fmod)$gamma[,2]
exp2_lik_by_trial["post_Bayes_Tr"] <- forwardbackward(fmod)$gamma[,3]

mean_post_2 <- exp2_lik_by_trial %>%
  dplyr::select(human_id,game,stage,round_condition, round, post_Nash, post_RLw, post_Bayes_Tr) %>%
  dplyr::group_by(game,stage, round) %>%
  dplyr::summarise(Nash_posterior = mean(post_Nash, na.rm = TRUE),
                   RL_posterior = mean(post_RLw, na.rm = TRUE),
            BCH_BT_posterior = mean(post_Bayes_Tr, na.rm = TRUE))

mean_post_2$game <- factor(mean_post_2$game,levels=c("rps","fwg","shootout"))

data_long_post2 <- gather(mean_post_2, strategy, probability, Nash_posterior, RL_posterior, BCH_BT_posterior, factor_key=TRUE)


```

```{r exp2-posteriors-plot, fig.cap = "Experiment2 likelihood by trial by game and opponent faced", fig.align ='center'}

ggplot(data_long_post2,aes(x=round,y=probability,colour=strategy)) +
  geom_line() +
  facet_grid(game~stage)

```

The switching model in Experiment 2 is also significantly better than the restricted non-switching model ($p < .001$).  <!-- On top of indications from looking at the likelihood by trial graphs, we have therefore further evidence that participants did indeed switch their strategies as the games progressed. --> The posterior probabilities of the strategies (Figure\ \@ref(fig:exp2-posteriors-plot)) show very clear evidence of strategy switching across games and stages, from using a BCH model in the initial rounds to an RL strategy later on. <!--. The switching also seems to happen very early on at the beginning of each game and stage, and is also consistently in the same direction: The probability of Bayesian models with transfer being initially high, then decreasing rapidly while the posterior probability of QL-Learning with states and within transfer learning increases rapidly. 

Therefore, HMM modelling shows clear evidence in favour of strategy switching by participants, specifically after a few rounds of play. The strategy switching is consistently from BCH models towards Q-learning with states models in both experiments.

\newpage

-->

# Discussion

In this study, we investigated human learning transfer across games by making human participants play against computer agents with limited levels of iterated reasoning. We were interested whether participants learn about the strategy of their opponent and transfer such knowledge between games, and whether this is modulated by the similarity between games and the sophistication of the agent.

The results of our first experiment show that the majority of participants learn to adapt to the opponent strategy over multiple interactions and generalise this learning to a similar game. Performance in early rounds indicated that transfer to the more dissimilar game was moderated by the degree of sophistication of the opponent, with evidence for transfer when players face the less sophisticated agent but not the more sophisticated one. In the second experiment, participants faced both types of opponents, which allows for a stronger test of opponent modelling, as participants would need to learn a different strategy for each opponent within a game. In Experiment 1, participants could learn a single strategy for each game, making opponent modelling possibly less pertinent. Experiment 2 alsp offers more opportunities to test transfer of a learnt opponent model. There were two opportunities to transfer opponent models within each game as well as two opportunities to transfer from each game to the next, giving a total of 6 transfer opportunities. **MS: we did not test transfer at all these points, or did we?** The results on learning transfer confirmed the findings from the first experiment. Again, there was clear evidence of transfer in the early rounds of the later games. <!--While there was no evidence of higher scores across interactions within the same game (likely due to the lower number of rounds per interaction and the higher cognitive load of facing two opponents rather than one), we found evidence for learning transfer across games as early round scores analysis confirmed. --> We also found that learning transfer is moderated by the type of opponent faced: Evidence of transfer was weaker for the level-2 opponent as compared to the level-1 opponent. That transfer was less evident for the more sophisticated level-2 opponent in both experiments may be due to a higher difficulty of learning that opponent's strategy. If it is more difficult to establish a model of the level-2 opponent, there is likely less knowledge to transfer to the new game. A player cannot transfer what they have not learnt. <!-- and as such, since it might be harder to learn the strategy of the level-2 opponent, this in turn might translate into weaker evidence for transfer. . When the players faced a level-1 opponent, they were able to transfer learning. However, when they faced a level-2 opponent, there was weaker evidence for transfer. The lack of transfer when facing the more sophisticated opponent might be due to the difficulty of learning that opponent strategy to start with. A player cannot transfer what they have not learnt and as such, since it might be harder to learn the strategy of the level-2 opponent, this in turn might translate into weaker evidence for transfer. -->

<!-- Coming back to learning transfer, we observed evidence that participants start off new games with prior knowledge as their scores are significantly higher than chance, confirmed both by early stage analysis as well as rounds 2-6 scores analysis. The question we ask ourselves therefore is: -->

What exactly did the players learn in RPS that allowed them to beat the opponent in FWG and Shootout? what did the players learn specifically about their opponent's strategy and what form did this learning take?

<!-- We will proceed by considering multiple potential answers to this question. A possible hypothesis for learning the opponent’s strategy is the use of -->

One possible answer is that participants learned simple rules based on last round play. For instance, "play scissors whenever my opponent played rock in last round", or "play paper whenever the last round play was either rock or scissors". These are the type of strategies that are learned by the model-free reinforcement learning we used in our computational modelling. While this strategy fitted participants' actions the best overall, there are at least two reasons why this account is not satisfactory as a complete description of participants' behaviour. Firstly, the learned strategies are not transferable to new games. There is no simple way to map "play scissors whenever my opponent played rock in last round" in the RPS game to "play grass whenever my opponent played fire in last round". Such a mapping may be possible by translating the rules and structure from RPS to FWG, but model-free reinforcement learning lacks the tools to do this. Model-free reinforcement learning would need to start from scratch in each new game, yet we found evidence that participants could successfully exploit their opponent's strategy in early rounds of new games. Secondly, a reinforcement learning strategy would fare equally well against the level-1 and level-2 opponent. Whilst choosing different actions, the contingency between the state (last round play) and actions is the same for both opponents. Yet, we found that participants performed better against the level-1 opponent compared to the level-2 opponent. The difference in performance between the two types of opponent indicate that the actions of the more sophisticated level-2 opponent, or the best response to these, were somehow more difficult to predict.

<!--
it is unsatisfactory in explaining some of the learning transfer evidence we showed. Indeed, learning the best action in a particular state is not transferable to a new game since the state space is different and there is no single mapping between the state spaces of the initial and latter games. These rules would therefore need to be learned anew in the latter game which is inconsistent with above chance performance in very early rounds. 

Likewise, assuming that players learn a complete model of the environment (for instance the transition probabilities from last round play to new play) might explain learning within games but is equally unable to account for early games transfer of learning as such models, besides being cognitively very expensive to learn, would require many rounds of practice. Another issue with these hypotheses is that they are not consistent with significant score differences between those facing level-2 and level-1 opponents. More specifically, if we assume that participants were using some type of associative learning or relying on spatial heuristics, then their scores should not depend on the degree of strategic sophistication of the opponent since their approaches would render this variable irrelevant. To be sure, if a participant learns to pick say "scissors" whenever the opponent last picked "rock", then the degree of strategic sophistication of the opponent (its level k) should not impact this learning, and we would expect in this case there would be no difference between scores when facing level-1 and level-2 opponents, which is not the case here. The fact that the degree of sophistication of the opponent matters points to the importance of opponent modelling to successful transfer of learning. 
-->

We are left with two possible explanations: First, it is possible that participants discovered a heuristic that allowed them to beat their opponent without explicitly modelling their strategy, and that this heuristic is transferable to new games. Because of the cyclicity in action choices (e.g., rock beats scissors, scissors beats paper, paper beats rock), it is possible to beat a level-2 opponent most of the time by following a simple rule: Play in the next round whatever the opponent played in the last round. This is a rule that wins and is transferable to other games as it does not depend on action labels. In the same vein, a heuristic that beats a level-1 player can be stated as "Choose the action that would have been beaten by my previous action". Intuitively, it seems that the heuristic for the level-2 player is simpler than that for the level-1 player, which is at odds with the difference in performance.

A second explanation is that participants engaged in iterative reasoning, inferring their opponent's beliefs and countering the resulting actions. For instance, this would be reasoning of the form "My opponent expects me to repeat my last action, choosing an action that would beat my last action. Hence, I will choose the action that beats their best response" or "My opponent thinks I expect them to repeat their action, hence expecting me to choose the action that beats their last action. They will therefore choose the action that beats this one, and hence I should choose the action that beats their best response." Beating a level-1 player, in this account, requires being a level-2 player, and beating a level-2 player requires being a level-3 player. Intuitively, the additional step of iterative reasoning involved in beating a level-2 player makes the level-3 strategy more demanding and difficult to implement, which is consistent with the lower performance against the level-2 opponent.

The differences in performance between the two players, coupled with the finding of positive transfer, point to participants engaging in iterative reasoning, and learning something useful about their opponent's limitations in this regard. This is the type of learning encapsulated by our Bayesian Cognitive Hierarchy model. It involves the evaluation of explicit hypotheses and results in better problem-solving skills (Mandler, 2004). Since it is less context dependent, this type of learning is generalizable to new situations, akin to the more general framework of rule-based learning explored by Stahl (2000, 2003). **MS: need to check relevance of these citations. Also, they need to be in reference list ** We admit that our implementation in the BCH models does not predict a performance difference between the types of opponents. Starting with an equal prior belief over the different levels of sophistication, a BCH player would perform equally well against the level-1 and level-2 opponent. There are two routes to explain the difference in performance. Firstly, prior beliefs might be biased against higher-level opponents (i.e., participants might have believed it is more likely that they would face level-1 opponent than a level-2 opponent). Secondly, if the actions of a level-2 opponent are more difficult to predict than those of a level-1 opponent, this might introduce more noise in the likelihood of the opponents actions given their level of sophistication. Either of these mechanisms would explain why learning the strategy of the level-2 opponent is more difficult and slower than learning the strategy of the level-1 opponent. **MS: This could be something to implement, actually** Using hidden Markov models, we found evidence of strategy switching between the BCH and RL strategies, and such switching seems more consistent with the latter idea. If predicting an opponent's actions through iterative reasoning is cognitively demanding and error-prone, it is resource-rational to switch to less costly yet equally successful strategies when these are available. **MS: add reference to resource-rational approach by Lieder & Griffiths ** Initially, a model-free reinforcement learning strategy will be less successful than an iterative reasoning one. However, given enough experience, it will be on par with an iterative reasoning strategy. As it involves simple state-action contingencies, a model-free RL strategy may also be computationally less costly, making it overall more effective to rely on this than iterative reasoning. This is similar to the arbitration between model-free and model-based RL [@Simon_Daw_11] **MS: add more references here**. In repeated and well-practised situations, relying on habits allows one to save cognitive resources for other demands. However, when the environment -- or game -- changes, it is prudent to use all available resources to reorient oneself.

<!--
building a mental representation of their opponent's strategy. 

of learning transfer is that it is driven by a group of participants that are able to build a mental representation of what the strategy of the opponent is. A successful mental representation would take the perspective of the opponent or endow it with intentionality in order to detect its strategy when the opponent is playing based on a level-k reasoning model. For instance, the player may think “My opponent is always trying to be one step ahead of me, therefore, I will be one step ahead of where it thinks I will be”. This mental representation would facilitate the use of theory of mind abilities and thus enable the players to learn opponent strategies when they are based on human-like reasoning models such as level-k or cognitive hierarchy. This type of learning would be deemed “explicit” in the psychology literature as a process through which knowledge consists of cognitive representations of concepts and rules, as well as the relationship between them. It involves the evaluation of explicit hypotheses and results in better problem-solving skills (Mandler, 2004). Since it is less context dependent, this type of learning is generalizable to new situations, akin to the more general framework of rule-based learning explored by Stahl (2000, 2003).

Our second experimental design allows us to test whether the first explanation holds. Since there is a simple transferable heuristic that works against level-2 players, and since as far as we know, there are no similar ones against level-1 players, if indeed participants were using this, they would perform better and transfer learning more easily when facing level-2 opponents. Because level-2 opponents use a higher level of strategic reasoning, they should in fact be harder to play against and in the absence of such a heuristic, performance and learning transfer should be worse.

Our results show that in fact, it was harder to transfer learning when facing level-2 opponents, both comparing first interactions across games and using early rounds analysis. Based on our assumptions, we conclude therefore that the most likely explanation is that participants who are able to beat the opponent and transfer learning are likely to be explicitly modelling the opponent strategy using level-k reasoning, compared to using simple learning rules they uncovered during the course of learning. 

Our computational modelling allowed us to delve deeper into what might be driving the observed learning transfer. Initial modelling of observations using all available data seems to indicate that the most likely model was a Q-learning type model. However, as we argued above, that would be inconsistent with the evidence for learning transfer. Breaking down likelihoods by trial and fitting a hidden Markov model to the data with states being the various strategies that participants are assumed to be using, we showed evidence for within game switching of strategies. Participants start the early rounds of a new game acting in a way consistent with a Bayesian Bayesian Cognitive Hierarchy level, which would be accurate and generalisable but computationally expensive. However, as trials continue, participants seem to switch to a habitual type of learning (QL-models). 

Why is this switching happening? We believe that participants show flexibility in their use of learning strategies. When a new game is started that is similar to a previously played game with the same opponent, participants need a way to transfer prior knowledge of the opponent and apply it to the new game in order to best respond. Adopting a Bayesian model based on BCH achieves the goal of transferring the opponent model and thus coming up with best responses in the early trials. However, Bayesian BCH models are computationally expensive and require higher order thinking (I think that you think that I think...). As such, as the games progresses, they may become burdensome and the higher amount of historical interaction in the new game allows participants to have enough data to start using the cognitively cheaper model-free learning strategies such as Q-learning. The preference for less computationally demanding strategies is well established [@Kool_2011].  Moreover, the ability to flexibly switch is also consistent with evidence from the literature on learning strategies in humans, showing that they indeed shift between model-based and model-free learning when the environment requires it [@Simon_Daw_11]. 

-->

# Conclusion 

Our results show that people can successfully deviate from Nash equilibrium play to exploit deviations from such play by their opponents. Moreover, people can transfer knowledge about the limitations of their opponents to new situations. This transfer of a model of the opponent depends on the similarity between the prior and new game, as well as the sophistication of the opponent. Transfer is better to similar games, and for less sophisticated agents. Within games, we found evidence for a switch from a more reasoning-based strategy which allows for between-game transfer, to a more habitual strategy which does not. This is consistent with a rational trade-off between the goal of maximising performance and minimizing the cost of computing the best possible actions.

<!--

online experiments results are consistent with behavioural game theory findings, in that human players can deviate from Nash equilibrium play and learn to adapt to the opponent strategy and exploit it when the opponent itself is deviating from Nash equilibrium. Moreover, we showed that participants transfer their learning to new games with varying degrees of similarity. The transfer is also moderated by the level of sophistication of the opponent, with participants showing more success in learning and transferring against opponents adopting a less sophisticated strategy. 

Having said that, there remains a high degree of heterogeneity between players. There is a high positive association between players who learn to beat the sophisticated and less sophisticated opponents, indicating that some players are more able to detect the patterns in opponent play and learn how to exploit them. Moreover, the computational modelling shows that it is likely that players start each game using a model-based learning strategy that facilitates generalisation and opponent model transfer, but then switch to behaviour that is consistent with a  model-free learning strategy as the experiment goes on. This is likely driven by a trade-off between computational complexity and accuracy between model based and model free strategies. 
\newpage
-->

# References


```{r create_r-references}
r_refs(file = "Mendeley2.bib")

```

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id = "refs"></div>
\endgroup
