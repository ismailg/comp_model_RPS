# First we deal with easy case of unique predicted action ( multiple possible preds in shoot are separated by "/" sign)
if (!grepl("/",pred_opp,fixed = TRUE)) {
# Multiply prior by likelihood of observation to get posterior. Done here ot take advantage of For loop.
if(as.character(data[t,"a_action"]) == pred_opp) {
lik_opp[k,t] <- (theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
lik_opp[k,t] <- ((1-theta)/nopts)*opp_prior_vec[k]
}
# Here we are in shootout and opponent predicted action is a vector of two possible moves (e.g "left/center")
} else {
if(grepl(as.character(data[t,"a_action"]), pred_opp,fixed = TRUE)) {
# Assume each action has likelihood of 50%
lik_opp[k,t] <- 0.5*(theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
#the 2 is to standardise the theta dependent probs: P(ai plays pred | opp is level k)
lik_opp[k,t] <- 2*((1-theta)/nopts)*opp_prior_vec[k]
}
}
# Get which opponent strategy human current actn maps to (i.e, if index = 0 then human current action is best response to level-0 comp_strat...)
indices <- which(grepl(as.character(data[t,"h_action"]),br_hum,fixed = TRUE))
# THEN likelihood of current human action is just prior on opponent vec (haven't updated priors with curr opp act yet)
# if action is not predicted by any level-k OM, assume human chooses randomly with prob eps
if(length(indices) == 0){
#lik_hum[t] <- eps/nopts
lik_hum[t] <- 1/nopts
# if same br action for multiple possible ai opponents, add probabilities
} else {
#lik_hum[t] <- sum(opp_prior_vec[indices])*(1-eps) + eps/nopts
lik_hum[t] <- 0.5*sum(opp_prior_vec[indices])
}
}
# Standardising the probabilities and updating prior
lik_opp[,t] <- lik_opp[,t]/sum(lik_opp[,t])
opp_prior_vec <- lik_opp[,t]
}
# for debugging uncomment the line below
#cat(as.character(pred_opp),as.character(data[t,"a_action"]),lik_opp[,t],"/n")
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
# Assumes within game transfer of opponent model between stages, but boolena for between game transfer.
TOM_model <- function(par, data, opp_strategy_vec, return_value, within_transfer, btwn_transfer, Naive = FALSE) {
# dat = data subset for one participant
# opp_strategy_vec = vector of possible opponent strtegies, model assumes humans restrict opp strategy space to vector   c("level0","level1","level2")
# return = -2logLik
# or "likelihood_by_trial" vector  (for later plotting, etc)
# btwn_transfer is a boolean. True if distribution of opponent strategies is kept across games. False otherwise.
# theta = parameter, probability computer will play its best response to what it thinks human is. Truth = 90%. in [0,1].
# eps: parameter controlling noise in human choice. Probability human will DEVIATE from its best response. In [0,1].
theta <- par[1]
# eps <- par[2]
num_strat = length(opp_strategy_vec)
# Load tables predicting oppponent movel
rps_predict_opp <- read.csv("rps_predict_opp.csv")
fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
shootout_predict_opp <- read.csv("shootout_predict_opp.csv")
#lik_opp is a vector that holds and updates probability distribution of opp strategy given actions.
lik_opp <- matrix(0.0,num_strat, nrow(data))
# Initiate prior vectors for each opponent as uniform
opp_prior_vec <- rep(1/num_strat, num_strat)
opp1_prior_vec <-rep(1/num_strat, num_strat)
opp2_prior_vec <-rep(1/num_strat, num_strat)
lik_hum <- matrix(0.0,nrow(data))
# br_hum is vector that stores best responses of human to actions taken by each level-k ai opponent at the round.
br_hum <- rep(NA,num_strat)
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
pred_file_opp <- switch(as.character(t_game),rps = rps_predict_opp,fwg = fwg_predict_opp, shootout = shootout_predict_opp)
nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
pred_opp <- NA
lik_hum[t] <- 1/nopts
## KEY CODE FOR LEARNING TRANSFER !! #####
#Naive
if (Naive) {
# pass. no update to prior vector throughout all stages and games
} else {
#abc
#def
# Round 1 of stage 1
if (data[t,"stage"] == 1) {
if (btwn_transfer){
# if there is between game transfer, then the opp_prior_vec should have stored what was learned about the SECOND opponent
# ...from stage 4 of previous game (or uniform if it is first game of RPS). store that before proceeding
opp2_prior_vec <- opp_prior_vec
# if between game transfer use stored opp1 prior vector from previous game (or uniform if first game  RPS)
opp_prior_vec <- opp1_prior_vec
} else { # <- No between game TRANSFER
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# we are round 1 of stage 2, keep opp vector from round 20 of stage 1  for stage 3
} else if (data[t,"stage"] == 2) {
if (within_transfer){
# hold the probs vector of Opp type learned in stage 1 in memory and save to temp.
opp1_prior_temp <- opp_prior_vec
if (btwn_transfer){
# If  between games transfer get prior from opp2 prior saved from last stage of last game
opp_prior_vec <- opp2_prior_vec
} else { # if no between transfer start of first interaction with secnd opp should be reset
opp_prior_vec <- rep(1/num_strat, num_strat)
}
} else { # Assume no between game transfer if no within. reasonable.
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# we are round 1 of stage 3
} else if (data[t,"stage"] == 3) {
if (within_transfer) {
# if within_transfer keep opp vector from round 20 of stage 2 for stage 4
opp2_prior_temp <- opp_prior_vec
# if within_transfer, use what we learned about opp1 in stage 1 as new prior.
opp_prior_vec <- opp1_prior_temp
} else { # No within transfer, dont store anything and reset priors
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# We are in round 1 of stage 4,
}  else if (data[t,"stage"] == 4) {
if (within_transfer) {
if (btwn_transfer) {
# Store what was learned about opponent 1 for nxt game...
opp1_prior_vec <- opp_prior_vec
}
# Use what was learned about opponent 2 from stage 2.
opp_prior_vec <- opp2_prior_temp
} else {
opp_prior_vec <- rep(1/num_strat, num_strat)
}
}
} #End of no naive condition
# Rounds 2 to End of stage
} else {
#cat("stage", data[t,"stage"], " opps ",  opp_prior_vec, "\n")
# get prediction of opponent action from CSV files
for (strategy in opp_strategy_vec) {
k = match(strategy,opp_strategy_vec)
pred_opp <- as.character(filter(pred_file_opp, pred_file_opp$human_previous == as.character(data[t-1,"h_action"]) & pred_file_opp$computer_previous == as.character(data[t-1,"a_action"]))[[strategy]])
# Given opponent action predicted, what would be human action best response for each opp strat
br_hum[[k]] <- exp2_best_response(t_game,as.character(pred_opp))
# First we deal with easy case of unique predicted action ( multiple possible preds in shoot are separated by "/" sign)
if (!grepl("/",pred_opp,fixed = TRUE)) {
# Multiply prior by likelihood of observation to get posterior. Done here ot take advantage of For loop.
if(as.character(data[t,"a_action"]) == pred_opp) {
lik_opp[k,t] <- (theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
lik_opp[k,t] <- ((1-theta)/nopts)*opp_prior_vec[k]
}
# Here we are in shootout and opponent predicted action is a vector of two possible moves (e.g "left/center")
} else {
if(grepl(as.character(data[t,"a_action"]), pred_opp,fixed = TRUE)) {
# Assume each action has likelihood of 50%
lik_opp[k,t] <- 0.5*(theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
#the 2 is to standardise the theta dependent probs: P(ai plays pred | opp is level k)
lik_opp[k,t] <- 2*((1-theta)/nopts)*opp_prior_vec[k]
}
}
# Get which opponent strategy human current actn maps to (i.e, if index = 0 then human current action is best response to level-0 comp_strat...)
indices <- which(grepl(as.character(data[t,"h_action"]),br_hum,fixed = TRUE))
# THEN likelihood of current human action is just prior on opponent vec (haven't updated priors with curr opp act yet)
# if action is not predicted by any level-k OM, assume human chooses randomly with prob eps
if(length(indices) == 0){
#lik_hum[t] <- eps/nopts
lik_hum[t] <- 1/nopts
# if same br action for multiple possible ai opponents, add probabilities
} else {
#lik_hum[t] <- sum(opp_prior_vec[indices])*(1-eps) + eps/nopts
lik_hum[t] <- sum(opp_prior_vec[indices])/length(indices)
}
}
# Standardising the probabilities and updating prior
lik_opp[,t] <- lik_opp[,t]/sum(lik_opp[,t])
opp_prior_vec <- lik_opp[,t]
}
# for debugging uncomment the line below
#cat(as.character(pred_opp),as.character(data[t,"a_action"]),lik_opp[,t],"/n")
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
TOM_model(c(0.1),test_data,c("level0","level1","level2"),"-2loglik", within_transfer = TRUE, btwn_transfer = TRUE, Naive = FALSE)
# Assumes within game transfer of opponent model between stages, but boolena for between game transfer.
TOM_model <- function(par, data, opp_strategy_vec, return_value, within_transfer, btwn_transfer, Naive = FALSE) {
# dat = data subset for one participant
# opp_strategy_vec = vector of possible opponent strtegies, model assumes humans restrict opp strategy space to vector   c("level0","level1","level2")
# return = -2logLik
# or "likelihood_by_trial" vector  (for later plotting, etc)
# btwn_transfer is a boolean. True if distribution of opponent strategies is kept across games. False otherwise.
# theta = parameter, probability computer will play its best response to what it thinks human is. Truth = 90%. in [0,1].
# eps: parameter controlling noise in human choice. Probability human will DEVIATE from its best response. In [0,1].
theta <- par[1]
# eps <- par[2]
num_strat = length(opp_strategy_vec)
# Load tables predicting oppponent movel
rps_predict_opp <- read.csv("rps_predict_opp.csv")
fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
shootout_predict_opp <- read.csv("shootout_predict_opp.csv")
#lik_opp is a vector that holds and updates probability distribution of opp strategy given actions.
lik_opp <- matrix(0.0,num_strat, nrow(data))
# Initiate prior vectors for each opponent as uniform
opp_prior_vec <- rep(1/num_strat, num_strat)
opp1_prior_vec <-rep(1/num_strat, num_strat)
opp2_prior_vec <-rep(1/num_strat, num_strat)
lik_hum <- matrix(0.0,nrow(data))
# br_hum is vector that stores best responses of human to actions taken by each level-k ai opponent at the round.
br_hum <- rep(NA,num_strat)
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
pred_file_opp <- switch(as.character(t_game),rps = rps_predict_opp,fwg = fwg_predict_opp, shootout = shootout_predict_opp)
nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
pred_opp <- NA
lik_hum[t] <- 1/nopts
## KEY CODE FOR LEARNING TRANSFER !! #####
#Naive
if (Naive) {
# pass. no update to prior vector throughout all stages and games
} else {
#abc
#def
# Round 1 of stage 1
if (data[t,"stage"] == 1) {
if (btwn_transfer){
# if there is between game transfer, then the opp_prior_vec should have stored what was learned about the SECOND opponent
# ...from stage 4 of previous game (or uniform if it is first game of RPS). store that before proceeding
opp2_prior_vec <- opp_prior_vec
# if between game transfer use stored opp1 prior vector from previous game (or uniform if first game  RPS)
opp_prior_vec <- opp1_prior_vec
} else { # <- No between game TRANSFER
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# we are round 1 of stage 2, keep opp vector from round 20 of stage 1  for stage 3
} else if (data[t,"stage"] == 2) {
if (within_transfer){
# hold the probs vector of Opp type learned in stage 1 in memory and save to temp.
opp1_prior_temp <- opp_prior_vec
if (btwn_transfer){
# If  between games transfer get prior from opp2 prior saved from last stage of last game
opp_prior_vec <- opp2_prior_vec
} else { # if no between transfer start of first interaction with secnd opp should be reset
opp_prior_vec <- rep(1/num_strat, num_strat)
}
} else { # Assume no between game transfer if no within. reasonable.
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# we are round 1 of stage 3
} else if (data[t,"stage"] == 3) {
if (within_transfer) {
# if within_transfer keep opp vector from round 20 of stage 2 for stage 4
opp2_prior_temp <- opp_prior_vec
# if within_transfer, use what we learned about opp1 in stage 1 as new prior.
opp_prior_vec <- opp1_prior_temp
} else { # No within transfer, dont store anything and reset priors
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# We are in round 1 of stage 4,
}  else if (data[t,"stage"] == 4) {
if (within_transfer) {
if (btwn_transfer) {
# Store what was learned about opponent 1 for nxt game...
opp1_prior_vec <- opp_prior_vec
}
# Use what was learned about opponent 2 from stage 2.
opp_prior_vec <- opp2_prior_temp
} else {
opp_prior_vec <- rep(1/num_strat, num_strat)
}
}
} #End of no naive condition
# Rounds 2 to End of stage
} else {
#cat("stage", data[t,"stage"], " opps ",  opp_prior_vec, "\n")
# get prediction of opponent action from CSV files
for (strategy in opp_strategy_vec) {
k = match(strategy,opp_strategy_vec)
pred_opp <- as.character(filter(pred_file_opp, pred_file_opp$human_previous == as.character(data[t-1,"h_action"]) & pred_file_opp$computer_previous == as.character(data[t-1,"a_action"]))[[strategy]])
# Given opponent action predicted, what would be human action best response for each opp strat
br_hum[[k]] <- exp2_best_response(t_game,as.character(pred_opp))
# First we deal with easy case of unique predicted action ( multiple possible preds in shoot are separated by "/" sign)
if (!grepl("/",pred_opp,fixed = TRUE)) {
# Multiply prior by likelihood of observation to get posterior. Done here ot take advantage of For loop.
if(as.character(data[t,"a_action"]) == pred_opp) {
lik_opp[k,t] <- (theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
lik_opp[k,t] <- ((1-theta)/nopts)*opp_prior_vec[k]
}
# Here we are in shootout and opponent predicted action is a vector of two possible moves (e.g "left/center")
} else {
if(grepl(as.character(data[t,"a_action"]), pred_opp,fixed = TRUE)) {
# Assume each action has likelihood of 50%
lik_opp[k,t] <- 0.5*(theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
#the 2 is to standardise the theta dependent probs: P(ai plays pred | opp is level k)
lik_opp[k,t] <- 2*((1-theta)/nopts)*opp_prior_vec[k]
}
}
# Get which opponent strategy human current actn maps to (i.e, if index = 0 then human current action is best response to level-0 comp_strat...)
indices <- which(grepl(as.character(data[t,"h_action"]),br_hum,fixed = TRUE))
# THEN likelihood of current human action is just prior on opponent vec (haven't updated priors with curr opp act yet)
# if action is not predicted by any level-k OM, assume human chooses randomly with prob eps
if(length(indices) == 0){
#lik_hum[t] <- eps/nopts
lik_hum[t] <- 1/nopts
# if same br action for multiple possible ai opponents, add probabilities
} else {
#lik_hum[t] <- sum(opp_prior_vec[indices])*(1-eps) + eps/nopts
lik_hum[t] <- sum(opp_prior_vec[indices])
}
}
# Standardising the probabilities and updating prior
lik_opp[,t] <- lik_opp[,t]/sum(lik_opp[,t])
opp_prior_vec <- lik_opp[,t]
}
# for debugging uncomment the line below
#cat(as.character(pred_opp),as.character(data[t,"a_action"]),lik_opp[,t],"/n")
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
TOM_model(c(0.1),test_data,c("level0","level1","level2"),"-2loglik", within_transfer = TRUE, btwn_transfer = TRUE, Naive = FALSE)
# Function to produce best reponse to ai_action in exp2
exp2_best_response <- function(game,ai_action){
if (game == "rps"){
if (ai_action == "R") {return("P")}
else if (ai_action == "P") {return("S")}
else if (ai_action == "S") {return("R")}
} else if (game == "fwg"){
if (ai_action == "F") {return("W")}
else if (ai_action == "W") {return("G")}
else if (ai_action == "G") {return("F")}
} else if (game == "shootout") {
if (ai_action == "left") {return("center/right")}
else if (ai_action == "center") {return("left/right")}
else if (ai_action == "right") {return("left/center")}
else if (ai_action == "left/right") {return("center")}
else if (ai_action == "left/center") {return("right")}
else if (ai_action == "center/right") {return("left")}
}
}
# Assumes within game transfer of opponent model between stages, but boolena for between game transfer.
TOM_model <- function(par, data, opp_strategy_vec, return_value, within_transfer, btwn_transfer, Naive = FALSE) {
# dat = data subset for one participant
# opp_strategy_vec = vector of possible opponent strtegies, model assumes humans restrict opp strategy space to vector   c("level0","level1","level2")
# return = -2logLik
# or "likelihood_by_trial" vector  (for later plotting, etc)
# btwn_transfer is a boolean. True if distribution of opponent strategies is kept across games. False otherwise.
# theta = parameter, probability computer will play its best response to what it thinks human is. Truth = 90%. in [0,1].
# eps: parameter controlling noise in human choice. Probability human will DEVIATE from its best response. In [0,1].
theta <- par[1]
# eps <- par[2]
num_strat = length(opp_strategy_vec)
# Load tables predicting oppponent movel
rps_predict_opp <- read.csv("rps_predict_opp.csv")
fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
shootout_predict_opp <- read.csv("shootout_predict_opp.csv")
#lik_opp is a vector that holds and updates probability distribution of opp strategy given actions.
lik_opp <- matrix(0.0,num_strat, nrow(data))
# Initiate prior vectors for each opponent as uniform
opp_prior_vec <- rep(1/num_strat, num_strat)
opp1_prior_vec <-rep(1/num_strat, num_strat)
opp2_prior_vec <-rep(1/num_strat, num_strat)
lik_hum <- matrix(0.0,nrow(data))
# br_hum is vector that stores best responses of human to actions taken by each level-k ai opponent at the round.
br_hum <- rep(NA,num_strat)
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
pred_file_opp <- switch(as.character(t_game),rps = rps_predict_opp,fwg = fwg_predict_opp, shootout = shootout_predict_opp)
nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
pred_opp <- NA
lik_hum[t] <- 1/nopts
## KEY CODE FOR LEARNING TRANSFER !! #####
#Naive
if (Naive) {
# pass. no update to prior vector throughout all stages and games
} else {
#abc
#def
# Round 1 of stage 1
if (data[t,"stage"] == 1) {
if (btwn_transfer){
# if there is between game transfer, then the opp_prior_vec should have stored what was learned about the SECOND opponent
# ...from stage 4 of previous game (or uniform if it is first game of RPS). store that before proceeding
opp2_prior_vec <- opp_prior_vec
# if between game transfer use stored opp1 prior vector from previous game (or uniform if first game  RPS)
opp_prior_vec <- opp1_prior_vec
} else { # <- No between game TRANSFER
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# we are round 1 of stage 2, keep opp vector from round 20 of stage 1  for stage 3
} else if (data[t,"stage"] == 2) {
if (within_transfer){
# hold the probs vector of Opp type learned in stage 1 in memory and save to temp.
opp1_prior_temp <- opp_prior_vec
if (btwn_transfer){
# If  between games transfer get prior from opp2 prior saved from last stage of last game
opp_prior_vec <- opp2_prior_vec
} else { # if no between transfer start of first interaction with secnd opp should be reset
opp_prior_vec <- rep(1/num_strat, num_strat)
}
} else { # Assume no between game transfer if no within. reasonable.
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# we are round 1 of stage 3
} else if (data[t,"stage"] == 3) {
if (within_transfer) {
# if within_transfer keep opp vector from round 20 of stage 2 for stage 4
opp2_prior_temp <- opp_prior_vec
# if within_transfer, use what we learned about opp1 in stage 1 as new prior.
opp_prior_vec <- opp1_prior_temp
} else { # No within transfer, dont store anything and reset priors
opp_prior_vec <- rep(1/num_strat, num_strat)
}
# We are in round 1 of stage 4,
}  else if (data[t,"stage"] == 4) {
if (within_transfer) {
if (btwn_transfer) {
# Store what was learned about opponent 1 for nxt game...
opp1_prior_vec <- opp_prior_vec
}
# Use what was learned about opponent 2 from stage 2.
opp_prior_vec <- opp2_prior_temp
} else {
opp_prior_vec <- rep(1/num_strat, num_strat)
}
}
} #End of no naive condition
# Rounds 2 to End of stage
} else {
#cat("stage", data[t,"stage"], " opps ",  opp_prior_vec, "\n")
# get prediction of opponent action from CSV files
for (strategy in opp_strategy_vec) {
k = match(strategy,opp_strategy_vec)
pred_opp <- as.character(filter(pred_file_opp, pred_file_opp$human_previous == as.character(data[t-1,"h_action"]) & pred_file_opp$computer_previous == as.character(data[t-1,"a_action"]))[[strategy]])
# Given opponent action predicted, what would be human action best response for each opp strat
br_hum[[k]] <- exp2_best_response(t_game,as.character(pred_opp))
# First we deal with easy case of unique predicted action ( multiple possible preds in shoot are separated by "/" sign)
if (!grepl("/",pred_opp,fixed = TRUE)) {
# Multiply prior by likelihood of observation to get posterior. Done here ot take advantage of For loop.
if(as.character(data[t,"a_action"]) == pred_opp) {
lik_opp[k,t] <- (theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
lik_opp[k,t] <- ((1-theta)/nopts)*opp_prior_vec[k]
}
# Here we are in shootout and opponent predicted action is a vector of two possible moves (e.g "left/center")
} else {
if(grepl(as.character(data[t,"a_action"]), pred_opp,fixed = TRUE)) {
# Assume each action has likelihood of 50%
lik_opp[k,t] <- 0.5*(theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
#the 2 is to standardise the theta dependent probs: P(ai plays pred | opp is level k)
lik_opp[k,t] <- 2*((1-theta)/nopts)*opp_prior_vec[k]
}
}
# Get which opponent strategy human current actn maps to (i.e, if index = 0 then human current action is best response to level-0 comp_strat...)
indices <- which(grepl(as.character(data[t,"h_action"]),br_hum,fixed = TRUE))
# THEN likelihood of current human action is just prior on opponent vec (haven't updated priors with curr opp act yet)
# if action is not predicted by any level-k OM, assume human chooses randomly with prob eps
if(length(indices) == 0){
#lik_hum[t] <- eps/nopts
lik_hum[t] <- 1/nopts
# if same br action for multiple possible ai opponents, add probabilities
} else {
#lik_hum[t] <- sum(opp_prior_vec[indices])*(1-eps) + eps/nopts
lik_hum[t] <- sum(opp_prior_vec[indices])/length(indices)
}
}
# Standardising the probabilities and updating prior
lik_opp[,t] <- lik_opp[,t]/sum(lik_opp[,t])
opp_prior_vec <- lik_opp[,t]
}
# for debugging uncomment the line below
#cat(as.character(pred_opp),as.character(data[t,"a_action"]),lik_opp[,t],"/n")
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
TOM_model(c(0.1),test_data,c("level0","level1","level2"),"-2loglik", within_transfer = TRUE, btwn_transfer = TRUE, Naive = FALSE)
TOM_model(c(0.5),test_data,c("level0","level1","level2"),"-2loglik", within_transfer = TRUE, btwn_transfer = TRUE, Naive = FALSE)
TOM_model(c(0.9),test_data,c("level0","level1","level2"),"-2loglik", within_transfer = TRUE, btwn_transfer = TRUE, Naive = FALSE)
TOM_model(c(0.001),test_data,c("level0","level1","level2"),"-2loglik", within_transfer = TRUE, btwn_transfer = TRUE, Naive = FALSE)
TOM_model(c(0.00001),test_data,c("level0","level1","level2"),"-2loglik", within_transfer = TRUE, btwn_transfer = TRUE, Naive = FALSE)
