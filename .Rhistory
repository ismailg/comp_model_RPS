# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
# Estimate phi(t)
R_t <- as.numeric(names(Att) == a_act_prev)
# cat("This is R(t)",R_t,"\n")
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
# cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
# cat("this is delta(t)",delta_t,"\n")
for (i in length(Att)) {
action <- as.character(names(Att)[i])
# cat("this is current strat",action,"\n")
# cat("this is previous humna action",h_act_prev,"\n")
# Attraction vector update rule
Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att)/sum(exp(lambda*Att))
cat(probs,"\n")
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, names(Att))
lik_hum[t] <- probs[[act_index]]
cat(lik_hum[t],"\n")
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
#
# EWA_modelling <- list()
# for(id in unique(dat$human_id)) {
#
#   EWA_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
# }
#
# save(EWA_modelling,file="EWA_modelling.RData")
EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_self_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
EWA_self_modelling[[id]] <- optim(10.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 20, method="L-BFGS-B")
}
#
# EWA_modelling <- list()
# for(id in unique(dat$human_id)) {
#
#   EWA_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
# }
#
# save(EWA_modelling,file="EWA_modelling.RData")
EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_self_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
EWA_self_modelling[[id]] <- optim(1.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 20, method="L-BFGS-B")
}
#######################################################################################
##  Self-Tuning EWA:
EWA_self <- function(par,data,return_value=c("-2loglik","likelihood_by_trial")){
lambda <- par[[1]]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
N <- 1.0
# Define attraction vectors for each game
A_RPS = matrix(0.0,3)
names(A_RPS) <- c("R","P","S")
A_FWG = matrix(0.0,3)
names(A_FWG) <- c("F","W","G")
A_NUM = matrix(0.0,5)
names(A_NUM) <- c("1","2","3","4","5")
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
# Estimate phi(t)
R_t <- as.numeric(names(Att) == a_act_prev)
# cat("This is R(t)",R_t,"\n")
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
# cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
# cat("this is delta(t)",delta_t,"\n")
for (i in length(Att)) {
action <- as.character(names(Att)[i])
# cat("this is current strat",action,"\n")
# cat("this is previous humna action",h_act_prev,"\n")
# Attraction vector update rule
Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att)/sum(exp(lambda*Att))
#cat(probs,"\n")
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, names(Att))
lik_hum[t] <- probs[[act_index]]
#cat(lik_hum[t],"\n")
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
#
# EWA_modelling <- list()
# for(id in unique(dat$human_id)) {
#
#   EWA_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
# }
#
# save(EWA_modelling,file="EWA_modelling.RData")
EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_self_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
EWA_self_modelling[[id]] <- optim(1.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 20, method="L-BFGS-B")
}
save(EWA_self_modelling,file="EWA_self_modelling.RData")
View(EWA_self_modelling)
#######################################################################################
##  Self-Tuning EWA:
EWA_self <- function(par,data,return_value=c("-2loglik","likelihood_by_trial")){
lambda <- par[[1]]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
N <- 1.0
# Define attraction vectors for each game
A_RPS = matrix(0.0,3)
names(A_RPS) <- c("R","P","S")
A_FWG = matrix(0.0,3)
names(A_FWG) <- c("F","W","G")
A_NUM = matrix(0.0,5)
names(A_NUM) <- c("1","2","3","4","5")
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
# Estimate R(t): 1 if last ai action, 0 otherwise
R_t <- as.numeric(names(Att) == a_act_prev)
cat("This is R(t)",R_t,"\n")
# Calc H(t) as (t-1)
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
# cat("this is delta(t)",delta_t,"\n")
for (i in length(Att)) {
action <- as.character(names(Att)[i])
# cat("this is current strat",action,"\n")
# cat("this is previous humna action",h_act_prev,"\n")
# Attraction vector update rule
Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att)/sum(exp(lambda*Att))
#cat(probs,"\n")
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, names(Att))
lik_hum[t] <- probs[[act_index]]
#cat(lik_hum[t],"\n")
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
#
# EWA_modelling <- list()
# for(id in unique(dat$human_id)) {
#
#   EWA_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
# }
#
# save(EWA_modelling,file="EWA_modelling.RData")
EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_self_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
EWA_self_modelling[[id]] <- optim(1.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 20, method="L-BFGS-B")
}
#######################################################################################
##  Self-Tuning EWA:
EWA_self <- function(par,data,return_value=c("-2loglik","likelihood_by_trial")){
lambda <- par[[1]]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
N <- 1.0
# Define attraction vectors for each game
A_RPS = matrix(0.0,3)
names(A_RPS) <- c("R","P","S")
A_FWG = matrix(0.0,3)
names(A_FWG) <- c("F","W","G")
A_NUM = matrix(0.0,5)
names(A_NUM) <- c("1","2","3","4","5")
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
# Estimate R(t): 1 if last ai action, 0 otherwise
R_t <- as.numeric(names(Att) == a_act_prev)
#cat("This is R(t)",R_t,"\n")
# Calc H(t) as (t-1)
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
#cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
# cat("this is delta(t)",delta_t,"\n")
for (i in length(Att)) {
action <- as.character(names(Att)[i])
# cat("this is current strat",action,"\n")
# cat("this is previous humna action",h_act_prev,"\n")
# Attraction vector update rule
Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att)/sum(exp(lambda*Att))
cat(probs,"\n")
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, names(Att))
lik_hum[t] <- probs[[act_index]]
#cat(lik_hum[t],"\n")
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
#######################################################################################
##  Self-Tuning EWA:
EWA_self <- function(par,data,return_value=c("-2loglik","likelihood_by_trial")){
lambda <- par[[1]]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
N <- 1.0
# Define attraction vectors for each game
A_RPS = matrix(0.0,3)
names(A_RPS) <- c("R","P","S")
A_FWG = matrix(0.0,3)
names(A_FWG) <- c("F","W","G")
A_NUM = matrix(0.0,5)
names(A_NUM) <- c("1","2","3","4","5")
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
# Estimate R(t): 1 if last ai action, 0 otherwise
R_t <- as.numeric(names(Att) == a_act_prev)
#cat("This is R(t)",R_t,"\n")
# Calc H(t) as (t-1)
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
#cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
# cat("this is delta(t)",delta_t,"\n")
for (i in length(Att)) {
action <- as.character(names(Att)[i])
# cat("this is current strat",action,"\n")
# cat("this is previous humna action",h_act_prev,"\n")
# Attraction vector update rule
Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att)/sum(exp(lambda*Att))
cat(probs,"\n")
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, names(Att))
lik_hum[t] <- probs[[act_index]]
cat(lik_hum[t],"\n")
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
#
# EWA_modelling <- list()
# for(id in unique(dat$human_id)) {
#
#   EWA_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
# }
#
# save(EWA_modelling,file="EWA_modelling.RData")
EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_self_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
EWA_self_modelling[[id]] <- optim(1.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 20, method="L-BFGS-B")
}
#######################################################################################
##  Self-Tuning EWA:
EWA_self <- function(par,data,return_value=c("-2loglik","likelihood_by_trial")){
lambda <- par[[1]]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
N <- 1.0
# Define attraction vectors for each game
A_RPS = matrix(0.0,3)
names(A_RPS) <- c("R","P","S")
A_FWG = matrix(0.0,3)
names(A_FWG) <- c("F","W","G")
A_NUM = matrix(0.0,5)
names(A_NUM) <- c("1","2","3","4","5")
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
# Estimate R(t): 1 if last ai action, 0 otherwise
R_t <- as.numeric(names(Att) == a_act_prev)
#cat("This is R(t)",R_t,"\n")
# Calc H(t) as (t-1)
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
#cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
# cat("this is delta(t)",delta_t,"\n")
for (i in length(Att)) {
action <- as.character(names(Att)[i])
# cat("this is current strat",action,"\n")
# cat("this is previous humna action",h_act_prev,"\n")
# Attraction vector update rule
Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att)/sum(exp(lambda*Att))
#cat(probs,"\n")
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, names(Att))
lik_hum[t] <- probs[[act_index]]
#cat(lik_hum[t],"\n")
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
#
# EWA_modelling <- list()
# for(id in unique(dat$human_id)) {
#
#   EWA_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
# }
#
# save(EWA_modelling,file="EWA_modelling.RData")
EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_self_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
EWA_self_modelling[[id]] <- optim(10.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 20, method="L-BFGS-B")
}
save(EWA_self_modelling,file="EWA_self_modelling.RData")
View(EWA_self_modelling)
View(EWA_modelling)
