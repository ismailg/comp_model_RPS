dat_exp2$adj_score <- recode(dat_exp2$score, ifelse(dat_exp2$game.f =="SHOOT", dat_exp2$score - 0.33, dat_exp2$score))
# Load data comparing models BIC for each participant
model_comp = read.csv("exp2_model_comp.csv")
datalist2 = list()
i = 0
#Build empty datafrsme with same names as original data
exp2_cum_score <- setNames(data.frame(matrix(ncol = ncol(dat_exp2), nrow = 0)), colnames(dat_exp2))
for(id in unique(dat_exp2$human_id)) {
i <- i+1
tdat <- subset(dat_exp2,human_id == id)
tdat$part_num <- i
# NB : USE ADJUSTED SCORE IN SHOOTOUT
tdat <- within(tdat, acc_sum <- cumsum(tdat$adj_score))
datalist2[[i]] <- tdat
}
# Merge all datasets into one
exp2_cum_score <- dplyr::bind_rows(datalist2)
# or exp2_cum_score <- data.table::rbindlist(datalist)
# Add column for time t
exp2_cum_score <- exp2_cum_score %>% group_by(human_id) %>% mutate(t = row_number())
# Participant number as a factor
# tdat$part_num <- as.factor(tdat$part_num)
# Add best fitting model per participant
exp2_cum_score <- merge(exp2_cum_score, model_comp[, c("human_id", "Best_model")], by="human_id")
ggplot(data = exp2_cum_score, aes(x = t, y=acc_sum, group = part_num)) +
geom_line(aes(color= Best_model))
View(dat_exp2)
dat_exp2 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- recode(dat_exp2$score, ifelse(dat_exp2$game == "shootout", dat_exp2$score - 0.33, dat_exp2$score))
# Load data comparing models BIC for each participant
model_comp = read.csv("exp2_model_comp.csv")
datalist2 = list()
i = 0
#Build empty datafrsme with same names as original data
exp2_cum_score <- setNames(data.frame(matrix(ncol = ncol(dat_exp2), nrow = 0)), colnames(dat_exp2))
for(id in unique(dat_exp2$human_id)) {
i <- i+1
tdat <- subset(dat_exp2,human_id == id)
tdat$part_num <- i
# NB : USE ADJUSTED SCORE IN SHOOTOUT
tdat <- within(tdat, acc_sum <- cumsum(tdat$adj_score))
datalist2[[i]] <- tdat
}
# Merge all datasets into one
exp2_cum_score <- dplyr::bind_rows(datalist2)
# or exp2_cum_score <- data.table::rbindlist(datalist)
# Add column for time t
exp2_cum_score <- exp2_cum_score %>% group_by(human_id) %>% mutate(t = row_number())
# Participant number as a factor
# tdat$part_num <- as.factor(tdat$part_num)
# Add best fitting model per participant
exp2_cum_score <- merge(exp2_cum_score, model_comp[, c("human_id", "Best_model")], by="human_id")
dat_exp2 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- recode(dat_exp2$score, ifelse(dat_exp2$game == "shootout", as.numeric(dat_exp2$score - 0.333), as.numeric(dat_exp2$score)))
# Load data comparing models BIC for each participant
model_comp = read.csv("exp2_model_comp.csv")
datalist2 = list()
i = 0
#Build empty datafrsme with same names as original data
exp2_cum_score <- setNames(data.frame(matrix(ncol = ncol(dat_exp2), nrow = 0)), colnames(dat_exp2))
for(id in unique(dat_exp2$human_id)) {
i <- i+1
tdat <- subset(dat_exp2,human_id == id)
tdat$part_num <- i
# NB : USE ADJUSTED SCORE IN SHOOTOUT
tdat <- within(tdat, acc_sum <- cumsum(tdat$adj_score))
datalist2[[i]] <- tdat
}
# Merge all datasets into one
exp2_cum_score <- dplyr::bind_rows(datalist2)
# or exp2_cum_score <- data.table::rbindlist(datalist)
# Add column for time t
exp2_cum_score <- exp2_cum_score %>% group_by(human_id) %>% mutate(t = row_number())
# Participant number as a factor
# tdat$part_num <- as.factor(tdat$part_num)
# Add best fitting model per participant
exp2_cum_score <- merge(exp2_cum_score, model_comp[, c("human_id", "Best_model")], by="human_id")
dat_exp3 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp3$adj_score <- recode(dat_exp3$score, ifelse(dat_exp2$game == "shootout", as.numeric(dat_exp3$score - 0.333), as.numeric(dat_exp3$score)))
# Load data comparing models BIC for each participant
model_comp = read.csv("exp2_model_comp.csv")
dat_exp3 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp3$adj_score <- recode(dat_exp3$score, ifelse(dat_exp3$game == "shootout", as.numeric(dat_exp3$score - 0.333), as.numeric(dat_exp3$score)))
# Load data comparing models BIC for each participant
model_comp = read.csv("exp2_model_comp.csv")
View(dat_exp3)
dat_exp2 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- recode(dat_exp2$score, ifelse(dat_exp2$game == "shootout", dat_exp2$score - 0.333))
dat_exp2 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- recode(dat_exp2$score, ifelse(dat_exp2$game == "shootout", dat_exp2$score - 0.333, NULL))
dat_exp2 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- recode(dat_exp2$score, ifelse(as.character(dat_exp2$game) == "shootout", dat_exp2$score - 0.333, NULL))
dat_exp2 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- recode(dat_exp2$score, ifelse(as.character(dat_exp2$game) == "shootout", dat_exp2$score - 0.333, dat_exp2$score))
# Load data comparing models BIC for each participant
model_comp = read.csv("exp2_model_comp.csv")
dat_exp2 = read.csv("data_exp2.csv")
#Adjust shootout score tor eflect easier game
dat_exp2$adj_score <- ifelse(as.character(dat_exp2$game) == "shootout", dat_exp2$score - 0.333, dat_exp2$score)
# Load data comparing models BIC for each participant
model_comp = read.csv("exp2_model_comp.csv")
datalist2 = list()
i = 0
#Build empty datafrsme with same names as original data
exp2_cum_score <- setNames(data.frame(matrix(ncol = ncol(dat_exp2), nrow = 0)), colnames(dat_exp2))
for(id in unique(dat_exp2$human_id)) {
i <- i+1
tdat <- subset(dat_exp2,human_id == id)
tdat$part_num <- i
# NB : USE ADJUSTED SCORE IN SHOOTOUT
tdat <- within(tdat, acc_sum <- cumsum(tdat$adj_score))
datalist2[[i]] <- tdat
}
# Merge all datasets into one
exp2_cum_score <- dplyr::bind_rows(datalist2)
# or exp2_cum_score <- data.table::rbindlist(datalist)
# Add column for time t
exp2_cum_score <- exp2_cum_score %>% group_by(human_id) %>% mutate(t = row_number())
# Participant number as a factor
# tdat$part_num <- as.factor(tdat$part_num)
# Add best fitting model per participant
exp2_cum_score <- merge(exp2_cum_score, model_comp[, c("human_id", "Best_model")], by="human_id")
ggplot(data = exp2_cum_score, aes(x = t, y=acc_sum, group = part_num)) +
geom_line(aes(color= Best_model))
temp <- exp2_cum_score[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>%
summarize(model_acc_sum = mean(acc_sum))
ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) +
geom_line(aes(color= Best_model))
temp <- exp2_cum_score[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>%
summarize(model_acc_sum = mean(acc_sum))
ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) +
geom_line(aes(color= Best_model)) + axis(1, at = 0:180, tck = 20, lty = 2, col = "grey", labels = NA)
temp <- exp2_cum_score[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>%
summarize(model_acc_sum = mean(acc_sum))
ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) +
geom_line(aes(color= Best_model))
temp <- exp2_cum_score[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>%
summarize(model_acc_sum = mean(acc_sum))
ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) +
geom_line(aes(color= Best_model)) +
scale_x_date(breaks = 60)
temp <- exp2_cum_score[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>%
summarize(model_acc_sum = mean(acc_sum))
ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) +
geom_line(aes(color= Best_model)) +
scale_x_continuous(minor_breaks = seq(0 , 180, 10), breaks = seq(0, 180, 60))
# Load various modelling tabeles
exp2_TR_bayes = load("expe_TR_bayes.RData")
# Load various modelling tabeles
exp2_TR_bayes = load("exp2_TR_bayes.RData")
exp2_NT_bayes = load("exp2_NT_bayes.RData")
exp2_QLS_modelling = load("exp2_QLS_modelling.RData")
exp2_SEWA_modelling = load(" exp2_SEWA_modelling.RData")
# Load various modelling tabeles
exp2_TR_bayes = load("exp2_TR_bayes.RData")
exp2_NT_bayes = load("exp2_NT_bayes.RData")
exp2_QLS_modelling = load("exp2_QLS_modelling.RData")
exp2_SEWA_modelling = load("exp2_SEWA_modelling.RData")
exp2_results <- data.frame()
for(id in unique(dat_exp2$human_id)) {
exp2_results <- rbind(exp2_results,
data.frame(
"ID" = id,
"condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
"Random_BIC" = -2*(180*log(1/3)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = exp2_TR_bayes[[id]]$value + 2*log(180),
"Bayes_No_Tr_BIC" = exp2_NT_bayes[[id]]$value+ 2*log(180),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = exp2_TR_bayes[[id]]$par[1],
"theta_no_transfer" = exp2_NT_bayes[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = exp2_TR_bayes[[id]]$par[2],
"eps_no_transfer" = exp2_NT_bayes[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = exp2_QLS_modelling[[id]]$optim$bestval + 2*log(180),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = exp2_QLS_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = exp2_QLS_modelling[[id]]$optim$bestmem[2],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = exp2_SEWA_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = exp2_SEWA_modelling[[id]]$optim$bestval + 1*log(180),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = exp2_SEWA_modelling[[id]]$optim$bestmem[1]
))
}
load("~/Documents/comp_model_RPS/Experiment_2/exp2_TR_bayes.RData")
View(exp2_TR_bayes)
View(dat_exp2)
# Load various modelling tabeles
exp2_TR_bayes = load("exp2_TR_bayes.RData")
head(exp2_TR_bayes)
# exp2_NT_bayes = load("exp2_NT_bayes.RData")
# exp2_QLS_modelling = load("exp2_QLS_modelling.RData")
# exp2_SEWA_modelling = load("exp2_SEWA_modelling.RData")
#
#
#
#
# exp2_results <- data.frame()
# for(id in unique(dat_exp2$human_id)) {
# exp2_results <- rbind(exp2_results,
#                        data.frame(
#                          "ID" = id,
#                          "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
#                          "Random_BIC" = -2*(180*log(1/3)),
#
#                          # Bayesian updating with/without transfer
#                          "Bayes_Tr_BIC" = exp2_TR_bayes[[id]]$value + 2*log(180),
#                          "Bayes_No_Tr_BIC" = exp2_NT_bayes[[id]]$value+ 2*log(180),
#                          # Theta is the parameter governing AI stochasticity. Truth is 0.9
#                          "theta_transfer" = exp2_TR_bayes[[id]]$par[1],
#                          "theta_no_transfer" = exp2_NT_bayes[[id]]$par[1],
#                          # Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
#                          "eps_transfer" = exp2_TR_bayes[[id]]$par[2],
#                          "eps_no_transfer" = exp2_NT_bayes[[id]]$par[2],
#
#
#                          # Q-learning with last round states
#                          "QL_states_BIC" = exp2_QLS_modelling[[id]]$optim$bestval + 2*log(180),
#                          # beta ->  inverse temperature parameter in softmax choice function
#                          "QL_states_Beta" = exp2_QLS_modelling[[id]]$optim$bestmem[1],
#                          # alpha -> learning rate in QL update
#                          "QL_states_alpha" = exp2_QLS_modelling[[id]]$optim$bestmem[2],
#
#
#                          # Self-Tuning EWA (only 1 parameter)
#                           # Parametric EWA BIC
#                          "EWA_self_2LL" = exp2_SEWA_modelling[[id]]$optim$bestval,
#                          "EWA_self_BIC" = exp2_SEWA_modelling[[id]]$optim$bestval + 1*log(180),
#                          #Lambda is a parameter of the softmax choice function (inverse Temperature)
#                          "EWA_self_Lambda" = exp2_SEWA_modelling[[id]]$optim$bestmem[1]
#
#                          ))
# }
#
# write.csv(exp2_results,file="exp2_results.csv",row.names = FALSE)
# Load various modelling tabeles
exp2_TR_bayes = load("exp2_TR_bayes.RData")[1]
head(exp2_TR_bayes[[]])
# Load various modelling tabeles
exp2_TR_bayes = load("exp2_TR_bayes.RData")[1]
head(exp2_TR_bayes)
# exp2_NT_bayes = load("exp2_NT_bayes.RData")
# exp2_QLS_modelling = load("exp2_QLS_modelling.RData")
# exp2_SEWA_modelling = load("exp2_SEWA_modelling.RData")
#
#
#
#
# exp2_results <- data.frame()
# for(id in unique(dat_exp2$human_id)) {
# exp2_results <- rbind(exp2_results,
#                        data.frame(
#                          "ID" = id,
#                          "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
#                          "Random_BIC" = -2*(180*log(1/3)),
#
#                          # Bayesian updating with/without transfer
#                          "Bayes_Tr_BIC" = exp2_TR_bayes[[id]]$value + 2*log(180),
#                          "Bayes_No_Tr_BIC" = exp2_NT_bayes[[id]]$value+ 2*log(180),
#                          # Theta is the parameter governing AI stochasticity. Truth is 0.9
#                          "theta_transfer" = exp2_TR_bayes[[id]]$par[1],
#                          "theta_no_transfer" = exp2_NT_bayes[[id]]$par[1],
#                          # Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
#                          "eps_transfer" = exp2_TR_bayes[[id]]$par[2],
#                          "eps_no_transfer" = exp2_NT_bayes[[id]]$par[2],
#
#
#                          # Q-learning with last round states
#                          "QL_states_BIC" = exp2_QLS_modelling[[id]]$optim$bestval + 2*log(180),
#                          # beta ->  inverse temperature parameter in softmax choice function
#                          "QL_states_Beta" = exp2_QLS_modelling[[id]]$optim$bestmem[1],
#                          # alpha -> learning rate in QL update
#                          "QL_states_alpha" = exp2_QLS_modelling[[id]]$optim$bestmem[2],
#
#
#                          # Self-Tuning EWA (only 1 parameter)
#                           # Parametric EWA BIC
#                          "EWA_self_2LL" = exp2_SEWA_modelling[[id]]$optim$bestval,
#                          "EWA_self_BIC" = exp2_SEWA_modelling[[id]]$optim$bestval + 1*log(180),
#                          #Lambda is a parameter of the softmax choice function (inverse Temperature)
#                          "EWA_self_Lambda" = exp2_SEWA_modelling[[id]]$optim$bestmem[1]
#
#                          ))
# }
#
# write.csv(exp2_results,file="exp2_results.csv",row.names = FALSE)
# Load various modelling tabeles
load("exp2_TR_bayes.RData")
head(exp2_TR_bayes)
# exp2_NT_bayes = load("exp2_NT_bayes.RData")
# exp2_QLS_modelling = load("exp2_QLS_modelling.RData")
# exp2_SEWA_modelling = load("exp2_SEWA_modelling.RData")
#
#
#
#
# exp2_results <- data.frame()
# for(id in unique(dat_exp2$human_id)) {
# exp2_results <- rbind(exp2_results,
#                        data.frame(
#                          "ID" = id,
#                          "condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
#                          "Random_BIC" = -2*(180*log(1/3)),
#
#                          # Bayesian updating with/without transfer
#                          "Bayes_Tr_BIC" = exp2_TR_bayes[[id]]$value + 2*log(180),
#                          "Bayes_No_Tr_BIC" = exp2_NT_bayes[[id]]$value+ 2*log(180),
#                          # Theta is the parameter governing AI stochasticity. Truth is 0.9
#                          "theta_transfer" = exp2_TR_bayes[[id]]$par[1],
#                          "theta_no_transfer" = exp2_NT_bayes[[id]]$par[1],
#                          # Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
#                          "eps_transfer" = exp2_TR_bayes[[id]]$par[2],
#                          "eps_no_transfer" = exp2_NT_bayes[[id]]$par[2],
#
#
#                          # Q-learning with last round states
#                          "QL_states_BIC" = exp2_QLS_modelling[[id]]$optim$bestval + 2*log(180),
#                          # beta ->  inverse temperature parameter in softmax choice function
#                          "QL_states_Beta" = exp2_QLS_modelling[[id]]$optim$bestmem[1],
#                          # alpha -> learning rate in QL update
#                          "QL_states_alpha" = exp2_QLS_modelling[[id]]$optim$bestmem[2],
#
#
#                          # Self-Tuning EWA (only 1 parameter)
#                           # Parametric EWA BIC
#                          "EWA_self_2LL" = exp2_SEWA_modelling[[id]]$optim$bestval,
#                          "EWA_self_BIC" = exp2_SEWA_modelling[[id]]$optim$bestval + 1*log(180),
#                          #Lambda is a parameter of the softmax choice function (inverse Temperature)
#                          "EWA_self_Lambda" = exp2_SEWA_modelling[[id]]$optim$bestmem[1]
#
#                          ))
# }
#
# write.csv(exp2_results,file="exp2_results.csv",row.names = FALSE)
# Load various modelling tabeles
load("exp2_TR_bayes.RData")
load("exp2_NT_bayes.RData")
load("exp2_QLS_modelling.RData")
load("exp2_SEWA_modelling.RData")
exp2_results <- data.frame()
for(id in unique(dat_exp2$human_id)) {
exp2_results <- rbind(exp2_results,
data.frame(
"ID" = id,
"condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
"Random_BIC" = -2*(180*log(1/3)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = exp2_TR_bayes[[id]]$value + 2*log(180),
"Bayes_No_Tr_BIC" = exp2_NT_bayes[[id]]$value+ 2*log(180),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = exp2_TR_bayes[[id]]$par[1],
"theta_no_transfer" = exp2_NT_bayes[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = exp2_TR_bayes[[id]]$par[2],
"eps_no_transfer" = exp2_NT_bayes[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = exp2_QLS_modelling[[id]]$optim$bestval + 2*log(180),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = exp2_QLS_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = exp2_QLS_modelling[[id]]$optim$bestmem[2],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = exp2_SEWA_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = exp2_SEWA_modelling[[id]]$optim$bestval + 1*log(180),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = exp2_SEWA_modelling[[id]]$optim$bestmem[1]
))
}
write.csv(exp2_results,file="exp2_results.csv",row.names = FALSE)
exp2_table_results <- table(exp2_results[, "condition"],c("random","Bayes Tr","Bayes No Tr", "QL_states","S_EWA")[apply(exp2_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","EWA_self_BIC")],1,which.min)])
write.csv(exp2_table_results,file="exp2_table_results.csv",row.names = TRUE)
kable(exp2_table_results)
exp2_model_comp <- data.frame()
for(id in unique(dat_exp2$human_id)) {
tdat <- subset(dat_exp2,human_id == id)
tot_score <- sum(tdat$score)
tot_time <- sum(tdat$human_rt)
early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg"))
#early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg" | game =="numbers") )
tr_score <- sum(early_dat$score)
id_results <- subset(exp2_results, ID == id)
min_BIC <- apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","EWA_self_BIC")],1,min)
best_model <- c("random","Bayes Tr","Bayes NT", "QL_states","S_EWA")[apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","EWA_self_BIC")],1,which.min)]
#
exp2_model_comp <- rbind(exp2_model_comp ,
data.frame(
"human_id" = id,
"condition" = dat_exp2[dat_exp2$human_id==id,"condition"][1],
"Early_game_score" = tr_score,
"Total_score" = tot_score,
"Best_model" = best_model,
"Total_time" = sum(tdat$human_rt),
"TR_minus_NT_BIC" = id_results[,"Bayes_Tr_BIC"] - id_results[,"Bayes_No_Tr_BIC"],
"Rand_minus_best_BIC" =  id_results[,"Random_BIC"] - min_BIC
))
}
write.csv(exp2_model_comp,file="exp2_model_comp.csv",row.names = FALSE)
cor.test(exp2_model_comp$TR_minus_NT_BIC, exp2_model_comp$Early_game_score, method="spearman")
cor.test(exp2_model_comp$Rand_minus_best_BIC, exp2_model_comp$Early_game_score, method="spearman")
barplot(table(exp2_model_comp$Best_model))
# ggboxplot(exp2_model_comp, x = "model", y = "Total_score",
#           color = "model", palette = c("#00AFBB", "#E7B800", "#FC4E07"),
#           order = c("Random", "No Transfer", "Transfer"),
#           ylab = "Total Score", xlab = "Model with best fit")
exp2_model_comp$model <- recode(exp2_model_comp$Best_model,"Bayes Tr" = "Tranfer","Bayes NT" = "No Transfer", "QL_states" = "Q_Learning",  .default = "Random")
model <- factor(exp2_model_comp$model)
condition <- factor(exp2_model_comp$condition)
# Total score by best predictive model
tapply(exp2_model_comp$Total_score, model, mean)
# Compute the analysis of variance
res.aov <- aov(Total_score ~ model + condition, data = exp2_model_comp)
# Summary of the analysis
summary(res.aov)
TukeyHSD(res.aov)
tapply(exp2_model_comp$Total_time, model, mean)
time.aov <- aov(Total_time ~ model, data = exp2_model_comp)
# Summary of the analysis
summary(time.aov)
TukeyHSD(time.aov)
load("TR_bayes_modelling.RData")
load("NT_bayes_modelling.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = TR_bayes_modelling[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = NT_bayes_modelling[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = TR_bayes_modelling[[id]]$par[1],
"theta_no_transfer" = NT_bayes_modelling[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = TR_bayes_modelling[[id]]$par[2],
"eps_no_transfer" = NT_bayes_modelling[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1]
))
}
write.csv(All_results,file="All_results.csv",row.names = FALSE)
Table_results <- table(All_results[, "condition"],c("random","Bayes Tr","Bayes No Tr","QL", "QL_states","EWA","S_EWA")[apply(All_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC")],1,which.min)])
write.csv(Table_results,file="Table_results.csv",row.names = TRUE)
kable(Table_results)
id_results_tst <- subset(All_results, ID == "38VxtUSv_h6RR5-tAAA2")
ggplot(data = exp2_cum_score, aes(x = t, y=acc_sum, group = part_num)) +
geom_line(aes(color= Best_model)) +
scale_x_continuous(minor_breaks = seq(0 , 180, 10), breaks = seq(0, 180, 60))
View(dat_by_model)
temp <- exp2_cum_score[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>%
summarize(model_acc_sum = mean(acc_sum))
ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) +
geom_line(aes(color= Best_model)) +
scale_x_continuous(minor_breaks = seq(0 , 180, 20), breaks = seq(0, 180, 60))
temp <- exp2_cum_score[,c("t","acc_sum","Best_model","condition","part_num")]
dat_by_model <- temp %>% group_by(Best_model,t) %>%
summarize(model_acc_sum = mean(acc_sum))
ggplot(data = dat_by_model, aes(x = t, y=model_acc_sum, group = Best_model)) +
geom_line(aes(color= Best_model)) +
scale_x_continuous(minor_breaks = seq(0 , 180, 10), breaks = seq(0, 180, 60))
