# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
exp1_ToM_TR <- load("exp1_ToM_TR.RData")
exp1_ToM_NT <- load("exp1_ToM_NT.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = exp1_ToM_TR[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = exp1_ToM_NT[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = exp1_ToM_TR[[id]]$par[1],
"theta_no_transfer" = exp1_ToM_NT[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = exp1_ToM_TR[[id]]$par[2],
"eps_no_transfer" = exp1_ToM_NT[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
load("~/Documents/comp_model_RPS/exp1_ToM_NT.RData")
load("~/Documents/comp_model_RPS/exp1_ToM_TR.RData")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
exp1_ToM_TR <- load("exp1_ToM_TR.RData")
exp1_ToM_NT <- load("exp1_ToM_NT.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = exp1_ToM_TR[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = exp1_ToM_NT[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = exp1_ToM_TR[[id]]$par[1],
"theta_no_transfer" = exp1_ToM_NT[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = exp1_ToM_TR[[id]]$par[2],
"eps_no_transfer" = exp1_ToM_NT[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
load("exp1_ToM_TR.RData")
load("exp1_ToM_NT.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = exp1_ToM_TR[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = exp1_ToM_NT[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = exp1_ToM_TR[[id]]$par[1],
"theta_no_transfer" = exp1_ToM_NT[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = exp1_ToM_TR[[id]]$par[2],
"eps_no_transfer" = exp1_ToM_NT[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
load("~/Documents/comp_model_RPS/exp1_ToM_NT.RData")
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
dat <- read.csv("data20180719.csv")
#dat <- read.csv("test_data.csv")
# using some functions from the 'tidyverse' (also for me to get use to them ;-)
library(tidyr)
library(dplyr)
library(DEoptim)
library(optimParallel)
library(ggpubr)
# create a new data.frame in a 'wide format'
# widedata <- dat %>%
#   unite(game_block,game,block) %>% # this creates a new variable which combines game and block
#     group_by(human_id,condition,game_block) %>% # let the functions know you want to separate things by ID, condition, and game_block
#       summarize(mean_score = mean(score)) %>% # compute average score (i.e wins - losses)
#         spread(game_block,mean_score) # reformat in the 'wide' format
# # save the data file as a .csv to use in e.g. SPSS
# write.csv(widedata,row.names=FALSE,file="scores_wide.csv")
# read in the various files which
rps_predict_opp <- read.csv("rps_predict_opp.csv")
fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
numbers_predict_opp <- read.csv("numbers_predict_opp.csv")
# transform 'winner' variable in numeric score
dat$score <- recode(dat$winner, human = 1, tie = 0, ai = -1)
# create a new variable 'block' with round 1...25 = block 1 and round 26...50 as block 2
dat$block <- as.numeric(cut(dat$round,2))
# recode actions to make them equal to the codes in these files
dat$h_action <- recode(dat$human_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G", "one" = "1", "two" = "2", "three" = "3", "four" = "4", "five" = "5")
dat$a_action <- recode(dat$ai_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G", "one" = "1", "two" = "2", "three" = "3", "four" = "4", "five" = "5")
write.csv(dat,file = "exp1_data.csv", row.names = FALSE)
# logit tranformation
my_logit <- function(x) {
ret <- log(x/(1-x))
ret[x==0] <- -Inf
ret[x==1] <- Inf
return(ret)
}
# inverse logit transformation
my_logistic <- function(x) {
ret <- 1/(1+exp(-x))
ret[x == -Inf] <- 0.0
ret[x == Inf] <- 1.0
return(ret)
}
# Function to produce best reponse to ai_action
best_response <- function(game,ai_action){
if (game == "rps"){
if (ai_action == "R") {return("P")}
else if (ai_action == "P") {return("S")}
else if (ai_action == "S") {return("R")}
} else if (game == "fwg"){
if (ai_action == "F") {return("W")}
else if (ai_action == "W") {return("G")}
else if (ai_action == "G") {return("F")}
} else if (game == "numbers") {
if (ai_action == "1") {return("2")}
else if (ai_action == "2") {return("3")}
else if (ai_action == "3") {return("4")}
else if (ai_action == "4") {return("5")}
else if (ai_action == "5") {return("1")}
}
}
exp1_ToM <- function(par,data,opp_strategy_vec,return_value,opp_mod_transfer) {
# dat = data subset for one participant
# opp_strategy_vec = vector of possible opponent strtegies, model assumes humans restrict opp strategy space to vector   c("level0","level1","level2")
# return = -2logLik
# or "likelihood_by_trial" vector  (for later plotting, etc)
# opp_mod_transfer is a boolean. True if distribution of opponent strategies is kept across games. False otherwise.
# theta = parameter, probability computer will play its best response to what it thinks human is. Truth = 90%. in [0,1].
# eps: parameter controlling noise in human choice. Probability human will DEVIATE from its best response. In [0,1].
theta <- par[1]
eps <- par[2]
num_strat = length(opp_strategy_vec)
# Load tables predicting oppponent move
rps_predict_opp <- read.csv("rps_predict_opp.csv")
fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
numbers_predict_opp <- read.csv("numbers_predict_opp.csv")
#lik_opp is a vector that holds and update probability distribution of opp strategy given actions.
lik_opp <- matrix(0.0,num_strat, nrow(data))
# Initiate prior vector as uniform
opp_prior_vec <-rep(1/num_strat, num_strat)
lik_hum <- matrix(0.0,nrow(data))
# br_hum is vector that stores best responses of human to actions taken by each level-k ai opponent at the round.
br_hum <- rep(NA,num_strat)
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
pred_file_opp <- switch(as.character(t_game),rps = rps_predict_opp,fwg = fwg_predict_opp, numbers = numbers_predict_opp)
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
pred_opp <- NA
lik_hum[t] <- 1/nopts
## KEY CODE FOR LEARNING TRANSFER !! (if no OM transfer, reset prior to uniform)
if (!(opp_mod_transfer)) {
opp_prior_vec <- rep(1/num_strat, num_strat)
lik_opp[,t] <- opp_prior_vec
}
# Rounds after first
} else {
# get prediction of opponent action from CSV files
for (strategy in opp_strategy_vec) {
k = match(strategy,opp_strategy_vec)
pred_opp <- as.character(filter(pred_file_opp, pred_file_opp$human_previous == as.character(data[t-1,"h_action"]) & pred_file_opp$computer_previous == as.character(data[t-1,"a_action"]))[[strategy]])
# Given opponent action predicted, what would be human action best response for each opp strat
br_hum[[k]] <- best_response(t_game,as.character(pred_opp))
# Multiply prior by likelihood of observation to get posterior. Done here ot take advantage of For loop.
if(as.character(data[t,"a_action"]) == pred_opp) {
lik_opp[k,t] <- (theta + (1-theta)/nopts)*opp_prior_vec[k]
} else {
lik_opp[k,t] <- ((1-theta)/nopts)*opp_prior_vec[k]
}
}
# Get which opponent strategy human current actn maps to (if index = 0 then human current action is br to level-0 comp_strat...)
indices <- which(br_hum %in% as.character(data[t,"h_action"]))
# THEN likelihood of current human action is just prior on opponent vec (haven't updated priors with curr opp act yet)
# if action is not predicted by any level-k OM, assume human chooses randomly with prob eps
if(length(indices) == 0){
lik_hum[t] <- eps/nopts
# if same br action for multiple possible ai opponents, add probabilities
} else {
lik_hum[t] <- sum(opp_prior_vec[indices])*(1-eps) + eps/nopts
}
# Standardising the probabilities and updating prior
lik_opp[,t] <- lik_opp[,t]/sum(lik_opp[,t])
opp_prior_vec <- lik_opp[,t]
}
# for debugging uncomment the line below
#cat(as.character(pred_opp),as.character(data[t,"a_action"]),lik_opp[,t],"\n")
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
# data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# exp1_ToM(c(0.9,0.99),data,c("level0","level1","level2"),"-2loglik",FALSE)
rps_predict_opp <- read.csv("rps_predict_opp.csv")
red_opp <- filter(rps_predict_opp, rps_predict_opp$human_previous == "R" & rps_predict_opp$computer_previous == "S")
as.character(red_opp[["level0"]])
load("~/Documents/comp_model_RPS/exp1_ToM_NT.RData")
load("exp1_ToM_TR.RData")
load("exp1_ToM_NT.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = exp1_ToM_TR[id]$value + 2*log(150),
"Bayes_No_Tr_BIC" = exp1_ToM_NT[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = exp1_ToM_TR[[id]]$par[1],
"theta_no_transfer" = exp1_ToM_NT[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = exp1_ToM_TR[[id]]$par[2],
"eps_no_transfer" = exp1_ToM_NT[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
exp1_ToM_TR <- load("exp1_ToM_TR.RData")
load("exp1_ToM_NT.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = exp1_ToM_TR[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = exp1_ToM_NT[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = exp1_ToM_TR[[id]]$par[1],
"theta_no_transfer" = exp1_ToM_NT[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = exp1_ToM_TR[[id]]$par[2],
"eps_no_transfer" = exp1_ToM_NT[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
