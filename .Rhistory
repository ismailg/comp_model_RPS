}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
# data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# EWA_states(0.1, data, "-2loglik")
EWA_states_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_states_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
#   EWA_states_modelling[[id]] <- optim(10.0,fn=EWA_states,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
EWA_states_modelling[[id]] <- DEoptim(fn=EWA_states, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}
save(EWA_states_modelling,file="EWA_states_modelling.RData")
View(EWA_states_modelling)
load("TR_bayes_modelling.RData")
load("NT_bayes_modelling.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = TR_bayes_modelling[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = NT_bayes_modelling[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = TR_bayes_modelling[[id]]$par[1],
"theta_no_transfer" = NT_bayes_modelling[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = TR_bayes_modelling[[id]]$par[2],
"eps_no_transfer" = NT_bayes_modelling[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
write.csv(All_results,file="All_results.csv",row.names = FALSE)
Table_results <- table(All_results[, "condition"],c("random","Bayes Tr","Bayes No Tr","QL", "QL_states","EWA","S_EWA","EWA_states", "MBM")[apply(All_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC","EWA_states_BIC","MBM_BIC")],1,which.min)])
write.csv(Table_results,file="Table_results.csv",row.names = TRUE)
kable(Table_results)
View(All_results)
EWA_states <- function(par,data,return_value){
lambda <- par[1]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
#Define matrix of state spaces for each game
G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
states_RPS <- paste0(G1$Var1,G1$Var2)
G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
states_FWG <- paste0(G2$Var1,G2$Var2)
G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
states_NUM <- paste0(G3$Var1,G3$Var2)
A_RPS = matrix(-0.5,9,3)
dimnames(A_RPS) = list(states_RPS, c("R", "P", "S"))
A_FWG = matrix(-0.5,9,3)
dimnames(A_FWG) = list(states_FWG, c("F", "W", "G"))
A_NUM = matrix(-0.5,25,5)
dimnames(A_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
# # Define reward matrices from the prospective of the row player (human in our case)
reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
state_vec <- switch(as.character(t_game), rps=states_RPS, fwg = states_FWG, numbers = states_NUM)
reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
N <- 1.0
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
curr_state <- paste0(h_act_prev,a_act_prev)
# Estimate phi(t)
R_t <- as.numeric(colnames(Att) == a_act_prev)
# cat("This is R(t)",R_t,"\n")
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
# cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
#cat("this is delta(t)",delta_t,"\n")
for (i in 1:nopts) {
action <- as.character(colnames(Att)[i])
cat("this is current action",action,"\n")
cat("this is previous human action",h_act_prev,"\n")
# Attraction vector update rule
Att[curr_state,i] <- (Phi_t*N*Att[curr_state,i]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
cat(Att,'\n')
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att[curr_state,])/sum(exp(lambda*Att[curr_state,]))
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, colnames(Att))
lik_hum[t] <- probs[[act_index]]
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
EWA_states(0.1, data, "-2loglik")
# EWA_states_modelling <- list()
# for(id in unique(dat$human_id)) {
#   EWA_states_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
# #   EWA_states_modelling[[id]] <- optim(10.0,fn=EWA_states,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# # }
#   EWA_states_modelling[[id]] <- DEoptim(fn=EWA_states, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
# }
#
# save(EWA_states_modelling,file="EWA_states_modelling.RData")
EWA_states <- function(par,data,return_value){
lambda <- par[1]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
#Define matrix of state spaces for each game
G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
states_RPS <- paste0(G1$Var1,G1$Var2)
G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
states_FWG <- paste0(G2$Var1,G2$Var2)
G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
states_NUM <- paste0(G3$Var1,G3$Var2)
A_RPS = matrix(-0.5,9,3)
dimnames(A_RPS) = list(states_RPS, c("R", "P", "S"))
A_FWG = matrix(-0.5,9,3)
dimnames(A_FWG) = list(states_FWG, c("F", "W", "G"))
A_NUM = matrix(-0.5,25,5)
dimnames(A_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
# # Define reward matrices from the prospective of the row player (human in our case)
reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
state_vec <- switch(as.character(t_game), rps=states_RPS, fwg = states_FWG, numbers = states_NUM)
reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
N <- 1.0
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
curr_state <- paste0(h_act_prev,a_act_prev)
state_indx <- match(curr_state,state_vec)
# Estimate phi(t)
R_t <- as.numeric(colnames(Att) == a_act_prev)
# cat("This is R(t)",R_t,"\n")
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
# cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
#cat("this is delta(t)",delta_t,"\n")
for (i in 1:nopts) {
action <- as.character(colnames(Att)[i])
cat("this is current action",action,"\n")
cat("this is previous human action",h_act_prev,"\n")
# Attraction vector update rule
Att[state_indx,i] <- (Phi_t*N*Att[state_indx,i]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
cat(Att[state_indx,],'\n')
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att[curr_state,])/sum(exp(lambda*Att[curr_state,]))
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, colnames(Att))
lik_hum[t] <- probs[[act_index]]
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
EWA_states(0.1, data, "-2loglik")
# EWA_states_modelling <- list()
# for(id in unique(dat$human_id)) {
#   EWA_states_modelling[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
# #   EWA_states_modelling[[id]] <- optim(10.0,fn=EWA_states,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# # }
#   EWA_states_modelling[[id]] <- DEoptim(fn=EWA_states, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
# }
#
# save(EWA_states_modelling,file="EWA_states_modelling.RData")
View(All_results)
EWA_states <- function(par,data,return_value){
lambda <- par[1]
# Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
#Define matrix of state spaces for each game
G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
states_RPS <- paste0(G1$Var1,G1$Var2)
G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
states_FWG <- paste0(G2$Var1,G2$Var2)
G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
states_NUM <- paste0(G3$Var1,G3$Var2)
A_RPS = matrix(-0.5,9,3)
dimnames(A_RPS) = list(states_RPS, c("R", "P", "S"))
A_FWG = matrix(-0.5,9,3)
dimnames(A_FWG) = list(states_FWG, c("F", "W", "G"))
A_NUM = matrix(-0.5,25,5)
dimnames(A_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
# # Define reward matrices from the prospective of the row player (human in our case)
reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
# Initiate likelihood by trial vector
lik_hum <- matrix(0.0,nrow(data))
for(t in 1:nrow(data)) {
t_game <- data[t,"game"]
if(t_game == "numbers") nopts <- 5 else nopts <- 3
if(data[t,"round"] == 1) {
# first round is uniform prediction
Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
state_vec <- switch(as.character(t_game), rps=states_RPS, fwg = states_FWG, numbers = states_NUM)
reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, numbers = reward_NUM)
game_data <- subset(data, game == t_game)
lik_hum[t] <- 1/nopts
N <- 1.0
R_t <- rep(0.0,nopts)
H_t <- R_t
} else {
indx <- data[t,"round"]
# Get reward and past human action
h_act_prev <- as.character(game_data[indx-1,"h_action"])
a_act_prev <- as.character(game_data[indx-1,"a_action"])
curr_state <- paste0(h_act_prev,a_act_prev)
state_indx <- match(curr_state,state_vec)
# Estimate phi(t)
R_t <- as.numeric(colnames(Att) == a_act_prev)
# cat("This is R(t)",R_t,"\n")
H_t <- (H_t*(indx-2) + R_t)/(indx-1)
# cat("this is H(t)",H_t,"\n")
Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
# cat("This is Phit(t)",Phi_t,"\t")
# Estimate vector Delta(t)
delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
#cat("this is delta(t)",delta_t,"\n")
for (i in 1:nopts) {
action <- as.character(colnames(Att)[i])
#cat("this is current action",action,"\n")
#cat("this is previous human action",h_act_prev,"\n")
# Attraction vector update rule
Att[state_indx,i] <- (Phi_t*N*Att[state_indx,i]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
}
#Update the value of N
N <- Phi_t*N + 1
#cat(Att[state_indx,],'\n')
# Assume human chooses action probabilistically using softmax on Attraction values
probs <- exp(lambda*Att[curr_state,])/sum(exp(lambda*Att[curr_state,]))
# Get actual human action and compute likelihood
h_act <- as.character(game_data[indx,"h_action"])
act_index <- match(h_act, colnames(Att))
lik_hum[t] <- probs[[act_index]]
}
}
if(return_value == "-2loglik") {
ret <- -2*sum(log(lik_hum))
if(is.infinite(ret) || is.nan(ret)) {
return(1e+300)
} else {
return(ret)
}
}
if(return_value == "likelihood_by_trial") return(lik_hum)
}
# data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# EWA_states(0.1, data, "-2loglik")
EWA_states_modelling <- list()
for(id in unique(dat$human_id)) {
EWA_states_modelling[[id]] <- list()
tdat <- subset(dat,human_id == id)
#   EWA_states_modelling[[id]] <- optim(10.0,fn=EWA_states,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
EWA_states_modelling[[id]] <- DEoptim(fn=EWA_states, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}
save(EWA_states_modelling,file="EWA_states_modelling.RData")
load("TR_bayes_modelling.RData")
load("NT_bayes_modelling.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = TR_bayes_modelling[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = NT_bayes_modelling[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = TR_bayes_modelling[[id]]$par[1],
"theta_no_transfer" = NT_bayes_modelling[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = TR_bayes_modelling[[id]]$par[2],
"eps_no_transfer" = NT_bayes_modelling[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
write.csv(All_results,file="All_results.csv",row.names = FALSE)
Table_results <- table(All_results[, "condition"],c("random","Bayes Tr","Bayes No Tr","QL", "QL_states","EWA","S_EWA","EWA_states", "MBM")[apply(All_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC","EWA_states_BIC","MBM_BIC")],1,which.min)])
write.csv(Table_results,file="Table_results.csv",row.names = TRUE)
kable(Table_results)
View(All_results)
View(All_results)
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
load("TR_bayes_modelling.RData")
load("NT_bayes_modelling.RData")
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("EWA_states_modelling.RData")
All_results <- data.frame()
for(id in unique(dat$human_id)) {
All_results <- rbind(All_results,
data.frame(
"ID" = id,
"condition" = dat[dat$human_id==id,"condition"][1],
"Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
# Bayesian updating with/without transfer
"Bayes_Tr_BIC" = TR_bayes_modelling[[id]]$value + 2*log(150),
"Bayes_No_Tr_BIC" = NT_bayes_modelling[[id]]$value+ 2*log(150),
# Theta is the parameter governing AI stochasticity. Truth is 0.9
"theta_transfer" = TR_bayes_modelling[[id]]$par[1],
"theta_no_transfer" = NT_bayes_modelling[[id]]$par[1],
# Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
"eps_transfer" = TR_bayes_modelling[[id]]$par[2],
"eps_no_transfer" = NT_bayes_modelling[[id]]$par[2],
# Q-Learning
"QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_Beta" = QL_modelling[[id]]$par[1],
# alpha -> learning rate in QL update
"QL_alpha" = QL_modelling[[id]]$par[2],
# Q-learning with last round states
"QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
# beta ->  inverse temperature parameter in softmax choice function
"QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
# alpha -> learning rate in QL update
"QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
# Parametric EWA
# Parametric EWA BIC
"EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
"EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
# Phi is depreciation of past attractions
"EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
# Delta is weight of foregone payoffs vs actual payoffs
"EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
# Rho is depreciation of the experience measure N(t)
"EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],
# Self-Tuning EWA (only 1 parameter)
# Parametric EWA BIC
"EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
"EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
#Lambda is a parameter of the softmax choice function (inverse Temperature)
"EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
# EWA with states
"EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
"EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1],
# MBM
# MBM BIC
"MBM_BIC" = MBM_modelling[[id]]$optim$bestval + 2*log(150),
# BEta parameter inverse temp in softmax
"MBM_beta" = MBM_modelling[[id]]$optim$bestmem[1],
"MBM_alpha" = MBM_modelling[[id]]$optim$bestmem[2]
))
}
write.csv(All_results,file="All_results.csv",row.names = FALSE)
Table_results <- table(All_results[, "condition"],c("random","Bayes Tr","Bayes No Tr","QL", "QL_states","EWA","S_EWA","EWA_states", "MBM")[apply(All_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC","EWA_states_BIC","MBM_BIC")],1,which.min)])
write.csv(Table_results,file="Table_results.csv",row.names = TRUE)
kable(Table_results)
View(All_results)
