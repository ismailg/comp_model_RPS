---
title: "opponent modelling"
author: "Ismail Guennouni & Maarten Speekenbrink"
date: "1 November 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

## Reading data

```{r}
dat <- read.csv("data20180719.csv")
#dat <- read.csv("test_data.csv")
```

## Preprosessing and transforming data

```{r}
# using some functions from the 'tidyverse' (also for me to get use to them ;-)
library(tidyr)
library(dplyr)
library(DEoptim)
library(optimParallel)
library(ggpubr)

# create a new data.frame in a 'wide format'
# widedata <- dat %>%
#   unite(game_block,game,block) %>% # this creates a new variable which combines game and block
#     group_by(human_id,condition,game_block) %>% # let the functions know you want to separate things by ID, condition, and game_block
#       summarize(mean_score = mean(score)) %>% # compute average score (i.e wins - losses)
#         spread(game_block,mean_score) # reformat in the 'wide' format
# # save the data file as a .csv to use in e.g. SPSS
# write.csv(widedata,row.names=FALSE,file="scores_wide.csv")

```



```{r estimate-models, cache=TRUE}
# read in the various files which 
rps_predict_opp <- read.csv("rps_predict_opp.csv")
fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
numbers_predict_opp <- read.csv("numbers_predict_opp.csv")

# transform 'winner' variable in numeric score
dat$score <- recode(dat$winner, human = 1, tie = 0, ai = -1)
# create a new variable 'block' with round 1...25 = block 1 and round 26...50 as block 2
dat$block <- as.numeric(cut(dat$round,2))

# recode actions to make them equal to the codes in these files
dat$h_action <- recode(dat$human_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G", "one" = "1", "two" = "2", "three" = "3", "four" = "4", "five" = "5")
dat$a_action <- recode(dat$ai_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G", "one" = "1", "two" = "2", "three" = "3", "four" = "4", "five" = "5")

write.csv(dat,file = "exp1_data.csv", row.names = FALSE)

# logit tranformation
my_logit <- function(x) {
  ret <- log(x/(1-x))
  ret[x==0] <- -Inf
  ret[x==1] <- Inf 
  return(ret)
}

# inverse logit transformation
my_logistic <- function(x) {
  ret <- 1/(1+exp(-x))
  ret[x == -Inf] <- 0.0
  ret[x == Inf] <- 1.0
  return(ret)
}
  

```

#### New work 


## Fitting simple Bayesian learning model with flat prior


## Fit a bayesian model to observed human play. Prior is vector of possible level-k opponent strategies.
```{r}
# # Function to produce best reponse to ai_action 
# best_response <- function(game,ai_action){
#   if (game == "rps"){
#     if (ai_action == "R") {return("P")}
#     else if (ai_action == "P") {return("S")}
#     else if (ai_action == "S") {return("R")}
#   } else if (game == "fwg"){ 
#     if (ai_action == "F") {return("W")}
#     else if (ai_action == "W") {return("G")}
#     else if (ai_action == "G") {return("F")}
#   } else if (game == "numbers") {
#     if (ai_action == "1") {return("2")}
#     else if (ai_action == "2") {return("3")}
#     else if (ai_action == "3") {return("4")}
#     else if (ai_action == "4") {return("5")}
#     else if (ai_action == "5") {return("1")}
#   }
# }
# 
# exp1_ToM <- function(par,data,opp_strategy_vec,return_value,opp_mod_transfer) {
#   
#   # dat = data subset for one participant
#   # opp_strategy_vec = vector of possible opponent strtegies, model assumes humans restrict opp strategy space to vector   c("level0","level1","level2")
#   # return = -2logLik 
#   # or "likelihood_by_trial" vector  (for later plotting, etc)
#   # opp_mod_transfer is a boolean. True if distribution of opponent strategies is kept across games. False otherwise. 
#   
#   # theta = parameter, probability computer will play its best response to what it thinks human is. Truth = 90%. in [0,1].
#   # eps: parameter controlling noise in human choice. Probability human will DEVIATE from its best response. In [0,1].
#   theta <- par[1]
#   eps <- par[2]
#   
#   num_strat = length(opp_strategy_vec) 
#   
#   # Load tables predicting oppponent move
#   rps_predict_opp <- read.csv("rps_predict_opp.csv")
#   fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
#   numbers_predict_opp <- read.csv("numbers_predict_opp.csv")
# 
#   #lik_opp is a vector that holds and update probability distribution of opp strategy given actions.
#   lik_opp <- matrix(0.0,num_strat, nrow(data))
#   # Initiate prior vector as uniform
#   opp_prior_vec <-rep(1/num_strat, num_strat)
#   
#   lik_hum <- matrix(0.0,nrow(data))
#   # br_hum is vector that stores best responses of human to actions taken by each level-k ai opponent at the round.
#   br_hum <- rep(NA,num_strat)
# 
#   
#   for(t in 1:nrow(data)) {
#     t_game <- data[t,"game"]
#     pred_file_opp <- switch(as.character(t_game),rps = rps_predict_opp,fwg = fwg_predict_opp, numbers = numbers_predict_opp)
#     
#     if(t_game == "numbers") nopts <- 5 else nopts <- 3
#     if(data[t,"round"] == 1) {
#       # first round is uniform prediction
#       pred_opp <- NA
#       lik_hum[t] <- 1/nopts
#       ## KEY CODE FOR LEARNING TRANSFER !! (if no OM transfer, reset prior to uniform)
#       if (!(opp_mod_transfer)) {
#         opp_prior_vec <- rep(1/num_strat, num_strat)
#         lik_opp[,t] <- opp_prior_vec
#         }
#       
#     # Rounds after first
#     } else {
#       # get prediction of opponent action from CSV files
#       for (strategy in opp_strategy_vec) {
#         k = match(strategy,opp_strategy_vec)
#         pred_opp <- as.character(filter(pred_file_opp, pred_file_opp$human_previous == as.character(data[t-1,"h_action"]) & pred_file_opp$computer_previous == as.character(data[t-1,"a_action"]))[[strategy]])
#         
#         # Given opponent action predicted, what would be human action best response for each opp strat
#         br_hum[[k]] <- best_response(t_game,as.character(pred_opp))
#         
#         # Multiply prior by likelihood of observation to get posterior. Done here ot take advantage of For loop. 
#         if(as.character(data[t,"a_action"]) == pred_opp) {
#           lik_opp[k,t] <- (theta + (1-theta)/nopts)*opp_prior_vec[k]
#         } else {
#           lik_opp[k,t] <- ((1-theta)/nopts)*opp_prior_vec[k]
#         }
#       }
#       # Get which opponent strategy human current actn maps to (if index = 0 then human current action is br to level-0 comp_strat...)
#       indices <- which(br_hum %in% as.character(data[t,"h_action"]))
# 
#       # THEN likelihood of current human action is just prior on opponent vec (haven't updated priors with curr opp act yet)
#       # if action is not predicted by any level-k OM, assume human chooses randomly with prob eps
#       if(length(indices) == 0){
#         lik_hum[t] <- eps/nopts
#       # if same br action for multiple possible ai opponents, add probabilities 
#       } else {
#         lik_hum[t] <- sum(opp_prior_vec[indices])*(1-eps) + eps/nopts
#       }
#       # Standardising the probabilities and updating prior
#       lik_opp[,t] <- lik_opp[,t]/sum(lik_opp[,t])
#       opp_prior_vec <- lik_opp[,t]
#     }
#     # for debugging uncomment the line below
#     #cat(as.character(pred_opp),as.character(data[t,"a_action"]),lik_opp[,t],"\n")
#   }
#   if(return_value == "-2loglik") {
#     ret <- -2*sum(log(lik_hum))
#     if(is.infinite(ret) || is.nan(ret)) {
#       return(1e+300)
#     } else {
#       return(ret)
#     }
#   }
#   if(return_value == "likelihood_by_trial") return(lik_hum)
# }
# 
# # data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# # exp1_ToM(c(0.9,0.99),data,c("level0","level1","level2"),"-2loglik",FALSE)
# ```
# 
# ```{r}
# # rps_predict_opp <- read.csv("rps_predict_opp.csv")
# # red_opp <- filter(rps_predict_opp, rps_predict_opp$human_previous == "R" & rps_predict_opp$computer_previous == "S")
# # 
# # as.character(red_opp[["level0"]])
# ```
# 
# 
# 
# # Fitting a bayesian model WITH BETWEEN GAME TRANSFER of opp strat distribution between games 
# ```{r, cache=TRUE}
# exp1_ToM_TR <- list()
# for(id in unique(dat$human_id)) {
#   exp1_ToM_TR[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   
#   # exp1_ToM_TR[[id]] <- DEoptim(fn=exp1_ToM, lower = c(0,0), upper = c(1,1), data=tdat, opp_strategy_vec = c("level0","level1","level2") ,return_value = "-2loglik", opp_mod_transfer = TRUE, control=list(trace = FALSE, parallelType=1,parVar = c("best_response")))
# 
#   exp1_ToM_TR[[id]] <- optim(c(0.1,0.1),fn=exp1_ToM,gr = NULL, data=tdat,opp_strategy_vec = c("level0","level1","level2"),return_value = "-2loglik", opp_mod_transfer = TRUE,lower = c(0.01,0.01), upper = c(0.99,0.99), method="L-BFGS-B")
# }
# save(exp1_ToM_TR,file="exp1_ToM_TR.RData")
# 
# ```
# 
# 
# # Fitting a bayesian model with NO BETWEEN GAME TRANSFER of opp strat distribution between games 
# ```{r, cache =TRUE}
# exp1_ToM_NT <- list()
# for(id in unique(dat$human_id)) {
#   exp1_ToM_NT[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   
#   # exp1_ToM_NT[[id]] <- DEoptim(fn=exp1_ToM, lower = c(0.0,0.0), upper = c(1.0,1.0), data=tdat, opp_strategy_vec = c("level0","level1","level2"),return_value = "-2loglik", opp_mod_transfer = FALSE, control=list(trace = FALSE, parallelType=0,parVar = c("best_response")))
# 
#   exp1_ToM_NT[[id]] <- optim(c(0.1,0.1),fn=exp1_ToM,gr = NULL, data=tdat,opp_strategy_vec = c("level0","level1","level2"),return_value = "-2loglik", opp_mod_transfer = FALSE ,lower = c(0.01,0.01), upper = c(0.99,0.99), method="L-BFGS-B")
# }
# 
# save(exp1_ToM_NT,file="exp1_ToM_NT.RData")
# 
# ```
# 
# 
# # Summarise results in dataframe for both transfer and no transfer models 
# ```{r}
# Nbayes_par_results <- data.frame()
# for(id in unique(dat$human_id)) {
# Nbayes_par_results <- rbind(Nbayes_par_results,
#                        data.frame(
#                          "id" = id,
#                          "condition" = dat[dat$human_id==id,"condition"][1],
#                          # BICs 
#                          "BIC_transfer" = exp1_ToM_TR[[id]]$value + 2*log(150),
#                          "BIC_no_transfer" = exp1_ToM_NT[[id]]$value+ 2*log(150),
#                          # Theta is the parameter governing AI stochasticity. Truth is 0.9
#                          "theta_transfer" = exp1_ToM_TR[[id]]$par[1],
#                          "theta_no_transfer" = exp1_ToM_NT[[id]]$par[1],
#                          # Epsilone is parameter showing cases where human deviates from predictions about opponent (lower better)
#                          "eps_transfer" = exp1_ToM_TR[[id]]$par[2],
#                          "eps_no_transfer" = exp1_ToM_NT[[id]]$par[2]))
# }
# 
# #Plotting the theta/eps scatter for all participants...
# plot(Nbayes_par_results$theta_transfer,Nbayes_par_results$eps_transfer, xlab="Theta", ylab= "Epsilone",col = ifelse(as.character(Nbayes_par_results$condition) == "Level2",'red','green'))
# # 
# write.csv(Nbayes_par_results,file="Nbayes_par_results.csv",row.names = FALSE)
```


## Q-learning (Basic)
```{r}
Q_learn <- function(par,data,return_value=c("-2loglik","likelihood_by_trial")){
  # actions
  # beta= inverse temperature parameter in softmax choice function
  # lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]
  Q_vals_RPS = matrix(0.0,3)
  names(Q_vals_RPS) <- c("R","P","S")
  Q_vals_FWG = matrix(0.0,3)
  names(Q_vals_FWG) <- c("F","W","G")
  Q_vals_NUM = matrix(0.0,5)
  names(Q_vals_NUM) <- c("1","2","3","4","5")
  
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, numbers = Q_vals_NUM)
      lik_hum[t] <- 1/nopts
    } else {
      # Get reward and past human action
      reward <- as.numeric(data[t-1,"score"])
      h_act_prev <- as.character(data[t-1,"h_action"])
      Q_vals[h_act_prev] <- Q_vals[[h_act_prev]] + as.numeric(alpha)*( reward - Q_vals[[h_act_prev]])
      
      # cat(Q_vals,"\n") 

      # Assume human chooses action probabilistically using softmax on Q values
      probs <- exp(Q_vals/beta)/sum(exp(Q_vals/beta))
      
      # Get actual human action and compute likelihood
      h_act <- as.character(data[t,"h_action"])
      act_index <- match(h_act, names(Q_vals))
      lik_hum[t] <- probs[[act_index]]
    }
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}


```

## Q-learning fit
```{r, cache=TRUE}
QL_modelling <- list()
for(id in unique(dat$human_id)) {
  QL_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  QL_modelling[[id]] <- optim(c(1,0.1),fn=Q_learn,gr = NULL, data=tdat,"-2loglik", lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")
}
save(QL_modelling,file="QL_modelling.RData")


```



## First attempt : Q-Learning with state space consisting of last round play (deprecated) 
```{r}

# Q_learn_states <- function(par,data,return_value,gamma){
#   # Par[1] -> beta= inverse temperature parameter in softmax choice function
#   # Par[2] -> lambda = learning rate (one per game?)
#   # data : participant and ai choice data.
#   # return_value=c("-2loglik","likelihood_by_trial")
#   # Gamma is discount factor for future rewards 
#   # Returns Q-values per trial and predicts choice using softmax
#   beta <- par[1]
#   alpha <- par[2]
# 
#   #gamma <- 0.9 
# 
#   #Define matrix of state spaces for each game 
#   G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
#   states_RPS <- paste0(G1$Var1,G1$Var2)
# 
#   G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
#   states_FWG <- paste0(G2$Var1,G2$Var2)
# 
#   G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
#   states_NUM <- paste0(G3$Var1,G3$Var2)
#   
#   Q_vals_RPS = matrix(-0.5,9,3)
#   dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))
# 
#   Q_vals_FWG = matrix(-0.5,9,3)
#   dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
#   
#   Q_vals_NUM = matrix(-0.5,25,5)
#   dimnames(Q_vals_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
#  
#   lik_hum <- matrix(0.0,nrow(data))
# 
#   for(t in 1:nrow(data)) {
#     t_game <- data[t,"game"]
#     if(t_game == "numbers") nopts <- 5 else nopts <- 3
#     
#     if(data[t,"round"] == 1) {
#       # first round is uniform prediction
#       Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, numbers = Q_vals_NUM)
#       state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, numbers = states_NUM)
#       lik_hum[t] <- 1/nopts
#       # Randomly select prev_state and actions for first round
#       curr_state <- sample(state_vec, size = 1)
#       h_act <- sample(colnames(Q_vals), size =1)
#       ai_act <- sample(colnames(Q_vals), size =1)
#       reward <- 0 
#       
#     } else {
#       # Get past human action and associated reward 
#       h_act <- as.character(data[t-1,"h_action"])
#       ai_act <- as.character(data[t-1,"a_action"])
#       reward <- as.numeric(data[t-1,"score"])
#     }
#     
#     
#     # cat(curr_state, " This is the previous state", "\n")
#     new_state <- paste0(h_act,ai_act)
#     # cat(new_state,"This is new state","\n")
#     
#     # Q_learning: update rule (time  = t-1)
#     Q_vals[curr_state, h_act] <- Q_vals[curr_state, h_act] + alpha*( reward + gamma*max(Q_vals[new_state,]) - Q_vals[curr_state, h_act])
#     
#     # Assume human chooses action probabilistically using softmax on Q values
#     probs <- exp(Q_vals[new_state,]/beta)/sum(exp(Q_vals[new_state,]/beta))
#     #if (data[t,"round"] == 50) { cat(Q_vals,"\n") }
#     
#     # Get actual human action and compute likelihood
#     h_act <- as.character(data[t,"h_action"])
#     act_index <- match(h_act, colnames(Q_vals))
#     lik_hum[t] <- probs[[act_index]]
#       
#     # Update state
#     curr_state <- new_state 
#     #}
#   }
#   if(return_value == "-2loglik") {
#     ret <- -2*sum(log(lik_hum))
#     if(is.infinite(ret) || is.nan(ret)) {
#       return(1e+300)
#     } else {
#       return(ret)
#     }
#   }
#   if(return_value == "likelihood_by_trial") return(lik_hum)
#   
# }

```



#Second attempt at QL algorithm with states: No guessing of initial states
```{r}
Q_learn_states <- function(par,data,return_value,gamma){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  states_NUM <- paste0(G3$Var1,G3$Var2)
  
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))

  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  
  Q_vals_NUM = matrix(-0.5,25,5)
  dimnames(Q_vals_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, numbers = Q_vals_NUM)
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, numbers = states_NUM)
      lik_hum[t] <- 1/nopts
      
    } else {
      # Get past human action and associated reward 
      h_act_prev <- as.character(data[t-1,"h_action"])
      ai_act_prev <- as.character(data[t-1,"a_action"])
      curr_state <- paste0(h_act_prev,ai_act_prev)
      
      
      h_act <- as.character(data[t,"h_action"])
      ai_act <- as.character(data[t,"a_action"])
      reward <- as.numeric(data[t,"score"])
      
      new_state <- paste0(h_act,ai_act)
      
      # Assume human chooses action probabilistically using softmax on Q values
      probs <- exp(Q_vals[curr_state,]/beta)/sum(exp(Q_vals[curr_state,]/beta))
      # Get actual human next action and compute likelihood
      act_index <- match(h_act, colnames(Q_vals))

      
      lik_hum[t] <- probs[[act_index]]
      
       # Q_learning: update rule (time = t)
      Q_vals[curr_state, h_act] <- Q_vals[curr_state, h_act] + alpha*( reward + gamma*max(Q_vals[new_state,]) - Q_vals[curr_state, h_act])
      
      # Update state
      curr_state <- new_state 

    }
    
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```

```{r}
# data34 <- subset(dat,human_id == "QSuzBXpbyRc370HsAACW")
# Q_learn_states(c(5, 0.5),data34 ,return_value = "likelihood_by_trial",gamma = 0 )


```

## Fitting Q_learning with states
```{r , cache =TRUE}
# data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# Q_learn_states(c(0.1,0.1), data, "-2loglik")

QL_states_modelling <- list()
for(id in unique(dat$human_id)) {
  QL_states_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  # QL_states_modelling[[id]] <- optim(c(1,0.1),fn=Q_learn_states,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   QL_states_modelling[[id]] <- DEoptim(fn=Q_learn_states,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0, control=list(trace = FALSE,parallelType=1))
}

save(QL_states_modelling, file="QL_states_modelling.RData")


# QL_states_gamma <- list()
# for(id in unique(dat$human_id)) {
#   QL_states_gamma[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   QL_states_gamma[[id]] <- DEoptim(fn=Q_learn_states,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0.9, control=list(trace = FALSE,parallelType=1))
# }
# save(QL_states_gamma,file="QL_states_gamma.RData")

```

```{r}
# OK, let's compare BICs and parameters for Gamma = 0 and Gamma = 0.9 (both for initial Q-values of -0.5)

compare_QL_states <- data.frame()
for(id in unique(dat$human_id)) {
compare_QL_states  <- rbind(compare_QL_states ,
                       data.frame(
                         "id" = id,
                         "condition" = dat[dat$human_id==id,"condition"][1],
                         "Zero_Gamma_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
                         "High_gamma_BIC" = QL_states_gamma[[id]]$optim$bestval + 2*log(150),
                         "diff" = QL_states_modelling[[id]]$optim$bestval - QL_states_gamma[[id]]$optim$bestval
                       ))
}

```
#Notes: Q-Learning as done above is far from optimal. The reason is that most states are never visited during play ( anecdotal evidence, about 40-60% in RPS/FWG and 10-20% in NUM). This method needs a lot more rounds to start modelling players behavior correctly. 


## Parametric Experience Weighted Attraction model (Camerer & Ho 1997)
```{r}
EWA_par <- function(par,data,return_value){
  # par: vector of parameters to the EWA model
  # Data :choice data by trial for each participant
  # Returns sum of loglikelihoods or likelihood per trial
  phi <- par[1]
  delta <- par[2]
  rho <- par[3]
  lambda <- par[4]


  # Define attraction vectors for each game
  A_RPS = matrix(0.0,3)
  names(A_RPS) <- c("R","P","S")
  A_FWG = matrix(0.0,3)
  names(A_FWG) <- c("F","W","G")
  A_NUM = matrix(0.0,5)
  names(A_NUM) <- c("1","2","3","4","5")
  
  # # Define reward matrices from the prospective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
  dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))

  # Initiate likelihood by trial vector
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
      reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
      lik_hum[t] <- 1/nopts
      # Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
      N <- 1
      
    } else {
      # Get reward and past human action
      h_act_prev <- as.character(data[t-1,"h_action"])
      a_act_prev <- as.character(data[t-1,"a_action"])
      for (i in 1:length(Att)) {
        action <- as.character(names(Att)[i])
        # cat("this is current strat",action,"\n")
        # cat("this is previous human action",h_act_prev,"\n")

        # Attraction vector update rule
        Att[[i]] <- (phi*N*Att[[i]]  + ( delta + (1- delta)*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (rho*N + 1)
      }
      #Update the value of N
      N <- rho*N + 1

      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att)/sum(exp(lambda*Att))

      # Get actual human action and compute likelihood
      h_act <- as.character(data[t,"h_action"])
      act_index <- match(h_act, names(Att))
      lik_hum[t] <- probs[[act_index]]
    }
  }

 if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret) ) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
}
```

## Fitting Parametric EWA to data 
```{r, cache =TRUE}

EWA_modelling <- list()
for(id in unique(dat$human_id)) {
  EWA_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  # EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
  EWA_modelling[[id]] <- DEoptim(fn=EWA_par,lower = c(0.01,0.01,0.01,0.01), upper = c(10,1,1,10), data=tdat,"-2loglik",control=list(trace = FALSE,parallelType=1))
}

save(EWA_modelling,file="EWA_modelling.RData")

```


##  Self-Tuning EWA (Camerer & Ho 2007) 
```{r}

EWA_self <- function(par,data,return_value){

  lambda <- par[1]
  # Initiate N(0) = 1 as in Camerer and Ho 1997 paper. 

  
  # Define attraction vectors for each game 
  A_RPS = matrix(0.0,3)
  names(A_RPS) <- c("R","P","S")
  A_FWG = matrix(0.0,3)
  names(A_FWG) <- c("F","W","G")
  A_NUM = matrix(0.0,5)
  names(A_NUM) <- c("1","2","3","4","5")
  
  # # Define reward matrices from the prospective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
  dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  
  # Initiate likelihood by trial vector 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
      reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
      game_data <- subset(data, game == t_game)
      lik_hum[t] <- 1/nopts
      N <- 1.0

      R_t <- rep(0.0,nopts)
      H_t <- R_t
      
    } else {
      
      indx <- data[t,"round"]
      # Get reward and past human action
      h_act_prev <- as.character(game_data[indx-1,"h_action"])
      a_act_prev <- as.character(game_data[indx-1,"a_action"])

      # Estimate phi(t)
      R_t <- as.numeric(names(Att) == a_act_prev)
      # cat("This is R(t)",R_t,"\n")
      
      H_t <- (H_t*(indx-2) + R_t)/(indx-1)
      # cat("this is H(t)",H_t,"\n")
      
      Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
      # cat("This is Phit(t)",Phi_t,"\t")
      
      # Estimate vector Delta(t)

      delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
      #cat("this is delta(t)",delta_t,"\n")
      
      for (i in 1:length(Att)) {
        action <- as.character(names(Att)[i])
        # cat("this is current strat",action,"\n")
        # cat("this is previous humna action",h_act_prev,"\n")
        
        # Attraction vector update rule 
        Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
      }
      #Update the value of N 
      N <- Phi_t*N + 1
      #cat(Att,'\n')
      
      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att)/sum(exp(lambda*Att))
      
      # Get actual human action and compute likelihood
      h_act <- as.character(game_data[indx,"h_action"])
      act_index <- match(h_act, names(Att))
      lik_hum[t] <- probs[[act_index]]
    }
  }
  
 if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
}

```


```{r}
##Plotting self_tuning EWA likelihood function for lambda from 0 to 100 
# 
# lambdas <- seq(1,10,0.01)
# count = 0.0
# 
# 
# 
# for(id in unique(dat$human_id)) {
#   if (count <= 15) {
#       tdat <- subset(dat,human_id == id)
#       likelihoods <- sapply(lambdas, EWA_self, tdat, "-2loglik")
#       plot(lambdas,likelihoods)
#       count <- count + 1
#   }
# }

```
  
## Fitting Self-Tuning EWA to data 
```{r, cache=TRUE}


EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
  EWA_self_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
#   EWA_self_modelling[[id]] <- optim(10.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  EWA_self_modelling[[id]] <- DEoptim(fn=EWA_self, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}

save(EWA_self_modelling,file="EWA_self_modelling.RData")

  
```

```{r}
# cat(EWA_self_modelling[["38VxtUSv_h6RR5-tAAA2"]]$optim$bestmem)
```



##  Self-Tuning EWA with prior round play as state (Camerer & Ho 2007) 
```{r}

EWA_states <- function(par,data,return_value){

  lambda <- par[1]
  # Initiate N(0) = 1 as in Camerer and Ho 1997 paper. 

  
  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  states_NUM <- paste0(G3$Var1,G3$Var2)
  
  A_RPS = matrix(-0.5,9,3)
  dimnames(A_RPS) = list(states_RPS, c("R", "P", "S"))

  A_FWG = matrix(-0.5,9,3)
  dimnames(A_FWG) = list(states_FWG, c("F", "W", "G"))
  
  A_NUM = matrix(-0.5,25,5)
  dimnames(A_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))

  
  # # Define reward matrices from the perspective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
  dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  
  # Initiate likelihood by trial vector 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    if(data[t,"round"] == 1) {
      Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
      state_vec <- switch(as.character(t_game), rps=states_RPS, fwg = states_FWG, numbers = states_NUM)
      reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, numbers = reward_NUM)
      game_data <- subset(data, game == t_game)
      # first round is uniform prediction
      lik_hum[t] <- 1/nopts
      N <- 1.0

      R_t <- rep(0.0,nopts)
      H_t <- R_t
      
      # Randomly select prev_state and actions for first round
      curr_state <- sample(state_vec, size = 1)
      h_act_prev <- sample(colnames(Att), size =1)
      a_act_prev <- sample(colnames(Att), size =1)
      #reward <- 0 
      
    } else {
      
      indx <- data[t,"round"]
      state_indx <- match(curr_state,state_vec)
      #cat("this is the current state index", state_indx, "\n")
      
      # Get reward and past human action
      h_act_prev <- as.character(game_data[indx-1,"h_action"])
      a_act_prev <- as.character(game_data[indx-1,"a_action"])
      new_state <- paste0(h_act_prev,a_act_prev)

      # Estimate phi(t)
      R_t <- as.numeric(colnames(Att) == a_act_prev)
      #cat("This is R(t)",R_t,"\n")
      
      H_t <- (H_t*(indx-2) + R_t)/(indx-1)
      #cat("this is H(t)",H_t,"\n")
      
      Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
      #cat("This is Phit(t)",Phi_t,"\t")
      
      # Estimate vector Delta(t)

      delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
      #cat("this is delta(t)",delta_t,"\n")
      
      for (i in 1:nopts) {
        action <- as.character(colnames(Att)[i])

        #cat("this is current indexed action",action,"\n")
        #cat("this is previous human action",h_act_prev,"\n")
        
        # Attraction vector update rule
        Att[state_indx,i] <- (Phi_t*N*Att[state_indx,i]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
      }
      
      #Update the value of N 
      N <- Phi_t*N + 1

      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att[new_state,])/sum(exp(lambda*Att[new_state,]))
      
      # Get actual human action and compute likelihood
      h_act <- as.character(game_data[indx,"h_action"])
      act_index <- match(h_act, colnames(Att))
      lik_hum[t] <- probs[[act_index]]
      
      curr_state <- new_state
      
      # if (indx == 50) {
      #   cat("this is the attraction matrix at last round of", "\n")
      #   print(Att)
      # }
    }
  }
  
 if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
}

```

```{r, cache=TRUE}
# data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# EWA_states(0.1, data, "-2loglik")
# optim(10.0,fn=EWA_states,gr = NULL, data=data,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")

EWA_states_modelling <- list()
for(id in unique(dat$human_id)) {
  EWA_states_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
#   EWA_states_modelling[[id]] <- optim(10.0,fn=EWA_states,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  EWA_states_modelling[[id]] <- DEoptim(fn=EWA_states, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}

save(EWA_states_modelling,file="EWA_states_modelling.RData")

```



## What next?: Fitting models that explicitly take into account opponent strategy. LOLA? Won't work, it suffers from same issues as normal QL in that most states are not visited during game play. Influence model? Like belief lerning. it still has no sequential memory, still computes distribution on beliefs. We need to incorporate models that can mimic human memory by looking at history of play and leverage sequence prediction capabilities. 


### Model based model: learn transition probabilities to new states. Weigh the value of an action by the conditional probability of ending up in a future state times the value of the new state.... once you take an action in current round, you can only end up in 3 states, with probabilities p, q and 1-p-q. You are learning these probabilities by updating priors (initial prior is dirichlet). 
```{r}
MBM <- function(par,data,return_value,gamma){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]

  #gamma <- 0.9 

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  states_NUM <- paste0(G3$Var1,G3$Var2)
  
  # Matrices to store Q_values in each state 
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))
  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  Q_vals_NUM = matrix(-0.5,25,5)
  dimnames(Q_vals_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
  
  # Transition Matrices 
  Transit_RPS = matrix(1/9,9,9)
  dimnames(Transit_RPS) = list(states_RPS, states_RPS)
  Transit_FWG = matrix(1/9,9,9)
  dimnames(Transit_FWG) = list(states_FWG, states_FWG)
  Transit_NUM = matrix(1/25,25,25)
  dimnames(Transit_NUM) = list(states_NUM, states_NUM)
  
  
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, numbers = Q_vals_NUM)
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, numbers = states_NUM)
      Transit <- switch(as.character(t_game),rps=Transit_RPS,fwg = Transit_FWG, numbers = Transit_NUM)
      lik_hum[t] <- 1/nopts
      # Randomly select prev_state and actions for first round
      curr_state <- sample(state_vec, size = 1)
      h_act <- sample(colnames(Q_vals), size =1)
      ai_act <- sample(colnames(Q_vals), size =1)
      reward <- 0 
      
    } else {
      # Get past human action and associated reward 
      h_act <- as.character(data[t-1,"h_action"])
      ai_act <- as.character(data[t-1,"a_action"])
      reward <- as.numeric(data[t-1,"score"])
    }
    
    
    # cat(curr_state, " This is the previous state", "\n")
    new_state <- paste0(h_act,ai_act)
    # cat(new_state,"This is new state","\n")
    
    
    # Update Transit matrix ....first create hot vector = 1 if new state, 0 otherwise 
    hot_vector <- state_vec == new_state
  
    
    # Transit probs update: TD learning on transition matrix, learning rate fixed at 0.1
    Transit[curr_state,] <- Transit[curr_state,] + 0.1* (hot_vector - Transit[curr_state,])
    #cat(Transit[curr_state,], "\n")
    
    # Update Q-values as current reward + a weighted (by trasnsit probs) average of future Q-values.
    Q_Row_max <- apply(Q_vals, 1, max, na.rm = TRUE)
    #cat(Transit[curr_state,] * Q_Row_max, "\n")
    Q_vals[curr_state, h_act] <- reward +  gamma*( Transit[curr_state,] %*% Q_Row_max  ) 
    
    
    
    # Assume human chooses action probabilistically using softmax on Q values
    probs <- exp(Q_vals[new_state,]/beta)/sum(exp(Q_vals[new_state,]/beta))
    #if (data[t,"round"] == 50) { cat(Q_vals,"\n") }
    
    # Get actual human action and compute likelihood
    h_act <- as.character(data[t,"h_action"])
    act_index <- match(h_act, colnames(Q_vals))
    lik_hum[t] <- probs[[act_index]]
      
    # Update state
    curr_state<- new_state 
    #}
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```


```{r, cache=TRUE}
MBM_modelling <- list()
for(id in unique(dat$human_id)) {
  MBM_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  # MBM_modelling[[id]] <- optim(c(1,0.1),fn=QMBM,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   MBM_modelling[[id]] <- DEoptim(fn=MBM,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0.9, control=list(trace = FALSE,parallelType=1))
}

save(MBM_modelling, file="MBM_modelling.RData")

```


## Putting results together 
```{r}
# load("QL_modelling.RData")
# load("QL_states_modelling.RData")
# load("EWA_modelling.RData")
# load("EWA_self_modelling.RData")
# load("MBM_modelling.RData")
# load("EWA_states_modelling.RData")


All_results <- data.frame()
for(id in unique(dat$human_id)) {
  All_results <- rbind(All_results,
                       data.frame(
                         "ID" = id,
                         "condition" = dat[dat$human_id==id,"condition"][1],
                         "Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
                         
                         # Bayesian updating with/without transfer
                         "Bayes_Tr_BIC" = exp1_Bayes_game_Tr[[id]]$optim$bestval+ 1*log(150),
                         "Bayes_No_Tr_BIC" = exp1_Bayes_no_Tr[[id]]$optim$bestval + 1*log(150),
                         # Theta is the parameter governing AI stochasticity. Truth is 0.9
                         "theta_transfer" = exp1_Bayes_game_Tr[[id]]$optim$bestmem[[1]],
                         "theta_no_transfer" = exp1_Bayes_no_Tr[[id]]$optim$bestmem[[1]],
                         
                                                                                    
                         # Q-Learning
                         "QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
                         # beta ->  inverse temperature parameter in softmax choice function
                         "QL_Beta" = QL_modelling[[id]]$par[1],
                         # alpha -> learning rate in QL update
                         "QL_alpha" = QL_modelling[[id]]$par[2],

                         
                         # Q-learning with last round states
                         "QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
                         # beta ->  inverse temperature parameter in softmax choice function
                         "QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
                         # alpha -> learning rate in QL update
                         "QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
                         
                        
                         # Parametric EWA
                         # Parametric EWA BIC
                         "EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
                         "EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
                         # Phi is depreciation of past attractions
                         "EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
                         # Delta is weight of foregone payoffs vs actual payoffs
                         "EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
                         # Rho is depreciation of the experience measure N(t)
                         "EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
                         #Lambda is a parameter of the softmax choice function (inverse Temperature)
                         "EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],


                         # Self-Tuning EWA (only 1 parameter)
                          # Parametric EWA BIC
                         "EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
                         "EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
                         #Lambda is a parameter of the softmax choice function (inverse Temperature)
                         "EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
                         
                         # EWA with states
                         
                         "EWA_states_BIC" = EWA_states_modelling[[id]]$optim$bestval + 1*log(150),
                         "EWA_states_Lambda" = EWA_states_modelling[[id]]$optim$bestmem[1]
                         

                         ))
}

write.csv(All_results,file="exp1_all_results.csv",row.names = FALSE)

```


## Calucalting BIC weights
```{r}

comp_BICs <- All_results[c("ID","Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC","EWA_states_BIC")]

exp1_BIC_weights <- comp_BICs["ID"]
exp1_BIC_weights[,2:ncol(comp_BICs)] <- t(apply(comp_BICs[,2:ncol(comp_BICs)], 1, function(i) exp(-0.5*(i-min(i)) )))
colnames(exp1_BIC_weights) <-colnames(comp_BICs)

exp1_BIC_weights[,2:ncol(exp1_BIC_weights)] <- t(apply(exp1_BIC_weights[,-1], 1, function(i) round(i/sum(i),2)))
                     
                     
```

## Building table of best model per participant
```{r}

Table_results <- table(All_results[, "condition"],c("random","ToM_BT","ToM_NBT","QL", "QL_states","EWA","S_EWA","EWA_states")[apply(All_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC","EWA_states_BIC")],1,which.min)])

 write.csv(Table_results,file="Table_results.csv",row.names = TRUE)
 kable(Table_results)
 #barplot(Table_results)
 
```
##### Looks like most participants behavior can be explained by QL_states. Bayesian models best explain a few more behaviors, while the others are consistent with random behavior, and only 1 with QL and 1 with S_EWA. 
 
 
###### Question: Can we see evidence for transfer by looking at difference in BICS btwn Bayes with and without transfer. Let's test correlation between difference in BICs  with early rounds score (evidence for transfer) 
```{r}
# id_results_tst <- subset(All_results, ID == "38VxtUSv_h6RR5-tAAA2")
```

```{r}

exp1_model_comp <- data.frame()
for(id in unique(dat$human_id)) {
  tdat <- subset(dat,human_id == id)
  tot_score <- sum(tdat$score)
  tot_time <- sum(tdat$human_rt)
  #early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg"))
  early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg" | game =="numbers") )
  tr_score <- sum(early_dat$score)
  id_results <- subset(All_results, ID == id)
  min_BIC <- apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC")],1,min)
  
  best_model <- c("random","ToM_Game_Tr","ToM_No_Tr","QL", "QL_states","EWA","S_EWA", "EWA_states")[apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC","EWA_states_BIC")],1,which.min)]
  # 
  exp1_model_comp <- rbind(exp1_model_comp,
                       data.frame(
                         "human_id" = id,
                         "condition" = dat[dat$human_id==id,"condition"][1],
                         "Early_game_score" = tr_score,
                         "Total_score" = tot_score,
                         "Best_model" = best_model,
                         "Total_time" = sum(tdat$human_rt),
                         "TR_minus_NT_BIC" = id_results[,"Bayes_Tr_BIC"] - id_results[,"Bayes_No_Tr_BIC"],
                         "Rand_minus_best_BIC" =  id_results[,"Random_BIC"] - min_BIC

                       ))
}

write.csv(exp1_model_comp,file = "exp1_model_comp.csv", row.names = FALSE)

```

### Correlate difference between BICs of Bayes transfer and no transfer with early rounds score (evidence for transfer) 
```{r}

cor.test(exp1_model_comp$TR_minus_NT_BIC,exp1_model_comp$Early_game_score, method="spearman")

```

### Correlation between early game score and difference between best model and random BIC
```{r}
cor.test(exp1_model_comp$Rand_minus_best_BIC, exp1_model_comp$Early_game_score, method="spearman")

```
### Histogram of best fitting models 
```{r}
barplot(table(exp1_model_comp$Best_model))
```
# Notes : There is positive and stat significant correlation between the difference between the best model BIC and random BIC on one hand, and the early scores from the target games ( FWG and NUM). What does this mean? It can be interpreted as a validation of early game score as good measure of how non-random play is....


## Let's compare total scores of each participant by the model of best fit, see if Bayes+transfer total scores are higher than Bayes + no transfer  

```{r}

# ggboxplot(exp1_model_comp, x = "model", y = "Total_score",
#           color = "model", palette = c("#00AFBB", "#E7B800", "#FC4E07"),
#           order = c("Random", "No Transfer", "Transfer"),
#           ylab = "Total Score", xlab = "Model with best fit")

exp1_model_comp$model <- recode(exp1_model_comp$Best_model,"Bayes Tr" = "Tranfer","Bayes No Tr" = "No Transfer", "QL_states" = "Q_Learning",  .default = "Random")
model <- factor(exp1_model_comp$model)
condition <- factor(exp1_model_comp$condition)

# Total score by best predictive model 
tapply(exp1_model_comp$Total_score, model, mean)

# Compute the analysis of variance
res.aov <- aov(Total_score ~ model + condition, data = exp1_model_comp)
# Summary of the analysis
summary(res.aov)
TukeyHSD(res.aov)
```

## What if we remove from data people who play randomly and then compare Transfer v No Transfer? 
```{r}

Non_Random_Only <- subset(exp1_model_comp, model != "Random" )

group_by(Non_Random_Only, model) %>%
  summarise(
    count = n(),
    mean = mean(Early_game_score, na.rm = TRUE),
    sd = sd(Early_game_score, na.rm = TRUE)
  )

res2 <- aov(Early_game_score ~ model, data = Non_Random_Only) 
summary(res2)
TukeyHSD(res2)
```

# Total time by best predictive model
```{r}
tapply(exp1_model_comp$Total_time, model, mean)

time.aov <- aov(Total_time ~ model, data = exp1_model_comp)
# Summary of the analysis
summary(time.aov)
TukeyHSD(time.aov)

``` 
#Early game scores for people best modelled with a Bayesian model with transfer are indeed higher than those best modelled without transfer but difference is not statistically significant. 



