---
title: "opponent modelling"
author: "Ismail Guennouni & Maarten Speekenbrink"
date: "1 November 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE)
```

## Reading data

```{r}
dat <- read.csv("data20180719.csv")
#dat <- read.csv("test_data.csv")
```

## Preprosessing and transforming data

```{r}
# using some functions from the 'tidyverse' (also for me to get use to them ;-)
library(tidyr)
library(dplyr)
library(DEoptim)
library(optimParallel)
library(ggpubr)

# create a new data.frame in a 'wide format'
# widedata <- dat %>%
#   unite(game_block,game,block) %>% # this creates a new variable which combines game and block
#     group_by(human_id,condition,game_block) %>% # let the functions know you want to separate things by ID, condition, and game_block
#       summarize(mean_score = mean(score)) %>% # compute average score (i.e wins - losses)
#         spread(game_block,mean_score) # reformat in the 'wide' format
# # save the data file as a .csv to use in e.g. SPSS
# write.csv(widedata,row.names=FALSE,file="scores_wide.csv")

```



```{r estimate-models, cache=TRUE}
# read in the various files which 
rps_predict_opp <- read.csv("rps_predict_opp.csv")
fwg_predict_opp <- read.csv("fwg_predict_opp.csv")
numbers_predict_opp <- read.csv("numbers_predict_opp.csv")

# transform 'winner' variable in numeric score
dat$score <- recode(dat$winner, human = 1, tie = 0, ai = -1)
# create a new variable 'block' with round 1...25 = block 1 and round 26...50 as block 2
dat$block <- as.numeric(cut(dat$round,2))

# recode actions to make them equal to the codes in these files
dat$h_action <- recode(dat$human_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G", "one" = "1", "two" = "2", "three" = "3", "four" = "4", "five" = "5")
dat$a_action <- recode(dat$ai_action,"rock" = "R", "paper" = "P", "scissors" = "S", "fire" = "F", "water" = "W", "grass" = "G", "one" = "1", "two" = "2", "three" = "3", "four" = "4", "five" = "5")

write.csv(dat,file = "exp1_data.csv", row.names = FALSE)

# logit tranformation
my_logit <- function(x) {
  ret <- log(x/(1-x))
  ret[x==0] <- -Inf
  ret[x==1] <- Inf 
  return(ret)
}

# inverse logit transformation
my_logistic <- function(x) {
  ret <- 1/(1+exp(-x))
  ret[x == -Inf] <- 0.0
  ret[x == Inf] <- 1.0
  return(ret)
}
  

```

#### New work 
## Q-learning (Basic)
```{r}
Q_learn <- function(par,data,return_value=c("-2loglik","likelihood_by_trial")){
  # actions
  # beta= inverse temperature parameter in softmax choice function
  # lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]
  Q_vals_RPS = matrix(0.0,3)
  names(Q_vals_RPS) <- c("R","P","S")
  Q_vals_FWG = matrix(0.0,3)
  names(Q_vals_FWG) <- c("F","W","G")
  Q_vals_NUM = matrix(0.0,5)
  names(Q_vals_NUM) <- c("1","2","3","4","5")
  
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, numbers = Q_vals_NUM)
      lik_hum[t] <- 1/nopts
    } else {
      # Get reward and past human action
      reward <- as.numeric(data[t-1,"score"])
      h_act_prev <- as.character(data[t-1,"h_action"])
      Q_vals[h_act_prev] <- Q_vals[[h_act_prev]] + as.numeric(alpha)*( reward - Q_vals[[h_act_prev]])
      
      # cat(Q_vals,"\n") 

      # Assume human chooses action probabilistically using softmax on Q values
      probs <- exp(Q_vals/beta)/sum(exp(Q_vals/beta))
      
      # Get actual human action and compute likelihood
      h_act <- as.character(data[t,"h_action"])
      act_index <- match(h_act, names(Q_vals))
      #cat(as.character(act_index))
      lik_hum[t] <- probs[[act_index]]
    }
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}


```

## Q-learning fit
```{r, cache=TRUE}
QL_modelling <- list()
for(id in unique(dat$human_id)) {
  QL_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  QL_modelling[[id]] <- optim(c(1,0.1),fn=Q_learn,gr = NULL, data=tdat,"-2loglik", lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")
}
save(QL_modelling,file="QL_modelling.RData")


```


#Second attempt at QL algorithm with states: No guessing of initial states
```{r}
Q_learn_states <- function(par,data,return_value,gamma){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[[1]]
  alpha <- par[[2]]

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  states_NUM <- paste0(G3$Var1,G3$Var2)
  
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))

  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  
  Q_vals_NUM = matrix(-0.5,25,5)
  dimnames(Q_vals_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, numbers = Q_vals_NUM)
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, numbers = states_NUM)
      lik_hum[t] <- 1/nopts
      
    } else {
      # Get past human action and associated reward 
      h_act_prev <- as.character(data[t-1,"h_action"])
      ai_act_prev <- as.character(data[t-1,"a_action"])
      curr_state <- paste0(h_act_prev,ai_act_prev)
      
      
      h_act <- as.character(data[t,"h_action"])
      ai_act <- as.character(data[t,"a_action"])
      reward <- as.numeric(data[t,"score"])
      
      new_state <- paste0(h_act,ai_act)
      
      # Assume human chooses action probabilistically using softmax on Q values
      probs <- exp(Q_vals[curr_state,]/beta)/sum(exp(Q_vals[curr_state,]/beta))
      
      # Get actual human next action and compute likelihood
      act_index <- match(h_act, colnames(Q_vals))
      lik_hum[t] <- probs[[act_index]]
      
       # Q_learning: update rule (time = t)
      Q_vals[curr_state, h_act] <- Q_vals[curr_state, h_act] + alpha*( reward + gamma*max(Q_vals[new_state,]) - Q_vals[curr_state, h_act])
      
      # Update state
      curr_state <- new_state 

    }
    
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```

```{r}
data34 <- subset(dat,human_id == "QSuzBXpbyRc370HsAACW")

# Q_learn_states(c(5, 0.5),data34 ,return_value = "likelihood_by_trial",gamma = 0 )


```

## Fitting Q_learning with states
```{r , cache =TRUE}
# data = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# Q_learn_states(c(0.1,0.1), data, "-2loglik")

QL_states_modelling <- list()
for(id in unique(dat$human_id)) {
  QL_states_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  # QL_states_modelling[[id]] <- optim(c(1,0.1),fn=Q_learn_states,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   QL_states_modelling[[id]] <- DEoptim(fn=Q_learn_states,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0, control=list(trace = FALSE,parallelType=1))
}

save(QL_states_modelling, file="QL_states_modelling.RData")


# QL_states_gamma <- list()
# for(id in unique(dat$human_id)) {
#   QL_states_gamma[[id]] <- list()
#   tdat <- subset(dat,human_id == id)
#   QL_states_gamma[[id]] <- DEoptim(fn=Q_learn_states,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0.9, control=list(trace = FALSE,parallelType=1))
# }
# save(QL_states_gamma,file="QL_states_gamma.RData")

```

```{r}
# OK, let's compare BICs and parameters for Gamma = 0 and Gamma = 0.9 (both for initial Q-values of -0.5)

compare_QL_states <- data.frame()
for(id in unique(dat$human_id)) {
compare_QL_states  <- rbind(compare_QL_states ,
                       data.frame(
                         "id" = id,
                         "condition" = dat[dat$human_id==id,"condition"][1],
                         "Zero_Gamma_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
                         "High_gamma_BIC" = QL_states_gamma[[id]]$optim$bestval + 2*log(150),
                         "diff" = QL_states_modelling[[id]]$optim$bestval - QL_states_gamma[[id]]$optim$bestval
                       ))
}

```
#Notes: Q-Learning as done above is far from optimal. The reason is that most states are never visited during play ( anecdotal evidence, about 40-60% in RPS/FWG and 10-20% in NUM). This method needs a lot more rounds to start modelling players behavior correctly. 


## Parametric Experience Weighted Attraction model (Camerer & Ho 1997)
```{r}
EWA_par <- function(par,data,return_value){
  # par: vector of parameters to the EWA model
  # Data :choice data by trial for each participant
  # Returns sum of loglikelihoods or likelihood per trial
  phi <- par[1]
  delta <- par[2]
  rho <- par[3]
  lambda <- par[4]


  # Define attraction vectors for each game
  A_RPS = matrix(0.0,3)
  names(A_RPS) <- c("R","P","S")
  A_FWG = matrix(0.0,3)
  names(A_FWG) <- c("F","W","G")
  A_NUM = matrix(0.0,5)
  names(A_NUM) <- c("1","2","3","4","5")
  
  # # Define reward matrices from the prospective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
  dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))

  # Initiate likelihood by trial vector
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
      reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
      lik_hum[t] <- 1/nopts
      # Initiate N(0) = 1 as in Camerer and Ho 1997 paper.
      N <- 1
      
    } else {
      # Get reward and past human action
      h_act_prev <- as.character(data[t-1,"h_action"])
      a_act_prev <- as.character(data[t-1,"a_action"])
      for (i in 1:length(Att)) {
        action <- as.character(names(Att)[i])
        # cat("this is current strat",action,"\n")
        # cat("this is previous human action",h_act_prev,"\n")

        # Attraction vector update rule
        Att[[i]] <- (phi*N*Att[[i]]  + ( delta + (1- delta)*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (rho*N + 1)
      }
      #Update the value of N
      N <- rho*N + 1

      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att)/sum(exp(lambda*Att))

      # Get actual human action and compute likelihood
      h_act <- as.character(data[t,"h_action"])
      act_index <- match(h_act, names(Att))
      lik_hum[t] <- probs[[act_index]]
    }
  }

 if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret) ) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
}
```

## Fitting Parametric EWA to data 
```{r, cache =TRUE}

EWA_modelling <- list()
for(id in unique(dat$human_id)) {
  EWA_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  # EWA_modelling[[id]] <- optim(c(0.5,0.5,0.5,0.5),fn=EWA_par,gr = NULL, data=tdat,"-2loglik", lower = c(0.001,0.001,0.001,0.001), upper = c(1,0.99,0.99,1), method="L-BFGS-B")
  EWA_modelling[[id]] <- DEoptim(fn=EWA_par,lower = c(0.01,0.01,0.01,0.01), upper = c(10,1,1,10), data=tdat,"-2loglik",control=list(trace = FALSE,parallelType=1))
}

save(EWA_modelling,file="EWA_modelling.RData")

```


##  Self-Tuning EWA (Camerer & Ho 2007) 
```{r}

EWA_self <- function(par,data,return_value){

  lambda <- par[1]
  # Initiate N(0) = 1 as in Camerer and Ho 1997 paper. 

  
  # Define attraction vectors for each game 
  A_RPS = matrix(0.0,3)
  names(A_RPS) <- c("R","P","S")
  A_FWG = matrix(0.0,3)
  names(A_FWG) <- c("F","W","G")
  A_NUM = matrix(0.0,5)
  names(A_NUM) <- c("1","2","3","4","5")
  
  # # Define reward matrices from the prospective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
  dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  
  # Initiate likelihood by trial vector 
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
      reward <- switch(as.character(t_game),rps=reward_RPS,fwg = reward_FWG, numbers = reward_NUM)
      game_data <- subset(data, game == t_game)
      lik_hum[t] <- 1/nopts
      N <- 1.0

      R_t <- rep(0.0,nopts)
      H_t <- R_t
      
    } else {
      
      indx <- data[t,"round"]
      # Get reward and past human action
      h_act_prev <- as.character(game_data[indx-1,"h_action"])
      a_act_prev <- as.character(game_data[indx-1,"a_action"])

      # Estimate phi(t)
      R_t <- as.numeric(names(Att) == a_act_prev)
      # cat("This is R(t)",R_t,"\n")
      
      H_t <- (H_t*(indx-2) + R_t)/(indx-1)
      # cat("this is H(t)",H_t,"\n")
      
      Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
      # cat("This is Phit(t)",Phi_t,"\t")
      
      # Estimate vector Delta(t)

      delta_t <- as.numeric(reward[,a_act_prev] >= as.numeric(game_data[indx-1,"score"]))
      #cat("this is delta(t)",delta_t,"\n")
      
      for (i in 1:length(Att)) {
        action <- as.character(names(Att)[i])
        # cat("this is current strat",action,"\n")
        # cat("this is previous humna action",h_act_prev,"\n")
        
        # Attraction vector update rule 
        Att[[i]] <- (Phi_t*N*Att[[i]]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act_prev))*as.numeric(reward[action,a_act_prev])) / (Phi_t*N + 1)
      }
      #Update the value of N 
      N <- Phi_t*N + 1
      #cat(Att,'\n')
      
      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att)/sum(exp(lambda*Att))
      
      # Get actual human action and compute likelihood
      h_act <- as.character(game_data[indx,"h_action"])
      act_index <- match(h_act, names(Att))
      lik_hum[t] <- probs[[act_index]]
    }
  }
  
 if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
}

```


```{r}
##Plotting self_tuning EWA likelihood function for lambda from 0 to 100 
# 
# lambdas <- seq(1,10,0.01)
# count = 0.0
# 
# 
# 
# for(id in unique(dat$human_id)) {
#   if (count <= 15) {
#       tdat <- subset(dat,human_id == id)
#       likelihoods <- sapply(lambdas, EWA_self, tdat, "-2loglik")
#       plot(lambdas,likelihoods)
#       count <- count + 1
#   }
# }

```
  
## Fitting Self-Tuning EWA to data 
```{r, cache=TRUE}


EWA_self_modelling <- list()
for(id in unique(dat$human_id)) {
  EWA_self_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
#   EWA_self_modelling[[id]] <- optim(10.0,fn=EWA_self,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  EWA_self_modelling[[id]] <- DEoptim(fn=EWA_self, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}

save(EWA_self_modelling,file="EWA_self_modelling.RData")

  
```

```{r}
# cat(EWA_self_modelling[["38VxtUSv_h6RR5-tAAA2"]]$optim$bestmem)
```



### Second attempt at ST_EWA with states and no initial guess of initial states. 
```{r}
ST_EWA_STATES <- function(par,data,return_value){

  lambda <- par[1]

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  states_NUM <- paste0(G3$Var1,G3$Var2)
  
  A_RPS = matrix(-0.5,9,3)
  dimnames(A_RPS) = list(states_RPS, c("R", "P", "S"))

  A_FWG = matrix(-0.5,9,3)
  dimnames(A_FWG) = list(states_FWG, c("F", "W", "G"))
  
  A_NUM = matrix(-0.5,25,5)
  dimnames(A_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))

  
  # # Define reward matrices from the perspective of the row player (human in our case)
  reward_RPS <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_RPS) = list(c("R", "P", "S"), c("R", "P", "S"))
  
  reward_FWG <- t(matrix(c(0,-1,1,1,0,-1,-1,1,0),nrow=3,ncol=3))
  dimnames(reward_FWG) = list(c("F", "W", "G"), c("F", "W", "G"))
  
  reward_NUM <- t(matrix(c(0,-1,0,0,1,1,0,-1,0,0,0,1,0,-1,0,0,0,1,0,-1,-1,0,0,1,0),nrow=5, ncol=5))
  dimnames(reward_NUM) = list(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  
  # Initiate likelihood by trial vector 
  lik_hum <- matrix(0.0,nrow(data))
  
   for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    
    if(data[t,"round"] == 1) {
      Att <- switch(as.character(t_game),rps=A_RPS,fwg = A_FWG, numbers = A_NUM)
      state_vec <- switch(as.character(t_game), rps=states_RPS, fwg = states_FWG, numbers = states_NUM)
      reward <- switch(as.character(t_game), rps=reward_RPS, fwg = reward_FWG, numbers = reward_NUM)
      game_data <- subset(data, game == t_game)
      
      # first round is uniform prediction
      lik_hum[t] <- 1/nopts
      # Asumme N(0) = 1 for now...see discussion in paper
      N <- 1.0
      # initialize H_t by looking at first action 
      # H_t <- rep(0.0,nopts)
      a_act <- as.character(game_data[1,"a_action"])
      H_t <- as.numeric(colnames(Att) == a_act)
      
      # Rounds 2 to end 
    } else { 

      indx <- data[t,"round"]
      
      # Get reward and past human action
      h_act_prev <- as.character(game_data[indx-1,"h_action"])
      a_act_prev <- as.character(game_data[indx-1,"a_action"])
      curr_state <- paste0(h_act_prev,a_act_prev)
      
      h_act <- as.character(game_data[indx,"h_action"])
      a_act <- as.character(game_data[indx,"a_action"])
      new_state <- paste0(h_act,a_act)
      
      # Assume human chooses action probabilistically using softmax on Attraction values
      probs <- exp(lambda*Att[curr_state,])/sum(exp(lambda*Att[curr_state,]))
      
      # Get actual human action and compute likelihood
      act_index <- match(h_act, colnames(Att))
      lik_hum[t] <- probs[[act_index]]
      
      # Update rule:
      # Estimate R(t) (recent history) then H(t) (history) then phi(t) (change detector)
      R_t <- as.numeric(colnames(Att) == a_act)
      #cat("This is R(t)",R_t,"\n")
      
      H_t <- (H_t*(indx-1) + R_t)/(indx)
      #cat("this is H(t)",H_t,"\n")
      
      Phi_t <- 1 - 0.5*sum((H_t-R_t)^2)
      #cat("This is Phit(t)",Phi_t,"\t")
      
      # Estimate vector Delta(t)
      delta_t <- as.numeric(reward[,a_act] >= as.numeric(game_data[indx,"score"]))
      #cat("this is delta(t)",delta_t,"\n")
      
      # This is a vectorised ST_EWA update rule, easier to follow the loop below in comments
      state_indx <- match(curr_state,state_vec)
      Att[state_indx,] <- (Phi_t*N*Att[state_indx,]  + ( delta_t + (1- delta_t)*(colnames(Att) == h_act))*as.numeric(reward[colnames(Att),a_act])) / (Phi_t*N + 1)
      
      ##############
      # for (i in 1:nopts) {
      #   action <- as.character(colnames(Att)[i])
      #   state_indx <- match(curr_state,state_vec)
      #   #cat("this is the current state index", state_indx, "\n")
      # 
      #   # Attraction vector update rule
      #   Att[state_indx,i] <- (Phi_t*N*Att[state_indx,i]  + ( delta_t[[i]] + (1- delta_t[[i]])*(action == h_act))*as.numeric(reward[action,a_act])) / (Phi_t*N + 1)
      # 
      # }
      # 
      #cat("this is the current Att vector", Att, "\n")
      ###################
      
      #Update the value of N 
      N <- Phi_t*N + 1
      
      #Update the state 
      curr_state <- new_state 
    }
   }  
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```


```{r}
# data12 = subset(dat,human_id == "38VxtUSv_h6RR5-tAAA2")
# ST_EWA_STATES(0.5, data12, "-2loglik")
# optim(10.0,fn=ST_EWA_STATES,gr = NULL, data=data12,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")

```


```{r, cache=TRUE}


ST_EWA_STATES_modelling <- list()
for(id in unique(dat$human_id)) {
  ST_EWA_STATES_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
#   ST_EWA_STATES_modelling[[id]] <- optim(10.0,fn=ST_EWA_STATES,gr = NULL, data=tdat,"-2loglik", lower = 0.0, upper = 200, method="L-BFGS-B")
# }
  ST_EWA_STATES_modelling[[id]] <- DEoptim(fn=ST_EWA_STATES, lower = 0.0, upper = 100.0, data=tdat, "-2loglik", control=list(trace = FALSE,parallelType=1))
}

save(ST_EWA_STATES_modelling,file="ST_EWA_STATES_modelling.RData")

```



## What next?: Fitting models that explicitly take into account opponent strategy. LOLA? Won't work, it suffers from same issues as normal QL in that most states are not visited during game play. Influence model? Like belief lerning. it still has no sequential memory, still computes distribution on beliefs. We need to incorporate models that can mimic human memory by looking at history of play and leverage sequence prediction capabilities. 


### Model based model: learn transition probabilities to new states. Weigh the value of an action by the conditional probability of ending up in a future state times the value of the new state.... once you take an action in current round, you can only end up in 3 states, with probabilities p, q and 1-p-q. You are learning these probabilities by updating priors (initial prior is dirichlet). 
```{r}
MBM <- function(par,data,return_value,gamma){
  # Par[1] -> beta= inverse temperature parameter in softmax choice function
  # Par[2] -> lambda = learning rate (one per game?)
  # data : participant and ai choice data.
  # return_value=c("-2loglik","likelihood_by_trial")
  # Gamma is discount factor for future rewards 
  # Returns Q-values per trial and predicts choice using softmax
  beta <- par[1]
  alpha <- par[2]

  #gamma <- 0.9 

  #Define matrix of state spaces for each game 
  G1 <- expand.grid(c("R", "P", "S"),c("R", "P", "S"))
  states_RPS <- paste0(G1$Var1,G1$Var2)

  G2 <- expand.grid(c("F", "W", "G"), c("F", "W", "G"))
  states_FWG <- paste0(G2$Var1,G2$Var2)

  G3 <- expand.grid(c("1", "2", "3", "4", "5"), c("1", "2", "3", "4", "5"))
  states_NUM <- paste0(G3$Var1,G3$Var2)
  
  # Matrices to store Q_values in each state 
  Q_vals_RPS = matrix(-0.5,9,3)
  dimnames(Q_vals_RPS) = list(states_RPS, c("R", "P", "S"))
  Q_vals_FWG = matrix(-0.5,9,3)
  dimnames(Q_vals_FWG) = list(states_FWG, c("F", "W", "G"))
  Q_vals_NUM = matrix(-0.5,25,5)
  dimnames(Q_vals_NUM) = list(states_NUM, c("1", "2", "3", "4", "5"))
  
  # Transition Matrices 
  Transit_RPS = matrix(1/9,9,9)
  dimnames(Transit_RPS) = list(states_RPS, states_RPS)
  Transit_FWG = matrix(1/9,9,9)
  dimnames(Transit_FWG) = list(states_FWG, states_FWG)
  Transit_NUM = matrix(1/25,25,25)
  dimnames(Transit_NUM) = list(states_NUM, states_NUM)
  
  
  lik_hum <- matrix(0.0,nrow(data))

  for(t in 1:nrow(data)) {
    t_game <- data[t,"game"]
    if(t_game == "numbers") nopts <- 5 else nopts <- 3
    
    if(data[t,"round"] == 1) {
      # first round is uniform prediction
      Q_vals <- switch(as.character(t_game),rps=Q_vals_RPS,fwg = Q_vals_FWG, numbers = Q_vals_NUM)
      state_vec <- switch(as.character(t_game),rps=states_RPS,fwg = states_FWG, numbers = states_NUM)
      Transit <- switch(as.character(t_game),rps=Transit_RPS,fwg = Transit_FWG, numbers = Transit_NUM)
      lik_hum[t] <- 1/nopts
      # Randomly select prev_state and actions for first round
      curr_state <- sample(state_vec, size = 1)
      h_act <- sample(colnames(Q_vals), size =1)
      ai_act <- sample(colnames(Q_vals), size =1)
      reward <- 0 
      
    } else {
      # Get past human action and associated reward 
      h_act <- as.character(data[t-1,"h_action"])
      ai_act <- as.character(data[t-1,"a_action"])
      reward <- as.numeric(data[t-1,"score"])
    }
    
    
    # cat(curr_state, " This is the previous state", "\n")
    new_state <- paste0(h_act,ai_act)
    # cat(new_state,"This is new state","\n")
    
    
    # Update Transit matrix ....first create hot vector = 1 if new state, 0 otherwise 
    hot_vector <- state_vec == new_state
  
    
    # Transit probs update: TD learning on transition matrix, learning rate fixed at 0.1
    Transit[curr_state,] <- Transit[curr_state,] + 0.1* (hot_vector - Transit[curr_state,])
    #cat(Transit[curr_state,], "\n")
    
    # Update Q-values as current reward + a weighted (by trasnsit probs) average of future Q-values.
    Q_Row_max <- apply(Q_vals, 1, max, na.rm = TRUE)
    #cat(Transit[curr_state,] * Q_Row_max, "\n")
    Q_vals[curr_state, h_act] <- reward +  gamma*( Transit[curr_state,] %*% Q_Row_max  ) 
    
    
    
    # Assume human chooses action probabilistically using softmax on Q values
    probs <- exp(Q_vals[new_state,]/beta)/sum(exp(Q_vals[new_state,]/beta))
    #if (data[t,"round"] == 50) { cat(Q_vals,"\n") }
    
    # Get actual human action and compute likelihood
    h_act <- as.character(data[t,"h_action"])
    act_index <- match(h_act, colnames(Q_vals))
    lik_hum[t] <- probs[[act_index]]
      
    # Update state
    curr_state<- new_state 
    #}
  }
  if(return_value == "-2loglik") {
    ret <- -2*sum(log(lik_hum))
    if(is.infinite(ret) || is.nan(ret)) {
      return(1e+300)
    } else {
      return(ret)
    }
  }
  if(return_value == "likelihood_by_trial") return(lik_hum)
  
}

```


```{r, cache=TRUE}
MBM_modelling <- list()
for(id in unique(dat$human_id)) {
  MBM_modelling[[id]] <- list()
  tdat <- subset(dat,human_id == id)
  # MBM_modelling[[id]] <- optim(c(1,0.1),fn=QMBM,gr = NULL, data=tdat,"-2loglik", gamma =0 , lower = c(0,0), upper = c(10,0.99), method="L-BFGS-B")

   MBM_modelling[[id]] <- DEoptim(fn=MBM,lower = c(0,0), upper = c(10,1), data=tdat,"-2loglik", gamma = 0.9, control=list(trace = FALSE,parallelType=1))
}

save(MBM_modelling, file="MBM_modelling.RData")

```


## Putting results together 
```{r}
load("QL_modelling.RData")
load("QL_states_modelling.RData")
load("EWA_modelling.RData")
load("EWA_self_modelling.RData")
load("MBM_modelling.RData")
load("ST_EWA_STATES_modelling.RData")


All_results <- data.frame()
for(id in unique(dat$human_id)) {
  All_results <- rbind(All_results,
                       data.frame(
                         "ID" = id,
                         "condition" = dat[dat$human_id==id,"condition"][1],
                         "Random_BIC" = -2*(100*log(1/3) + 50*log(1/5)),
                         
                         # Bayesian updating with/without transfer 
                         # "Bayes_Tr_BIC" = exp1_Bayes_game_Tr[[id]]$optim$bestval+ 1*log(150),
                         # "Bayes_No_Tr_BIC" = exp1_Bayes_no_Tr[[id]]$optim$bestval + 1*log(150),
                         # # Theta is the parameter governing AI stochasticity. Truth is 0.9
                         # "theta_transfer" = exp1_Bayes_game_Tr[[id]]$optim$bestmem[[1]],
                         # "theta_no_transfer" = exp1_Bayes_no_Tr[[id]]$optim$bestmem[[1]],
                         # 
                         
                         # BCH with/without transfer (With Softmax) 
                         "Bayes_Tr_BIC" = use_softmax_tr[[id]]$optim$bestval+ 2*log(150),
                         "Bayes_No_Tr_BIC" = use_softmax_NT[[id]]$optim$bestval+ 2*log(150),
                         
                         # Theta is the parameter governing AI stochasticity. Truth is 0.9
                         "theta_transfer" = use_softmax_tr[[id]]$optim$bestmem[[1]],
                         "lambda_transfer" = use_softmax_tr[[id]]$optim$bestmem[[2]],
                         
                         "theta_no_transfer" = use_softmax_NT[[id]]$optim$bestmem[[1]],
                         "lambda_no_transfer" = use_softmax_NT[[id]]$optim$bestmem[[2]],
                         
                         
                                                                                    
                         # Q-Learning
                         "QL_BIC" = QL_modelling[[id]]$value + 2*log(150),
                         # beta ->  inverse temperature parameter in softmax choice function
                         "QL_Beta" = QL_modelling[[id]]$par[1],
                         # alpha -> learning rate in QL update
                         "QL_alpha" = QL_modelling[[id]]$par[2],

                         
                         # Q-learning with last round states
                         "QL_states_BIC" = QL_states_modelling[[id]]$optim$bestval + 2*log(150),
                         # beta ->  inverse temperature parameter in softmax choice function
                         "QL_states_Beta" = QL_states_modelling[[id]]$optim$bestmem[1],
                         # alpha -> learning rate in QL update
                         "QL_states_alpha" = QL_states_modelling[[id]]$optim$bestmem[2],
                         
                        
                         # Parametric EWA
                         # Parametric EWA BIC
                         "EWA_BIC" = EWA_modelling[[id]]$optim$bestval + 4*log(150),
                         "EWA_2LL" = EWA_modelling[[id]]$optim$bestval,
                         # Phi is depreciation of past attractions
                         "EWA_Phi" = EWA_modelling[[id]]$optim$bestmem[1],
                         # Delta is weight of foregone payoffs vs actual payoffs
                         "EWA_Delta" = EWA_modelling[[id]]$optim$bestmem[2],
                         # Rho is depreciation of the experience measure N(t)
                         "EWA_Rho" = EWA_modelling[[id]]$optim$bestmem[3],
                         #Lambda is a parameter of the softmax choice function (inverse Temperature)
                         "EWA_Lambda" = EWA_modelling[[id]]$optim$bestmem[4],


                         # Self-Tuning EWA (only 1 parameter)
                          # Parametric EWA BIC
                         # "EWA_self_2LL" = EWA_self_modelling[[id]]$optim$bestval,
                         # "EWA_self_BIC" = EWA_self_modelling[[id]]$optim$bestval + 1*log(150),
                         # #Lambda is a parameter of the softmax choice function (inverse Temperature)
                         # "EWA_self_Lambda" = EWA_self_modelling[[id]]$optim$bestmem[1],
                         
                         # self_tuning EWA with states
                         
                         "ST_EWA_STATES_BIC" = ST_EWA_STATES_modelling[[id]]$optim$bestval + 1*log(150),
                         "ST_EWA_STATES_Lambda" = ST_EWA_STATES_modelling[[id]]$optim$bestmem[1]
                         

                         ))
}

write.csv(All_results,file="exp1_all_results.csv",row.names = FALSE)

```


## Calucalting BIC weights
```{r}

comp_BICs <- All_results[c("ID","Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","ST_EWA_STATES_BIC")]

exp1_BIC_weights <- comp_BICs["ID"]
exp1_BIC_weights[,2:ncol(comp_BICs)] <- t(apply(comp_BICs[,2:ncol(comp_BICs)], 1, function(i) exp(-0.5*(i-min(i)) )))
colnames(exp1_BIC_weights) <-colnames(comp_BICs)

exp1_BIC_weights[,2:ncol(exp1_BIC_weights)] <- t(apply(exp1_BIC_weights[,-1], 1, function(i) round(i/sum(i),2)))
                     
                     
```

## Building table of best model per participant
```{r}

# Table_results <- table(All_results[, "condition"],c("random","ToM_BT","ToM_NBT","QL", "QL_states","EWA","S_EWA","ST_EWA_STATES")[apply(All_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_BIC","QL_states_BIC","EWA_BIC","EWA_self_BIC","ST_EWA_STATES_BIC")],1,which.min)])

# We choose to omit the simple QL and EWA and opt to only include the ones with states as they are more relevant. 

Table_results <- table(All_results[, "condition"],c("random","ToM_BT","ToM_NBT", "QL_states","ST_EWA_STATES")[apply(All_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")],1,which.min)])

 write.csv(Table_results,file="Table_results.csv",row.names = TRUE)
 kable(Table_results)
 #barplot(Table_results)
 
```
##### Looks like most participants behavior can be explained by QL_states. Bayesian models best explain a few more behaviors, while the others are consistent with random behavior, and only 1 with QL and 1 with S_EWA. 
 
 
###### Question: Can we see evidence for transfer by looking at difference in BICS btwn Bayes with and without transfer. Let's test correlation between difference in BICs  with early rounds score (evidence for transfer) 
```{r}
# id_results_tst <- subset(All_results, ID == "38VxtUSv_h6RR5-tAAA2")
```

```{r}

exp1_model_comp <- data.frame()
for(id in unique(dat$human_id)) {
  tdat <- subset(dat,human_id == id)
  tot_score <- sum(tdat$score)
  tot_time <- sum(tdat$human_rt)
  #early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg"))
  early_dat <- subset(tdat,between(tdat$round,2,6) & (game =="fwg" | game =="numbers") )
  tr_score <- sum(early_dat$score)
  id_results <- subset(All_results, ID == id)
  min_BIC <- apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")],1,min)
  
  best_model <- c("random","ToM_Game_Tr","ToM_No_Tr", "QL_states","ST_EWA_STATES")[apply(id_results[,c("Random_BIC","Bayes_Tr_BIC","Bayes_No_Tr_BIC","QL_states_BIC","ST_EWA_STATES_BIC")],1,which.min)]
  
  exp1_model_comp <- rbind(exp1_model_comp,
                       data.frame(
                         "human_id" = id,
                         "condition" = dat[dat$human_id==id,"condition"][1],
                         "Early_game_score" = tr_score,
                         "Total_score" = tot_score,
                         "Best_model" = best_model,
                         "Total_time" = sum(tdat$human_rt),
                         "TR_minus_NT_BIC" = id_results[,"Bayes_Tr_BIC"] - id_results[,"Bayes_No_Tr_BIC"],
                         "Rand_minus_best_BIC" =  id_results[,"Random_BIC"] - min_BIC

                       ))
}

write.csv(exp1_model_comp,file = "exp1_model_comp.csv", row.names = FALSE)

```

### Correlate difference between BICs of Bayes transfer and no transfer with early rounds score (evidence for transfer) 
```{r}

cor.test(exp1_model_comp$TR_minus_NT_BIC,exp1_model_comp$Early_game_score, method="spearman")

```

### Correlation between early game score and difference between best model and random BIC
```{r}
cor.test(exp1_model_comp$Rand_minus_best_BIC, exp1_model_comp$Early_game_score, method="spearman")

```
### Histogram of best fitting models 
```{r}
barplot(table(exp1_model_comp$Best_model))
```
# Notes : There is positive and stat significant correlation between the difference between the best model BIC and random BIC on one hand, and the early scores from the target games ( FWG and NUM). What does this mean? It can be interpreted as a validation of early game score as good measure of how non-random play is....


# More exploratory work......


## Let's compare total scores of each participant by the model of best fit, see if Bayes+transfer total scores are higher than Bayes + no transfer  

```{r}

# ggboxplot(exp1_model_comp, x = "model", y = "Total_score",
#           color = "model", palette = c("#00AFBB", "#E7B800", "#FC4E07"),
#           order = c("Random", "No Transfer", "Transfer"),
#           ylab = "Total Score", xlab = "Model with best fit")

exp1_model_comp$model <- recode(exp1_model_comp$Best_model,"Bayes Tr" = "Tranfer","Bayes No Tr" = "No Transfer", "QL_states" = "Q_Learning",  .default = "Random")
model <- factor(exp1_model_comp$model)
condition <- factor(exp1_model_comp$condition)

# Total score by best predictive model 
tapply(exp1_model_comp$Total_score, model, mean)

# Compute the analysis of variance
res.aov <- aov(Total_score ~ model + condition, data = exp1_model_comp)
# Summary of the analysis
summary(res.aov)
TukeyHSD(res.aov)
```

## What if we remove from data people who play randomly and then compare Transfer v No Transfer? 
```{r}

Non_Random_Only <- subset(exp1_model_comp, model != "Random" )

group_by(Non_Random_Only, model) %>%
  summarise(
    count = n(),
    mean = mean(Early_game_score, na.rm = TRUE),
    sd = sd(Early_game_score, na.rm = TRUE)
  )

res2 <- aov(Early_game_score ~ model, data = Non_Random_Only) 
summary(res2)
TukeyHSD(res2)
```

# Total time by best predictive model
```{r}
tapply(exp1_model_comp$Total_time, model, mean)

time.aov <- aov(Total_time ~ model, data = exp1_model_comp)
# Summary of the analysis
summary(time.aov)
TukeyHSD(time.aov)

``` 
#Early game scores for people best modelled with a Bayesian model with transfer are indeed higher than those best modelled without transfer but difference is not statistically significant. 



